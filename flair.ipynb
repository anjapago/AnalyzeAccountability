{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/anjapago/AnalyzeAccountability/blob/master/flair.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtgjqStCtTF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/anjapago/flair\n",
      "  Cloning https://github.com/anjapago/flair to /private/var/folders/xw/04j4bflj1qq10rwcc1nmw4v80000gn/T/pip-req-build-eg3bn6hk\n",
      "  Running command git clone -q https://github.com/anjapago/flair /private/var/folders/xw/04j4bflj1qq10rwcc1nmw4v80000gn/T/pip-req-build-eg3bn6hk\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sklearn in /opt/anaconda3/lib/python3.7/site-packages (from flair==0.4.2) (0.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /opt/anaconda3/lib/python3.7/site-packages (from flair==0.4.2) (3.1.1)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.7/site-packages (from flair==0.4.2) (2019.8.19)\n",
      "Requirement already satisfied: bpemb>=0.2.9 in /opt/anaconda3/lib/python3.7/site-packages (from flair==0.4.2) (0.3.0)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from flair==0.4.2) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /opt/anaconda3/lib/python3.7/site-packages (from flair==0.4.2) (4.36.1)\n",
      "Collecting langdetect (from flair==0.4.2)\n",
      "Collecting hyperopt>=0.1.1 (from flair==0.4.2)\n",
      "  Using cached https://files.pythonhosted.org/packages/0b/4a/79541d4f61e7878f846f68ab31ed709bac6ee99345378c0e02658c3be0d4/hyperopt-0.2.2-py3-none-any.whl\n",
      "Collecting ipython==7.6.1 (from flair==0.4.2)\n",
      "  Using cached https://files.pythonhosted.org/packages/a6/2c/c7d44277b599df35af734d8f4142d501192fdb7aef5d04daf882d7eccfbc/ipython-7.6.1-py3-none-any.whl\n",
      "Collecting segtok>=1.5.7 (from flair==0.4.2)\n",
      "Collecting sqlitedict>=1.6.0 (from flair==0.4.2)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from flair==0.4.2) (0.2.0)\n",
      "Requirement already satisfied: pytest>=3.6.4 in /opt/anaconda3/lib/python3.7/site-packages (from flair==0.4.2) (5.2.1)\n",
      "Collecting torch>=1.1.0 (from flair==0.4.2)\n",
      "  Using cached https://files.pythonhosted.org/packages/7e/94/0ed9f7899aa0f5e7ff753a3a2b6944c146eef3f4cd51c59ab07c4575992b/torch-1.3.1-cp37-none-macosx_10_7_x86_64.whl\n",
      "Collecting deprecated>=1.2.4 (from flair==0.4.2)\n",
      "  Using cached https://files.pythonhosted.org/packages/f6/89/62912e01f3cede11edcc0abf81298e3439d9c06c8dce644369380ed13f6d/Deprecated-1.2.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20 in /opt/anaconda3/lib/python3.7/site-packages (from flair==0.4.2) (1.24.2)\n",
      "Collecting mpld3==0.3 (from flair==0.4.2)\n",
      "Collecting pytorch-transformers>=1.1.0 (from flair==0.4.2)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl\n",
      "Collecting tabulate (from flair==0.4.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.7/site-packages (from sklearn->flair==0.4.2) (0.21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair==0.4.2) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair==0.4.2) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair==0.4.2) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair==0.4.2) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair==0.4.2) (1.17.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from bpemb>=0.2.9->flair==0.4.2) (2.22.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.7/site-packages (from bpemb>=0.2.9->flair==0.4.2) (0.1.83)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim>=3.4.0->flair==0.4.2) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from gensim>=3.4.0->flair==0.4.2) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim>=3.4.0->flair==0.4.2) (1.9.0)\n",
      "Collecting networkx==2.2 (from hyperopt>=0.1.1->flair==0.4.2)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.7/site-packages (from hyperopt>=0.1.1->flair==0.4.2) (0.17.1)\n",
      "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.7/site-packages (from hyperopt>=0.1.1->flair==0.4.2) (1.2.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/anaconda3/lib/python3.7/site-packages (from ipython==7.6.1->flair==0.4.2) (4.3.3)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from ipython==7.6.1->flair==0.4.2) (2.0.10)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /opt/anaconda3/lib/python3.7/site-packages (from ipython==7.6.1->flair==0.4.2) (0.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/anaconda3/lib/python3.7/site-packages (from ipython==7.6.1->flair==0.4.2) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from ipython==7.6.1->flair==0.4.2) (0.15.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/anaconda3/lib/python3.7/site-packages (from ipython==7.6.1->flair==0.4.2) (4.7.0)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.7/site-packages (from ipython==7.6.1->flair==0.4.2) (4.4.0)\n",
      "Requirement already satisfied: pygments in /opt/anaconda3/lib/python3.7/site-packages (from ipython==7.6.1->flair==0.4.2) (2.4.2)\n",
      "Requirement already satisfied: backcall in /opt/anaconda3/lib/python3.7/site-packages (from ipython==7.6.1->flair==0.4.2) (0.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/anaconda3/lib/python3.7/site-packages (from ipython==7.6.1->flair==0.4.2) (41.5.1)\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.6.4->flair==0.4.2) (1.8.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.6.4->flair==0.4.2) (19.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.6.4->flair==0.4.2) (19.2.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.6.4->flair==0.4.2) (7.2.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.6.4->flair==0.4.2) (1.3.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.6.4->flair==0.4.2) (0.13.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.6.4->flair==0.4.2) (0.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/anaconda3/lib/python3.7/site-packages (from pytest>=3.6.4->flair==0.4.2) (0.23)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/anaconda3/lib/python3.7/site-packages (from deprecated>=1.2.4->flair==0.4.2) (1.11.2)\n",
      "Requirement already satisfied: sacremoses in /opt/anaconda3/lib/python3.7/site-packages (from pytorch-transformers>=1.1.0->flair==0.4.2) (0.0.35)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/lib/python3.7/site-packages (from pytorch-transformers>=1.1.0->flair==0.4.2) (1.10.26)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn->flair==0.4.2) (0.13.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->bpemb>=0.2.9->flair==0.4.2) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->bpemb>=0.2.9->flair==0.4.2) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->bpemb>=0.2.9->flair==0.4.2) (3.0.4)\n",
      "Requirement already satisfied: boto>=2.32 in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim>=3.4.0->flair==0.4.2) (2.49.0)\n",
      "Requirement already satisfied: parso>=0.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython==7.6.1->flair==0.4.2) (0.5.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair==0.4.2) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest>=3.6.4->flair==0.4.2) (0.6.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.7/site-packages (from sacremoses->pytorch-transformers>=1.1.0->flair==0.4.2) (7.0)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.26 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->pytorch-transformers>=1.1.0->flair==0.4.2) (1.13.26)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->pytorch-transformers>=1.1.0->flair==0.4.2) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->pytorch-transformers>=1.1.0->flair==0.4.2) (0.2.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.26->boto3->pytorch-transformers>=1.1.0->flair==0.4.2) (0.15.2)\n",
      "Building wheels for collected packages: flair\n",
      "  Building wheel for flair (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flair: filename=flair-0.4.2-cp37-none-any.whl size=102196 sha256=6f6ec2799bbd0f55089762854846adf46ab37c49849fc6b3d9adc417be679363\n",
      "  Stored in directory: /private/var/folders/xw/04j4bflj1qq10rwcc1nmw4v80000gn/T/pip-ephem-wheel-cache-42hzniue/wheels/7f/d7/5a/291cb0d8fe7f5f4db43d32dd7560c29dbc2a508cb1a8248138\n",
      "Successfully built flair\n",
      "Installing collected packages: langdetect, networkx, hyperopt, ipython, segtok, sqlitedict, torch, deprecated, mpld3, pytorch-transformers, tabulate, flair\n",
      "  Found existing installation: networkx 2.3\n",
      "    Uninstalling networkx-2.3:\n",
      "      Successfully uninstalled networkx-2.3\n",
      "  Found existing installation: ipython 7.8.0\n",
      "    Uninstalling ipython-7.8.0:\n",
      "      Successfully uninstalled ipython-7.8.0\n",
      "Successfully installed deprecated-1.2.7 flair-0.4.2 hyperopt-0.2.2 ipython-7.6.1 langdetect-1.0.7 mpld3-0.3 networkx-2.2 pytorch-transformers-1.2.0 segtok-1.5.7 sqlitedict-1.6.0 tabulate-0.8.6 torch-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/anjapago/flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '/opt/anaconda3/lib/python3.7' not in sys.path:\n",
    "    sys.path.append('/opt/anaconda3/lib/python3.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/andreapagotto/Documents/GitHub/gsoc/AnalyzeAccountability',\n",
       " '/opt/anaconda3/envs/transformers/lib/python37.zip',\n",
       " '/opt/anaconda3/envs/transformers/lib/python3.7',\n",
       " '/opt/anaconda3/envs/transformers/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/Users/andreapagotto/.local/lib/python3.7/site-packages',\n",
       " '/Users/andreapagotto/.local/lib/python3.7/site-packages/simpletransformers-0.4.3-py3.7.egg',\n",
       " '/opt/anaconda3/envs/transformers/lib/python3.7/site-packages',\n",
       " '/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/Users/andreapagotto/.ipython',\n",
       " '/opt/anaconda3/bin/pip',\n",
       " '/opt/anaconda3/lib/python3.7']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e42b806689dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flair'"
     ]
    }
   ],
   "source": [
    "import flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sKBOr5FsNy19",
    "outputId": "81dec7bf-a08c-4c6d-ad62-11ed0494dc4f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-be02d0d558e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlairEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDocumentPoolEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_fetcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLPTaskDataFetcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flair'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'),\n",
    "                                                       test_file='flair_test.csv',\n",
    "                                                       dev_file='flair_dev.csv',\n",
    "                                                       train_file='flair_train.csv')\n",
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "                   FlairEmbeddings('news-forward-fast'),\n",
    "                   FlairEmbeddings('news-backward-fast')]\n",
    "document_embeddings = DocumentPoolEmbeddings(word_embeddings)\n",
    "\n",
    "print(\"Create Classifier *************************\")\n",
    "classifier = TextClassifier(document_embeddings,\n",
    "                            label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "print(\"Create Trainer *******************************\")\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "print(\"Begin Training *******************************\")\n",
    "trainer.train(max_epochs=10, base_path = \"flair_pooled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zxfs6DE-PsZx",
    "outputId": "d784c941-435b-4104-dca2-9a41a2b29c95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 04:00:11,940 Reading data from .\n",
      "2019-07-22 04:00:11,941 Train: flair_train.csv\n",
      "2019-07-22 04:00:11,943 Dev: flair_dev.csv\n",
      "2019-07-22 04:00:11,944 Test: flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:447: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:454: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:463: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 04:00:29,254 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34066/34066 [00:00<00:00, 258607.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 04:00:29,390 [b'0', b'1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 04:00:29,513 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:00:29,514 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2148, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-07-22 04:00:29,516 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:00:29,517 Corpus: \"Corpus: 34066 train + 5678 dev + 5678 test sentences\"\n",
      "2019-07-22 04:00:29,518 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:00:29,518 Parameters:\n",
      "2019-07-22 04:00:29,519  - learning_rate: \"0.1\"\n",
      "2019-07-22 04:00:29,521  - mini_batch_size: \"32\"\n",
      "2019-07-22 04:00:29,521  - patience: \"3\"\n",
      "2019-07-22 04:00:29,522  - anneal_factor: \"0.5\"\n",
      "2019-07-22 04:00:29,523  - max_epochs: \"10\"\n",
      "2019-07-22 04:00:29,524  - shuffle: \"True\"\n",
      "2019-07-22 04:00:29,525  - train_with_dev: \"False\"\n",
      "2019-07-22 04:00:29,525 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:00:29,526 Model training base path: \"flair_rnn\"\n",
      "2019-07-22 04:00:29,527 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:00:29,528 Device: cuda:0\n",
      "2019-07-22 04:00:29,529 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:00:29,530 Embedding storage mode: cpu\n",
      "2019-07-22 04:00:29,531 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:00:29,771 epoch 1 - iter 0/1065 - loss 0.71697867\n",
      "2019-07-22 04:00:46,573 epoch 1 - iter 106/1065 - loss 0.39584383\n",
      "2019-07-22 04:01:03,520 epoch 1 - iter 212/1065 - loss 0.38525563\n",
      "2019-07-22 04:01:36,681 epoch 1 - iter 318/1065 - loss 0.38102248\n",
      "2019-07-22 04:01:53,170 epoch 1 - iter 424/1065 - loss 0.37796114\n",
      "2019-07-22 04:02:11,695 epoch 1 - iter 530/1065 - loss 0.37297430\n",
      "2019-07-22 04:02:28,415 epoch 1 - iter 636/1065 - loss 0.37292372\n",
      "2019-07-22 04:02:46,593 epoch 1 - iter 742/1065 - loss 0.37001234\n",
      "2019-07-22 04:03:04,123 epoch 1 - iter 848/1065 - loss 0.36906210\n",
      "2019-07-22 04:03:24,429 epoch 1 - iter 954/1065 - loss 0.37027735\n",
      "2019-07-22 04:03:40,711 epoch 1 - iter 1060/1065 - loss 0.36767127\n",
      "2019-07-22 04:03:41,329 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:03:41,330 EPOCH 1 done: loss 0.3679 - lr 0.1000\n",
      "2019-07-22 04:04:00,505 DEV : loss 0.3532070815563202 - score 0.1001\n",
      "2019-07-22 04:04:07,975 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 04:04:11,524 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:04:11,566 epoch 2 - iter 0/1065 - loss 0.30223742\n",
      "2019-07-22 04:04:15,058 epoch 2 - iter 106/1065 - loss 0.36038454\n",
      "2019-07-22 04:04:18,506 epoch 2 - iter 212/1065 - loss 0.35472878\n",
      "2019-07-22 04:04:22,041 epoch 2 - iter 318/1065 - loss 0.35719530\n",
      "2019-07-22 04:04:25,482 epoch 2 - iter 424/1065 - loss 0.35433677\n",
      "2019-07-22 04:04:29,059 epoch 2 - iter 530/1065 - loss 0.35541551\n",
      "2019-07-22 04:04:32,545 epoch 2 - iter 636/1065 - loss 0.35463561\n",
      "2019-07-22 04:04:36,108 epoch 2 - iter 742/1065 - loss 0.35337699\n",
      "2019-07-22 04:04:39,556 epoch 2 - iter 848/1065 - loss 0.34951509\n",
      "2019-07-22 04:04:42,954 epoch 2 - iter 954/1065 - loss 0.34826944\n",
      "2019-07-22 04:04:46,461 epoch 2 - iter 1060/1065 - loss 0.34781102\n",
      "2019-07-22 04:04:46,608 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:04:46,609 EPOCH 2 done: loss 0.3475 - lr 0.1000\n",
      "2019-07-22 04:04:50,119 DEV : loss 0.34453338384628296 - score 0.0858\n",
      "2019-07-22 04:04:50,621 BAD EPOCHS (no improvement): 1\n",
      "2019-07-22 04:04:50,622 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:04:50,668 epoch 3 - iter 0/1065 - loss 0.08161123\n",
      "2019-07-22 04:04:54,268 epoch 3 - iter 106/1065 - loss 0.33274176\n",
      "2019-07-22 04:04:57,788 epoch 3 - iter 212/1065 - loss 0.33402880\n",
      "2019-07-22 04:05:01,212 epoch 3 - iter 318/1065 - loss 0.33780109\n",
      "2019-07-22 04:05:04,714 epoch 3 - iter 424/1065 - loss 0.33544124\n",
      "2019-07-22 04:05:08,190 epoch 3 - iter 530/1065 - loss 0.33574307\n",
      "2019-07-22 04:05:11,747 epoch 3 - iter 636/1065 - loss 0.33423237\n",
      "2019-07-22 04:05:15,286 epoch 3 - iter 742/1065 - loss 0.33644470\n",
      "2019-07-22 04:05:18,749 epoch 3 - iter 848/1065 - loss 0.33693728\n",
      "2019-07-22 04:05:22,284 epoch 3 - iter 954/1065 - loss 0.33755691\n",
      "2019-07-22 04:05:25,882 epoch 3 - iter 1060/1065 - loss 0.33598328\n",
      "2019-07-22 04:05:26,030 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:05:26,032 EPOCH 3 done: loss 0.3357 - lr 0.1000\n",
      "2019-07-22 04:05:29,769 DEV : loss 0.35708364844322205 - score 0.0587\n",
      "2019-07-22 04:05:30,247 BAD EPOCHS (no improvement): 2\n",
      "2019-07-22 04:05:30,248 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:05:30,285 epoch 4 - iter 0/1065 - loss 0.47227418\n",
      "2019-07-22 04:05:33,879 epoch 4 - iter 106/1065 - loss 0.32297500\n",
      "2019-07-22 04:05:37,494 epoch 4 - iter 212/1065 - loss 0.33161216\n",
      "2019-07-22 04:05:41,027 epoch 4 - iter 318/1065 - loss 0.32630636\n",
      "2019-07-22 04:05:44,640 epoch 4 - iter 424/1065 - loss 0.32840556\n",
      "2019-07-22 04:05:48,153 epoch 4 - iter 530/1065 - loss 0.33165247\n",
      "2019-07-22 04:05:51,563 epoch 4 - iter 636/1065 - loss 0.33042862\n",
      "2019-07-22 04:05:55,035 epoch 4 - iter 742/1065 - loss 0.33239045\n",
      "2019-07-22 04:05:58,562 epoch 4 - iter 848/1065 - loss 0.33086279\n",
      "2019-07-22 04:06:02,035 epoch 4 - iter 954/1065 - loss 0.33097519\n",
      "2019-07-22 04:06:05,767 epoch 4 - iter 1060/1065 - loss 0.33091111\n",
      "2019-07-22 04:06:05,924 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:06:05,925 EPOCH 4 done: loss 0.3308 - lr 0.1000\n",
      "2019-07-22 04:06:09,714 DEV : loss 0.32863762974739075 - score 0.3167\n",
      "2019-07-22 04:06:10,228 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 04:06:13,783 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:06:13,832 epoch 5 - iter 0/1065 - loss 0.35133940\n",
      "2019-07-22 04:06:17,569 epoch 5 - iter 106/1065 - loss 0.30968454\n",
      "2019-07-22 04:06:21,109 epoch 5 - iter 212/1065 - loss 0.31722393\n",
      "2019-07-22 04:06:24,678 epoch 5 - iter 318/1065 - loss 0.31818248\n",
      "2019-07-22 04:06:28,226 epoch 5 - iter 424/1065 - loss 0.31867800\n",
      "2019-07-22 04:06:31,656 epoch 5 - iter 530/1065 - loss 0.32416169\n",
      "2019-07-22 04:06:35,126 epoch 5 - iter 636/1065 - loss 0.32439272\n",
      "2019-07-22 04:06:38,680 epoch 5 - iter 742/1065 - loss 0.32482870\n",
      "2019-07-22 04:06:42,250 epoch 5 - iter 848/1065 - loss 0.32500438\n",
      "2019-07-22 04:06:45,737 epoch 5 - iter 954/1065 - loss 0.32518803\n",
      "2019-07-22 04:06:49,382 epoch 5 - iter 1060/1065 - loss 0.32646913\n",
      "2019-07-22 04:06:49,531 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:06:49,532 EPOCH 5 done: loss 0.3266 - lr 0.1000\n",
      "2019-07-22 04:06:53,275 DEV : loss 0.3483636677265167 - score 0.0202\n",
      "2019-07-22 04:06:53,789 BAD EPOCHS (no improvement): 1\n",
      "2019-07-22 04:06:53,790 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:06:53,829 epoch 6 - iter 0/1065 - loss 0.29777828\n",
      "2019-07-22 04:06:57,424 epoch 6 - iter 106/1065 - loss 0.31616796\n",
      "2019-07-22 04:07:00,928 epoch 6 - iter 212/1065 - loss 0.30954436\n",
      "2019-07-22 04:07:04,455 epoch 6 - iter 318/1065 - loss 0.31853026\n",
      "2019-07-22 04:07:07,981 epoch 6 - iter 424/1065 - loss 0.31777798\n",
      "2019-07-22 04:07:11,640 epoch 6 - iter 530/1065 - loss 0.32031835\n",
      "2019-07-22 04:07:15,106 epoch 6 - iter 636/1065 - loss 0.32171163\n",
      "2019-07-22 04:07:18,559 epoch 6 - iter 742/1065 - loss 0.32217125\n",
      "2019-07-22 04:07:22,018 epoch 6 - iter 848/1065 - loss 0.32130660\n",
      "2019-07-22 04:07:25,525 epoch 6 - iter 954/1065 - loss 0.32290519\n",
      "2019-07-22 04:07:29,125 epoch 6 - iter 1060/1065 - loss 0.32211131\n",
      "2019-07-22 04:07:29,278 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:07:29,279 EPOCH 6 done: loss 0.3221 - lr 0.1000\n",
      "2019-07-22 04:07:33,007 DEV : loss 0.31842419505119324 - score 0.3051\n",
      "2019-07-22 04:07:33,476 BAD EPOCHS (no improvement): 2\n",
      "2019-07-22 04:07:33,478 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:07:33,516 epoch 7 - iter 0/1065 - loss 0.23396115\n",
      "2019-07-22 04:07:37,151 epoch 7 - iter 106/1065 - loss 0.32623226\n",
      "2019-07-22 04:07:40,780 epoch 7 - iter 212/1065 - loss 0.32532029\n",
      "2019-07-22 04:07:44,448 epoch 7 - iter 318/1065 - loss 0.31917765\n",
      "2019-07-22 04:07:48,045 epoch 7 - iter 424/1065 - loss 0.31860317\n",
      "2019-07-22 04:07:51,474 epoch 7 - iter 530/1065 - loss 0.31785295\n",
      "2019-07-22 04:07:54,909 epoch 7 - iter 636/1065 - loss 0.32079883\n",
      "2019-07-22 04:07:58,421 epoch 7 - iter 742/1065 - loss 0.31885311\n",
      "2019-07-22 04:08:01,981 epoch 7 - iter 848/1065 - loss 0.32024579\n",
      "2019-07-22 04:08:05,526 epoch 7 - iter 954/1065 - loss 0.32087414\n",
      "2019-07-22 04:08:09,195 epoch 7 - iter 1060/1065 - loss 0.31946803\n",
      "2019-07-22 04:08:09,369 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:08:09,370 EPOCH 7 done: loss 0.3195 - lr 0.1000\n",
      "2019-07-22 04:08:13,133 DEV : loss 0.3209840953350067 - score 0.2009\n",
      "2019-07-22 04:08:13,650 BAD EPOCHS (no improvement): 3\n",
      "2019-07-22 04:08:13,651 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:08:13,688 epoch 8 - iter 0/1065 - loss 0.08575824\n",
      "2019-07-22 04:08:17,344 epoch 8 - iter 106/1065 - loss 0.32727436\n",
      "2019-07-22 04:08:20,930 epoch 8 - iter 212/1065 - loss 0.31455543\n",
      "2019-07-22 04:08:24,453 epoch 8 - iter 318/1065 - loss 0.31370052\n",
      "2019-07-22 04:08:28,033 epoch 8 - iter 424/1065 - loss 0.31409459\n",
      "2019-07-22 04:08:31,609 epoch 8 - iter 530/1065 - loss 0.31674290\n",
      "2019-07-22 04:08:35,156 epoch 8 - iter 636/1065 - loss 0.31534873\n",
      "2019-07-22 04:08:38,682 epoch 8 - iter 742/1065 - loss 0.31474659\n",
      "2019-07-22 04:08:42,291 epoch 8 - iter 848/1065 - loss 0.31443979\n",
      "2019-07-22 04:08:45,847 epoch 8 - iter 954/1065 - loss 0.31605055\n",
      "2019-07-22 04:08:49,535 epoch 8 - iter 1060/1065 - loss 0.31534385\n",
      "2019-07-22 04:08:49,690 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:08:49,692 EPOCH 8 done: loss 0.3155 - lr 0.1000\n",
      "2019-07-22 04:08:53,484 DEV : loss 0.31844088435173035 - score 0.1502\n",
      "Epoch     7: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-07-22 04:08:53,995 BAD EPOCHS (no improvement): 4\n",
      "2019-07-22 04:08:53,997 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:08:54,038 epoch 9 - iter 0/1065 - loss 0.41295421\n",
      "2019-07-22 04:08:57,633 epoch 9 - iter 106/1065 - loss 0.30033069\n",
      "2019-07-22 04:09:01,247 epoch 9 - iter 212/1065 - loss 0.30407640\n",
      "2019-07-22 04:09:04,798 epoch 9 - iter 318/1065 - loss 0.31099269\n",
      "2019-07-22 04:09:08,287 epoch 9 - iter 424/1065 - loss 0.30671896\n",
      "2019-07-22 04:09:11,841 epoch 9 - iter 530/1065 - loss 0.30556380\n",
      "2019-07-22 04:09:15,316 epoch 9 - iter 636/1065 - loss 0.30644969\n",
      "2019-07-22 04:09:18,821 epoch 9 - iter 742/1065 - loss 0.30621005\n",
      "2019-07-22 04:09:22,346 epoch 9 - iter 848/1065 - loss 0.30313624\n",
      "2019-07-22 04:09:25,916 epoch 9 - iter 954/1065 - loss 0.30370158\n",
      "2019-07-22 04:09:29,603 epoch 9 - iter 1060/1065 - loss 0.30425255\n",
      "2019-07-22 04:09:29,751 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:09:29,752 EPOCH 9 done: loss 0.3038 - lr 0.0500\n",
      "2019-07-22 04:09:33,561 DEV : loss 0.32166430354118347 - score 0.195\n",
      "2019-07-22 04:09:34,073 BAD EPOCHS (no improvement): 1\n",
      "2019-07-22 04:09:34,075 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:09:34,120 epoch 10 - iter 0/1065 - loss 0.37681454\n",
      "2019-07-22 04:09:37,768 epoch 10 - iter 106/1065 - loss 0.29064316\n",
      "2019-07-22 04:09:41,443 epoch 10 - iter 212/1065 - loss 0.29390306\n",
      "2019-07-22 04:09:44,971 epoch 10 - iter 318/1065 - loss 0.29577083\n",
      "2019-07-22 04:09:48,572 epoch 10 - iter 424/1065 - loss 0.29963113\n",
      "2019-07-22 04:09:52,083 epoch 10 - iter 530/1065 - loss 0.30291720\n",
      "2019-07-22 04:09:55,671 epoch 10 - iter 636/1065 - loss 0.30074662\n",
      "2019-07-22 04:09:59,266 epoch 10 - iter 742/1065 - loss 0.30041449\n",
      "2019-07-22 04:10:02,797 epoch 10 - iter 848/1065 - loss 0.30100073\n",
      "2019-07-22 04:10:06,344 epoch 10 - iter 954/1065 - loss 0.30327278\n",
      "2019-07-22 04:10:09,898 epoch 10 - iter 1060/1065 - loss 0.30173496\n",
      "2019-07-22 04:10:10,046 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:10:10,047 EPOCH 10 done: loss 0.3021 - lr 0.0500\n",
      "2019-07-22 04:10:13,654 DEV : loss 0.3139718472957611 - score 0.4215\n",
      "2019-07-22 04:10:14,090 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 04:10:20,893 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 04:10:20,894 Testing using best model ...\n",
      "2019-07-22 04:10:20,895 loading file flair_rnn/best-model.pt\n",
      "2019-07-22 04:10:42,912 0.5032\t0.3179\t0.3896\n",
      "2019-07-22 04:10:42,914 \n",
      "MICRO_AVG: acc 0.7713 - f1-score 0.8709\n",
      "MACRO_AVG: acc 0.5537 - f1-score 0.6587\n",
      "0          tp: 4711 - fp: 502 - fn: 231 - tn: 234 - precision: 0.9037 - recall: 0.9533 - accuracy: 0.8654 - f1-score: 0.9278\n",
      "1          tp: 234 - fp: 231 - fn: 502 - tn: 4711 - precision: 0.5032 - recall: 0.3179 - accuracy: 0.2420 - f1-score: 0.3896\n",
      "2019-07-22 04:10:42,914 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dev_loss_history': [tensor(0.3532, device='cuda:0'),\n",
       "  tensor(0.3445, device='cuda:0'),\n",
       "  tensor(0.3571, device='cuda:0'),\n",
       "  tensor(0.3286, device='cuda:0'),\n",
       "  tensor(0.3484, device='cuda:0'),\n",
       "  tensor(0.3184, device='cuda:0'),\n",
       "  tensor(0.3210, device='cuda:0'),\n",
       "  tensor(0.3184, device='cuda:0'),\n",
       "  tensor(0.3217, device='cuda:0'),\n",
       "  tensor(0.3140, device='cuda:0')],\n",
       " 'dev_score_history': [0.1001,\n",
       "  0.0858,\n",
       "  0.0587,\n",
       "  0.3167,\n",
       "  0.0202,\n",
       "  0.3051,\n",
       "  0.2009,\n",
       "  0.1502,\n",
       "  0.195,\n",
       "  0.4215],\n",
       " 'test_score': 0.3896,\n",
       " 'train_loss_history': [0.36786057157135904,\n",
       "  0.34752518860666964,\n",
       "  0.33568752971893184,\n",
       "  0.3308070373787007,\n",
       "  0.3266023180084609,\n",
       "  0.3220671186984425,\n",
       "  0.3195135344781786,\n",
       "  0.31547420419787575,\n",
       "  0.3038143427718973,\n",
       "  0.3021200100790727]}"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.embeddings import DocumentRNNEmbeddings\n",
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'),\n",
    "                                                       test_file='flair_test.csv',\n",
    "                                                       dev_file='flair_dev.csv',\n",
    "                                                       train_file='flair_train.csv')\n",
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "                   FlairEmbeddings('news-forward-fast'),\n",
    "                   FlairEmbeddings('news-backward-fast')]\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512,\n",
    "                                             reproject_words=True, reproject_words_dimension=256)\n",
    "\n",
    "classifier = TextClassifier(document_embeddings,\n",
    "                            label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train(max_epochs=10, base_path = \"flair_rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Gh_FtqT24k_f",
    "outputId": "45582706-c86f-4052-d07a-2126eba0b416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:43:02,549 Reading data from .\n",
      "2019-07-22 23:43:02,550 Train: flair_train.csv\n",
      "2019-07-22 23:43:02,551 Dev: flair_dev.csv\n",
      "2019-07-22 23:43:02,552 Test: flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:447: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:454: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:463: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:43:19,538 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/en-fasttext-news-300d-1M.vectors.npy not found in cache, downloading to /tmp/tmp0yy6pgle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200000128/1200000128 [00:55<00:00, 21556953.77B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:44:15,765 copying /tmp/tmp0yy6pgle to cache at /root/.flair/embeddings/en-fasttext-news-300d-1M.vectors.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:44:19,131 removing temp file /tmp/tmp0yy6pgle\n",
      "2019-07-22 23:44:19,735 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/en-fasttext-news-300d-1M not found in cache, downloading to /tmp/tmpm3sj67a1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54600983/54600983 [00:10<00:00, 5084712.60B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:44:31,020 copying /tmp/tmpm3sj67a1 to cache at /root/.flair/embeddings/en-fasttext-news-300d-1M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:44:31,091 removing temp file /tmp/tmpm3sj67a1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Classifier *************************\n",
      "2019-07-22 23:44:43,637 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34066/34066 [00:00<00:00, 273660.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:44:43,777 [b'0', b'1']\n",
      "Create Trainer *******************************\n",
      "Begin Training *******************************\n",
      "2019-07-22 23:44:43,781 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:44:43,782 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentPoolEmbeddings(\n",
      "    fine_tune_mode=linear, pooling=mean\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('news')\n",
      "    )\n",
      "    (embedding_flex): Linear(in_features=300, out_features=300, bias=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=300, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-07-22 23:44:43,783 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:44:43,784 Corpus: \"Corpus: 34066 train + 5678 dev + 5678 test sentences\"\n",
      "2019-07-22 23:44:43,785 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:44:43,786 Parameters:\n",
      "2019-07-22 23:44:43,786  - learning_rate: \"0.1\"\n",
      "2019-07-22 23:44:43,787  - mini_batch_size: \"32\"\n",
      "2019-07-22 23:44:43,788  - patience: \"3\"\n",
      "2019-07-22 23:44:43,788  - anneal_factor: \"0.5\"\n",
      "2019-07-22 23:44:43,789  - max_epochs: \"10\"\n",
      "2019-07-22 23:44:43,790  - shuffle: \"True\"\n",
      "2019-07-22 23:44:43,791  - train_with_dev: \"False\"\n",
      "2019-07-22 23:44:43,792 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:44:43,793 Model training base path: \"fasttext_pooled\"\n",
      "2019-07-22 23:44:43,794 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:44:43,795 Device: cuda:0\n",
      "2019-07-22 23:44:43,796 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:44:43,798 Embedding storage mode: cpu\n",
      "2019-07-22 23:44:43,800 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:44:43,957 epoch 1 - iter 0/1065 - loss 0.69401217\n",
      "2019-07-22 23:44:49,305 epoch 1 - iter 106/1065 - loss 0.39745150\n",
      "2019-07-22 23:44:54,584 epoch 1 - iter 212/1065 - loss 0.39613033\n",
      "2019-07-22 23:44:59,789 epoch 1 - iter 318/1065 - loss 0.38813759\n",
      "2019-07-22 23:45:04,995 epoch 1 - iter 424/1065 - loss 0.38154050\n",
      "2019-07-22 23:45:11,743 epoch 1 - iter 530/1065 - loss 0.37667488\n",
      "2019-07-22 23:45:17,060 epoch 1 - iter 636/1065 - loss 0.37362134\n",
      "2019-07-22 23:45:22,274 epoch 1 - iter 742/1065 - loss 0.37297126\n",
      "2019-07-22 23:45:27,909 epoch 1 - iter 848/1065 - loss 0.36948554\n",
      "2019-07-22 23:45:33,196 epoch 1 - iter 954/1065 - loss 0.36840121\n",
      "2019-07-22 23:45:38,486 epoch 1 - iter 1060/1065 - loss 0.36774734\n",
      "2019-07-22 23:45:38,665 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:45:38,667 EPOCH 1 done: loss 0.3673 - lr 0.1000\n",
      "2019-07-22 23:45:47,216 DEV : loss 0.35957905650138855 - score 0.0101\n",
      "2019-07-22 23:45:49,919 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 23:46:04,169 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:46:04,196 epoch 2 - iter 0/1065 - loss 0.34704548\n",
      "2019-07-22 23:46:07,079 epoch 2 - iter 106/1065 - loss 0.34655996\n",
      "2019-07-22 23:46:09,222 epoch 2 - iter 212/1065 - loss 0.34859594\n",
      "2019-07-22 23:46:11,293 epoch 2 - iter 318/1065 - loss 0.34352397\n",
      "2019-07-22 23:46:13,449 epoch 2 - iter 424/1065 - loss 0.34284883\n",
      "2019-07-22 23:46:15,678 epoch 2 - iter 530/1065 - loss 0.34369203\n",
      "2019-07-22 23:46:18,000 epoch 2 - iter 636/1065 - loss 0.34144319\n",
      "2019-07-22 23:46:20,293 epoch 2 - iter 742/1065 - loss 0.34143377\n",
      "2019-07-22 23:46:22,514 epoch 2 - iter 848/1065 - loss 0.33972695\n",
      "2019-07-22 23:46:24,742 epoch 2 - iter 954/1065 - loss 0.33774148\n",
      "2019-07-22 23:46:26,875 epoch 2 - iter 1060/1065 - loss 0.33714413\n",
      "2019-07-22 23:46:26,948 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:46:26,949 EPOCH 2 done: loss 0.3368 - lr 0.1000\n",
      "2019-07-22 23:46:29,756 DEV : loss 0.350004643201828 - score 0.0151\n",
      "2019-07-22 23:46:29,995 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 23:46:43,914 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:46:43,941 epoch 3 - iter 0/1065 - loss 0.17419717\n",
      "2019-07-22 23:46:46,088 epoch 3 - iter 106/1065 - loss 0.32025411\n",
      "2019-07-22 23:46:48,242 epoch 3 - iter 212/1065 - loss 0.32386021\n",
      "2019-07-22 23:46:50,326 epoch 3 - iter 318/1065 - loss 0.32665852\n",
      "2019-07-22 23:46:52,439 epoch 3 - iter 424/1065 - loss 0.32633386\n",
      "2019-07-22 23:46:54,545 epoch 3 - iter 530/1065 - loss 0.32415458\n",
      "2019-07-22 23:46:56,698 epoch 3 - iter 636/1065 - loss 0.32300663\n",
      "2019-07-22 23:46:58,873 epoch 3 - iter 742/1065 - loss 0.32444810\n",
      "2019-07-22 23:47:01,018 epoch 3 - iter 848/1065 - loss 0.32509770\n",
      "2019-07-22 23:47:03,199 epoch 3 - iter 954/1065 - loss 0.32440632\n",
      "2019-07-22 23:47:05,428 epoch 3 - iter 1060/1065 - loss 0.32238685\n",
      "2019-07-22 23:47:05,514 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:47:05,515 EPOCH 3 done: loss 0.3223 - lr 0.1000\n",
      "2019-07-22 23:47:08,284 DEV : loss 0.3224788308143616 - score 0.1746\n",
      "2019-07-22 23:47:08,546 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 23:47:21,864 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:47:21,894 epoch 4 - iter 0/1065 - loss 0.35565060\n",
      "2019-07-22 23:47:24,128 epoch 4 - iter 106/1065 - loss 0.30914591\n",
      "2019-07-22 23:47:26,332 epoch 4 - iter 212/1065 - loss 0.30762350\n",
      "2019-07-22 23:47:28,541 epoch 4 - iter 318/1065 - loss 0.31513113\n",
      "2019-07-22 23:47:30,668 epoch 4 - iter 424/1065 - loss 0.31516736\n",
      "2019-07-22 23:47:32,798 epoch 4 - iter 530/1065 - loss 0.31464684\n",
      "2019-07-22 23:47:34,965 epoch 4 - iter 636/1065 - loss 0.31607395\n",
      "2019-07-22 23:47:37,182 epoch 4 - iter 742/1065 - loss 0.31473744\n",
      "2019-07-22 23:47:39,335 epoch 4 - iter 848/1065 - loss 0.31564666\n",
      "2019-07-22 23:47:41,505 epoch 4 - iter 954/1065 - loss 0.31547435\n",
      "2019-07-22 23:47:43,657 epoch 4 - iter 1060/1065 - loss 0.31664227\n",
      "2019-07-22 23:47:43,727 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:47:43,728 EPOCH 4 done: loss 0.3166 - lr 0.1000\n",
      "2019-07-22 23:47:46,502 DEV : loss 0.3197724521160126 - score 0.175\n",
      "2019-07-22 23:47:46,748 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 23:47:59,729 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:47:59,760 epoch 5 - iter 0/1065 - loss 0.27244526\n",
      "2019-07-22 23:48:01,930 epoch 5 - iter 106/1065 - loss 0.31766631\n",
      "2019-07-22 23:48:04,035 epoch 5 - iter 212/1065 - loss 0.30835033\n",
      "2019-07-22 23:48:06,176 epoch 5 - iter 318/1065 - loss 0.30626811\n",
      "2019-07-22 23:48:08,283 epoch 5 - iter 424/1065 - loss 0.31567608\n",
      "2019-07-22 23:48:10,420 epoch 5 - iter 530/1065 - loss 0.31449838\n",
      "2019-07-22 23:48:12,538 epoch 5 - iter 636/1065 - loss 0.31262714\n",
      "2019-07-22 23:48:14,733 epoch 5 - iter 742/1065 - loss 0.31319585\n",
      "2019-07-22 23:48:16,921 epoch 5 - iter 848/1065 - loss 0.31136795\n",
      "2019-07-22 23:48:19,071 epoch 5 - iter 954/1065 - loss 0.31288475\n",
      "2019-07-22 23:48:21,207 epoch 5 - iter 1060/1065 - loss 0.31294949\n",
      "2019-07-22 23:48:21,281 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:48:21,282 EPOCH 5 done: loss 0.3130 - lr 0.1000\n",
      "2019-07-22 23:48:23,992 DEV : loss 0.31536415219306946 - score 0.2253\n",
      "2019-07-22 23:48:24,251 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 23:48:37,403 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:48:37,430 epoch 6 - iter 0/1065 - loss 0.27181810\n",
      "2019-07-22 23:48:39,597 epoch 6 - iter 106/1065 - loss 0.32204584\n",
      "2019-07-22 23:48:41,692 epoch 6 - iter 212/1065 - loss 0.32583833\n",
      "2019-07-22 23:48:43,912 epoch 6 - iter 318/1065 - loss 0.31883948\n",
      "2019-07-22 23:48:45,965 epoch 6 - iter 424/1065 - loss 0.31688579\n",
      "2019-07-22 23:48:48,087 epoch 6 - iter 530/1065 - loss 0.31519163\n",
      "2019-07-22 23:48:50,149 epoch 6 - iter 636/1065 - loss 0.31241791\n",
      "2019-07-22 23:48:52,299 epoch 6 - iter 742/1065 - loss 0.31373493\n",
      "2019-07-22 23:48:54,483 epoch 6 - iter 848/1065 - loss 0.31279514\n",
      "2019-07-22 23:48:56,688 epoch 6 - iter 954/1065 - loss 0.31260241\n",
      "2019-07-22 23:48:58,842 epoch 6 - iter 1060/1065 - loss 0.31054493\n",
      "2019-07-22 23:48:58,928 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:48:58,930 EPOCH 6 done: loss 0.3102 - lr 0.1000\n",
      "2019-07-22 23:49:01,636 DEV : loss 0.3259889483451843 - score 0.1332\n",
      "2019-07-22 23:49:01,869 BAD EPOCHS (no improvement): 1\n",
      "2019-07-22 23:49:01,871 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:49:01,902 epoch 7 - iter 0/1065 - loss 0.19690756\n",
      "2019-07-22 23:49:04,038 epoch 7 - iter 106/1065 - loss 0.31787315\n",
      "2019-07-22 23:49:06,224 epoch 7 - iter 212/1065 - loss 0.30776099\n",
      "2019-07-22 23:49:08,429 epoch 7 - iter 318/1065 - loss 0.30502370\n",
      "2019-07-22 23:49:10,564 epoch 7 - iter 424/1065 - loss 0.30577614\n",
      "2019-07-22 23:49:12,671 epoch 7 - iter 530/1065 - loss 0.30833577\n",
      "2019-07-22 23:49:14,776 epoch 7 - iter 636/1065 - loss 0.30999143\n",
      "2019-07-22 23:49:16,975 epoch 7 - iter 742/1065 - loss 0.31024234\n",
      "2019-07-22 23:49:19,162 epoch 7 - iter 848/1065 - loss 0.30773207\n",
      "2019-07-22 23:49:21,360 epoch 7 - iter 954/1065 - loss 0.30784668\n",
      "2019-07-22 23:49:23,551 epoch 7 - iter 1060/1065 - loss 0.30864332\n",
      "2019-07-22 23:49:23,631 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:49:23,632 EPOCH 7 done: loss 0.3084 - lr 0.1000\n",
      "2019-07-22 23:49:26,529 DEV : loss 0.32585781812667847 - score 0.1349\n",
      "2019-07-22 23:49:26,791 BAD EPOCHS (no improvement): 2\n",
      "2019-07-22 23:49:26,792 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:49:26,819 epoch 8 - iter 0/1065 - loss 0.27911419\n",
      "2019-07-22 23:49:28,991 epoch 8 - iter 106/1065 - loss 0.29889066\n",
      "2019-07-22 23:49:31,108 epoch 8 - iter 212/1065 - loss 0.30604222\n",
      "2019-07-22 23:49:33,399 epoch 8 - iter 318/1065 - loss 0.30855588\n",
      "2019-07-22 23:49:35,656 epoch 8 - iter 424/1065 - loss 0.30988967\n",
      "2019-07-22 23:49:37,881 epoch 8 - iter 530/1065 - loss 0.30965358\n",
      "2019-07-22 23:49:40,116 epoch 8 - iter 636/1065 - loss 0.31054987\n",
      "2019-07-22 23:49:42,361 epoch 8 - iter 742/1065 - loss 0.30834645\n",
      "2019-07-22 23:49:44,560 epoch 8 - iter 848/1065 - loss 0.30719736\n",
      "2019-07-22 23:49:46,786 epoch 8 - iter 954/1065 - loss 0.30721675\n",
      "2019-07-22 23:49:49,050 epoch 8 - iter 1060/1065 - loss 0.30769179\n",
      "2019-07-22 23:49:49,138 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:49:49,139 EPOCH 8 done: loss 0.3080 - lr 0.1000\n",
      "2019-07-22 23:49:52,014 DEV : loss 0.31235218048095703 - score 0.3033\n",
      "2019-07-22 23:49:52,284 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 23:50:05,929 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:50:05,960 epoch 9 - iter 0/1065 - loss 0.19856194\n",
      "2019-07-22 23:50:08,137 epoch 9 - iter 106/1065 - loss 0.29615372\n",
      "2019-07-22 23:50:10,326 epoch 9 - iter 212/1065 - loss 0.30292978\n",
      "2019-07-22 23:50:12,532 epoch 9 - iter 318/1065 - loss 0.30926533\n",
      "2019-07-22 23:50:14,650 epoch 9 - iter 424/1065 - loss 0.30549317\n",
      "2019-07-22 23:50:16,810 epoch 9 - iter 530/1065 - loss 0.30639012\n",
      "2019-07-22 23:50:19,056 epoch 9 - iter 636/1065 - loss 0.30602493\n",
      "2019-07-22 23:50:21,245 epoch 9 - iter 742/1065 - loss 0.30694391\n",
      "2019-07-22 23:50:23,485 epoch 9 - iter 848/1065 - loss 0.30658355\n",
      "2019-07-22 23:50:25,762 epoch 9 - iter 954/1065 - loss 0.30606787\n",
      "2019-07-22 23:50:27,992 epoch 9 - iter 1060/1065 - loss 0.30691610\n",
      "2019-07-22 23:50:28,075 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:50:28,076 EPOCH 9 done: loss 0.3071 - lr 0.1000\n",
      "2019-07-22 23:50:30,795 DEV : loss 0.311906635761261 - score 0.2354\n",
      "2019-07-22 23:50:31,019 BAD EPOCHS (no improvement): 1\n",
      "2019-07-22 23:50:31,020 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:50:31,048 epoch 10 - iter 0/1065 - loss 0.38763377\n",
      "2019-07-22 23:50:33,149 epoch 10 - iter 106/1065 - loss 0.31278341\n",
      "2019-07-22 23:50:35,250 epoch 10 - iter 212/1065 - loss 0.31129861\n",
      "2019-07-22 23:50:37,313 epoch 10 - iter 318/1065 - loss 0.30986859\n",
      "2019-07-22 23:50:39,451 epoch 10 - iter 424/1065 - loss 0.30760684\n",
      "2019-07-22 23:50:41,520 epoch 10 - iter 530/1065 - loss 0.30688396\n",
      "2019-07-22 23:50:43,690 epoch 10 - iter 636/1065 - loss 0.30656288\n",
      "2019-07-22 23:50:45,802 epoch 10 - iter 742/1065 - loss 0.30716724\n",
      "2019-07-22 23:50:47,965 epoch 10 - iter 848/1065 - loss 0.30775676\n",
      "2019-07-22 23:50:50,108 epoch 10 - iter 954/1065 - loss 0.30607700\n",
      "2019-07-22 23:50:52,284 epoch 10 - iter 1060/1065 - loss 0.30602403\n",
      "2019-07-22 23:50:52,369 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:50:52,370 EPOCH 10 done: loss 0.3060 - lr 0.1000\n",
      "2019-07-22 23:50:55,181 DEV : loss 0.3213725984096527 - score 0.1648\n",
      "2019-07-22 23:50:55,447 BAD EPOCHS (no improvement): 2\n",
      "2019-07-22 23:51:08,679 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:51:08,680 Testing using best model ...\n",
      "2019-07-22 23:51:08,682 loading file fasttext_pooled/best-model.pt\n",
      "2019-07-22 23:51:21,321 0.5804\t0.2011\t0.2987\n",
      "2019-07-22 23:51:21,323 \n",
      "MICRO_AVG: acc 0.7819 - f1-score 0.8776\n",
      "MACRO_AVG: acc 0.525 - f1-score 0.6158\n",
      "0          tp: 4835 - fp: 588 - fn: 107 - tn: 148 - precision: 0.8916 - recall: 0.9783 - accuracy: 0.8743 - f1-score: 0.9329\n",
      "1          tp: 148 - fp: 107 - fn: 588 - tn: 4835 - precision: 0.5804 - recall: 0.2011 - accuracy: 0.1756 - f1-score: 0.2987\n",
      "2019-07-22 23:51:21,324 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dev_loss_history': [tensor(0.3596, device='cuda:0'),\n",
       "  tensor(0.3500, device='cuda:0'),\n",
       "  tensor(0.3225, device='cuda:0'),\n",
       "  tensor(0.3198, device='cuda:0'),\n",
       "  tensor(0.3154, device='cuda:0'),\n",
       "  tensor(0.3260, device='cuda:0'),\n",
       "  tensor(0.3259, device='cuda:0'),\n",
       "  tensor(0.3124, device='cuda:0'),\n",
       "  tensor(0.3119, device='cuda:0'),\n",
       "  tensor(0.3214, device='cuda:0')],\n",
       " 'dev_score_history': [0.0101,\n",
       "  0.0151,\n",
       "  0.1746,\n",
       "  0.175,\n",
       "  0.2253,\n",
       "  0.1332,\n",
       "  0.1349,\n",
       "  0.3033,\n",
       "  0.2354,\n",
       "  0.1648],\n",
       " 'test_score': 0.2987,\n",
       " 'train_loss_history': [0.36731406753191925,\n",
       "  0.3367832300948425,\n",
       "  0.3223428954191051,\n",
       "  0.3166024279286604,\n",
       "  0.3130212271087606,\n",
       "  0.3102030845874912,\n",
       "  0.30843144180209425,\n",
       "  0.3080318818000001,\n",
       "  0.30708569089221843,\n",
       "  0.3059988208849665]}"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.embeddings import BertEmbeddings, ELMoEmbeddings\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'),\n",
    "                                                       test_file='flair_test.csv',\n",
    "                                                       dev_file='flair_dev.csv',\n",
    "                                                       train_file='flair_train.csv')\n",
    "word_embeddings = [#BertEmbeddings(),\n",
    "                   WordEmbeddings(\"news\")] # bert and fasttext\n",
    "document_embeddings = DocumentPoolEmbeddings(word_embeddings)\n",
    "\n",
    "print(\"Create Classifier *************************\")\n",
    "classifier = TextClassifier(document_embeddings,\n",
    "                            label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "print(\"Create Trainer *******************************\")\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "print(\"Begin Training *******************************\")\n",
    "trainer.train(max_epochs=10, base_path = \"fasttext_pooled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "E5K1g5iHG82o",
    "outputId": "aa177f0b-9aae-4ef6-ed74-b070c51e482a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:54:58,872 Reading data from .\n",
      "2019-07-22 23:54:58,874 Train: flair_train.csv\n",
      "2019-07-22 23:54:58,875 Dev: flair_dev.csv\n",
      "2019-07-22 23:54:58,876 Test: flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:447: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:454: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:463: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:55:14,736 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmplmiuwpxh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000128/160000128 [00:08<00:00, 18407355.87B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:55:23,947 copying /tmp/tmplmiuwpxh to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:55:24,209 removing temp file /tmp/tmplmiuwpxh\n",
      "2019-07-22 23:55:25,307 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /tmp/tmp2pscfuag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21494764/21494764 [00:01<00:00, 12583532.55B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:55:27,539 copying /tmp/tmp2pscfuag to cache at /root/.flair/embeddings/glove.gensim\n",
      "2019-07-22 23:55:27,567 removing temp file /tmp/tmp2pscfuag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Classifier *************************\n",
      "2019-07-22 23:55:28,475 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34066/34066 [00:00<00:00, 250128.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 23:55:28,615 [b'0', b'1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Trainer *******************************\n",
      "Begin Training *******************************\n",
      "2019-07-22 23:55:28,979 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:55:28,981 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentPoolEmbeddings(\n",
      "    fine_tune_mode=linear, pooling=mean\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "    )\n",
      "    (embedding_flex): Linear(in_features=100, out_features=100, bias=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-07-22 23:55:28,982 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:55:28,983 Corpus: \"Corpus: 34066 train + 5678 dev + 5678 test sentences\"\n",
      "2019-07-22 23:55:28,985 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:55:28,986 Parameters:\n",
      "2019-07-22 23:55:28,987  - learning_rate: \"0.1\"\n",
      "2019-07-22 23:55:28,988  - mini_batch_size: \"32\"\n",
      "2019-07-22 23:55:28,989  - patience: \"3\"\n",
      "2019-07-22 23:55:28,990  - anneal_factor: \"0.5\"\n",
      "2019-07-22 23:55:28,991  - max_epochs: \"10\"\n",
      "2019-07-22 23:55:28,992  - shuffle: \"True\"\n",
      "2019-07-22 23:55:28,993  - train_with_dev: \"False\"\n",
      "2019-07-22 23:55:28,994 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:55:28,996 Model training base path: \"glove_pooled\"\n",
      "2019-07-22 23:55:28,997 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:55:28,998 Device: cuda:0\n",
      "2019-07-22 23:55:28,999 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:55:29,000 Embedding storage mode: cpu\n",
      "2019-07-22 23:55:29,002 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:55:29,082 epoch 1 - iter 0/1065 - loss 0.47407722\n",
      "2019-07-22 23:55:34,413 epoch 1 - iter 106/1065 - loss 0.37933892\n",
      "2019-07-22 23:55:39,549 epoch 1 - iter 212/1065 - loss 0.37630068\n",
      "2019-07-22 23:55:44,642 epoch 1 - iter 318/1065 - loss 0.37417412\n",
      "2019-07-22 23:55:58,697 epoch 1 - iter 424/1065 - loss 0.37067737\n",
      "2019-07-22 23:56:03,803 epoch 1 - iter 530/1065 - loss 0.36854811\n",
      "2019-07-22 23:56:08,984 epoch 1 - iter 636/1065 - loss 0.36562941\n",
      "2019-07-22 23:56:14,336 epoch 1 - iter 742/1065 - loss 0.36612532\n",
      "2019-07-22 23:56:19,728 epoch 1 - iter 848/1065 - loss 0.36306159\n",
      "2019-07-22 23:56:26,794 epoch 1 - iter 954/1065 - loss 0.35847722\n",
      "2019-07-22 23:56:32,066 epoch 1 - iter 1060/1065 - loss 0.35614858\n",
      "2019-07-22 23:56:32,258 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:56:32,259 EPOCH 1 done: loss 0.3560 - lr 0.1000\n",
      "2019-07-22 23:56:38,167 DEV : loss 0.3487299978733063 - score 0.0584\n",
      "2019-07-22 23:56:40,755 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 23:56:44,375 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:56:44,397 epoch 2 - iter 0/1065 - loss 0.46448153\n",
      "2019-07-22 23:56:46,417 epoch 2 - iter 106/1065 - loss 0.33617453\n",
      "2019-07-22 23:56:48,314 epoch 2 - iter 212/1065 - loss 0.32354798\n",
      "2019-07-22 23:56:50,103 epoch 2 - iter 318/1065 - loss 0.32616865\n",
      "2019-07-22 23:56:52,095 epoch 2 - iter 424/1065 - loss 0.33120874\n",
      "2019-07-22 23:56:54,000 epoch 2 - iter 530/1065 - loss 0.33323381\n",
      "2019-07-22 23:56:56,001 epoch 2 - iter 636/1065 - loss 0.33582188\n",
      "2019-07-22 23:56:57,952 epoch 2 - iter 742/1065 - loss 0.33749195\n",
      "2019-07-22 23:56:59,924 epoch 2 - iter 848/1065 - loss 0.33961119\n",
      "2019-07-22 23:57:01,930 epoch 2 - iter 954/1065 - loss 0.33977168\n",
      "2019-07-22 23:57:03,817 epoch 2 - iter 1060/1065 - loss 0.34063439\n",
      "2019-07-22 23:57:03,885 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:57:03,887 EPOCH 2 done: loss 0.3408 - lr 0.1000\n",
      "2019-07-22 23:57:06,310 DEV : loss 0.3407863974571228 - score 0.1454\n",
      "2019-07-22 23:57:06,579 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 23:57:10,130 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:57:10,159 epoch 3 - iter 0/1065 - loss 0.22999910\n",
      "2019-07-22 23:57:12,159 epoch 3 - iter 106/1065 - loss 0.33442709\n",
      "2019-07-22 23:57:14,235 epoch 3 - iter 212/1065 - loss 0.33513266\n",
      "2019-07-22 23:57:16,297 epoch 3 - iter 318/1065 - loss 0.34073129\n",
      "2019-07-22 23:57:18,250 epoch 3 - iter 424/1065 - loss 0.34098836\n",
      "2019-07-22 23:57:20,276 epoch 3 - iter 530/1065 - loss 0.34242628\n",
      "2019-07-22 23:57:22,324 epoch 3 - iter 636/1065 - loss 0.34058629\n",
      "2019-07-22 23:57:24,423 epoch 3 - iter 742/1065 - loss 0.33841437\n",
      "2019-07-22 23:57:26,396 epoch 3 - iter 848/1065 - loss 0.33784283\n",
      "2019-07-22 23:57:28,259 epoch 3 - iter 954/1065 - loss 0.33684930\n",
      "2019-07-22 23:57:30,223 epoch 3 - iter 1060/1065 - loss 0.33680004\n",
      "2019-07-22 23:57:30,295 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:57:30,296 EPOCH 3 done: loss 0.3369 - lr 0.1000\n",
      "2019-07-22 23:57:32,782 DEV : loss 0.34672239422798157 - score 0.0696\n",
      "2019-07-22 23:57:33,019 BAD EPOCHS (no improvement): 1\n",
      "2019-07-22 23:57:33,020 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:57:33,044 epoch 4 - iter 0/1065 - loss 0.24688275\n",
      "2019-07-22 23:57:34,944 epoch 4 - iter 106/1065 - loss 0.34041306\n",
      "2019-07-22 23:57:36,899 epoch 4 - iter 212/1065 - loss 0.33003495\n",
      "2019-07-22 23:57:38,930 epoch 4 - iter 318/1065 - loss 0.33179412\n",
      "2019-07-22 23:57:41,033 epoch 4 - iter 424/1065 - loss 0.33160112\n",
      "2019-07-22 23:57:42,868 epoch 4 - iter 530/1065 - loss 0.33434046\n",
      "2019-07-22 23:57:44,919 epoch 4 - iter 636/1065 - loss 0.33604477\n",
      "2019-07-22 23:57:46,769 epoch 4 - iter 742/1065 - loss 0.33706626\n",
      "2019-07-22 23:57:48,786 epoch 4 - iter 848/1065 - loss 0.33532774\n",
      "2019-07-22 23:57:50,850 epoch 4 - iter 954/1065 - loss 0.33463459\n",
      "2019-07-22 23:57:52,792 epoch 4 - iter 1060/1065 - loss 0.33429426\n",
      "2019-07-22 23:57:52,863 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:57:52,865 EPOCH 4 done: loss 0.3345 - lr 0.1000\n",
      "2019-07-22 23:57:55,212 DEV : loss 0.33764439821243286 - score 0.1314\n",
      "2019-07-22 23:57:55,441 BAD EPOCHS (no improvement): 2\n",
      "2019-07-22 23:57:55,443 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:57:55,466 epoch 5 - iter 0/1065 - loss 0.33961880\n",
      "2019-07-22 23:57:57,336 epoch 5 - iter 106/1065 - loss 0.34684362\n",
      "2019-07-22 23:57:59,285 epoch 5 - iter 212/1065 - loss 0.33993697\n",
      "2019-07-22 23:58:01,171 epoch 5 - iter 318/1065 - loss 0.33809665\n",
      "2019-07-22 23:58:03,067 epoch 5 - iter 424/1065 - loss 0.33734654\n",
      "2019-07-22 23:58:04,998 epoch 5 - iter 530/1065 - loss 0.33604110\n",
      "2019-07-22 23:58:06,942 epoch 5 - iter 636/1065 - loss 0.33483805\n",
      "2019-07-22 23:58:08,838 epoch 5 - iter 742/1065 - loss 0.33537669\n",
      "2019-07-22 23:58:10,893 epoch 5 - iter 848/1065 - loss 0.33685992\n",
      "2019-07-22 23:58:13,016 epoch 5 - iter 954/1065 - loss 0.33624319\n",
      "2019-07-22 23:58:15,010 epoch 5 - iter 1060/1065 - loss 0.33408987\n",
      "2019-07-22 23:58:15,085 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:58:15,086 EPOCH 5 done: loss 0.3337 - lr 0.1000\n",
      "2019-07-22 23:58:17,483 DEV : loss 0.3457311987876892 - score 0.0695\n",
      "2019-07-22 23:58:17,725 BAD EPOCHS (no improvement): 3\n",
      "2019-07-22 23:58:17,726 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:58:17,751 epoch 6 - iter 0/1065 - loss 0.39861330\n",
      "2019-07-22 23:58:19,679 epoch 6 - iter 106/1065 - loss 0.32075344\n",
      "2019-07-22 23:58:21,599 epoch 6 - iter 212/1065 - loss 0.31866460\n",
      "2019-07-22 23:58:23,499 epoch 6 - iter 318/1065 - loss 0.32517141\n",
      "2019-07-22 23:58:25,538 epoch 6 - iter 424/1065 - loss 0.32502424\n",
      "2019-07-22 23:58:27,528 epoch 6 - iter 530/1065 - loss 0.32900927\n",
      "2019-07-22 23:58:29,558 epoch 6 - iter 636/1065 - loss 0.32878903\n",
      "2019-07-22 23:58:31,617 epoch 6 - iter 742/1065 - loss 0.32830760\n",
      "2019-07-22 23:58:33,734 epoch 6 - iter 848/1065 - loss 0.33075084\n",
      "2019-07-22 23:58:35,829 epoch 6 - iter 954/1065 - loss 0.33163779\n",
      "2019-07-22 23:58:37,879 epoch 6 - iter 1060/1065 - loss 0.33311522\n",
      "2019-07-22 23:58:37,951 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:58:37,953 EPOCH 6 done: loss 0.3325 - lr 0.1000\n",
      "2019-07-22 23:58:40,448 DEV : loss 0.3596385717391968 - score 0.0442\n",
      "Epoch     5: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-07-22 23:58:40,711 BAD EPOCHS (no improvement): 4\n",
      "2019-07-22 23:58:40,712 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:58:40,737 epoch 7 - iter 0/1065 - loss 0.52485675\n",
      "2019-07-22 23:58:42,773 epoch 7 - iter 106/1065 - loss 0.32945322\n",
      "2019-07-22 23:58:44,803 epoch 7 - iter 212/1065 - loss 0.32791824\n",
      "2019-07-22 23:58:46,837 epoch 7 - iter 318/1065 - loss 0.32563812\n",
      "2019-07-22 23:58:48,830 epoch 7 - iter 424/1065 - loss 0.33006220\n",
      "2019-07-22 23:58:50,840 epoch 7 - iter 530/1065 - loss 0.33086143\n",
      "2019-07-22 23:58:52,835 epoch 7 - iter 636/1065 - loss 0.33211919\n",
      "2019-07-22 23:58:54,827 epoch 7 - iter 742/1065 - loss 0.33192105\n",
      "2019-07-22 23:58:56,850 epoch 7 - iter 848/1065 - loss 0.33170117\n",
      "2019-07-22 23:58:58,907 epoch 7 - iter 954/1065 - loss 0.32978280\n",
      "2019-07-22 23:59:00,913 epoch 7 - iter 1060/1065 - loss 0.32884448\n",
      "2019-07-22 23:59:00,986 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:59:00,987 EPOCH 7 done: loss 0.3294 - lr 0.0500\n",
      "2019-07-22 23:59:03,428 DEV : loss 0.3452749252319336 - score 0.2562\n",
      "2019-07-22 23:59:03,702 BAD EPOCHS (no improvement): 0\n",
      "2019-07-22 23:59:07,050 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:59:07,072 epoch 8 - iter 0/1065 - loss 0.31553683\n",
      "2019-07-22 23:59:09,077 epoch 8 - iter 106/1065 - loss 0.33919927\n",
      "2019-07-22 23:59:11,153 epoch 8 - iter 212/1065 - loss 0.33084688\n",
      "2019-07-22 23:59:13,241 epoch 8 - iter 318/1065 - loss 0.33433597\n",
      "2019-07-22 23:59:15,294 epoch 8 - iter 424/1065 - loss 0.33496357\n",
      "2019-07-22 23:59:17,350 epoch 8 - iter 530/1065 - loss 0.33231873\n",
      "2019-07-22 23:59:19,325 epoch 8 - iter 636/1065 - loss 0.33321583\n",
      "2019-07-22 23:59:21,363 epoch 8 - iter 742/1065 - loss 0.33513163\n",
      "2019-07-22 23:59:23,428 epoch 8 - iter 848/1065 - loss 0.33333290\n",
      "2019-07-22 23:59:25,388 epoch 8 - iter 954/1065 - loss 0.33170121\n",
      "2019-07-22 23:59:27,359 epoch 8 - iter 1060/1065 - loss 0.32961703\n",
      "2019-07-22 23:59:27,436 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:59:27,438 EPOCH 8 done: loss 0.3295 - lr 0.0500\n",
      "2019-07-22 23:59:29,827 DEV : loss 0.34334132075309753 - score 0.0967\n",
      "2019-07-22 23:59:30,057 BAD EPOCHS (no improvement): 1\n",
      "2019-07-22 23:59:30,059 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:59:30,081 epoch 9 - iter 0/1065 - loss 0.16748482\n",
      "2019-07-22 23:59:32,080 epoch 9 - iter 106/1065 - loss 0.32938928\n",
      "2019-07-22 23:59:34,042 epoch 9 - iter 212/1065 - loss 0.32594452\n",
      "2019-07-22 23:59:36,043 epoch 9 - iter 318/1065 - loss 0.33235485\n",
      "2019-07-22 23:59:38,076 epoch 9 - iter 424/1065 - loss 0.33168436\n",
      "2019-07-22 23:59:40,195 epoch 9 - iter 530/1065 - loss 0.33209525\n",
      "2019-07-22 23:59:42,109 epoch 9 - iter 636/1065 - loss 0.33183659\n",
      "2019-07-22 23:59:44,224 epoch 9 - iter 742/1065 - loss 0.33030834\n",
      "2019-07-22 23:59:46,212 epoch 9 - iter 848/1065 - loss 0.33026285\n",
      "2019-07-22 23:59:48,295 epoch 9 - iter 954/1065 - loss 0.32979311\n",
      "2019-07-22 23:59:50,446 epoch 9 - iter 1060/1065 - loss 0.32909443\n",
      "2019-07-22 23:59:50,521 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:59:50,522 EPOCH 9 done: loss 0.3293 - lr 0.0500\n",
      "2019-07-22 23:59:52,953 DEV : loss 0.3374671936035156 - score 0.2216\n",
      "2019-07-22 23:59:53,189 BAD EPOCHS (no improvement): 2\n",
      "2019-07-22 23:59:53,190 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-22 23:59:53,213 epoch 10 - iter 0/1065 - loss 0.30786675\n",
      "2019-07-22 23:59:55,152 epoch 10 - iter 106/1065 - loss 0.31912883\n",
      "2019-07-22 23:59:57,029 epoch 10 - iter 212/1065 - loss 0.33088570\n",
      "2019-07-22 23:59:59,016 epoch 10 - iter 318/1065 - loss 0.33046736\n",
      "2019-07-23 00:00:00,936 epoch 10 - iter 424/1065 - loss 0.33017014\n",
      "2019-07-23 00:00:02,888 epoch 10 - iter 530/1065 - loss 0.33535287\n",
      "2019-07-23 00:00:04,852 epoch 10 - iter 636/1065 - loss 0.33376231\n",
      "2019-07-23 00:00:06,985 epoch 10 - iter 742/1065 - loss 0.33146507\n",
      "2019-07-23 00:00:08,962 epoch 10 - iter 848/1065 - loss 0.33125969\n",
      "2019-07-23 00:00:10,894 epoch 10 - iter 954/1065 - loss 0.33055819\n",
      "2019-07-23 00:00:12,927 epoch 10 - iter 1060/1065 - loss 0.32917423\n",
      "2019-07-23 00:00:13,000 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:00:13,002 EPOCH 10 done: loss 0.3290 - lr 0.0500\n",
      "2019-07-23 00:00:15,357 DEV : loss 0.3363032042980194 - score 0.1982\n",
      "2019-07-23 00:00:15,594 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 00:00:19,082 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:00:19,083 Testing using best model ...\n",
      "2019-07-23 00:00:19,084 loading file glove_pooled/best-model.pt\n",
      "2019-07-23 00:00:28,568 0.4384\t0.1644\t0.2391\n",
      "2019-07-23 00:00:28,569 \n",
      "MICRO_AVG: acc 0.7612 - f1-score 0.8644\n",
      "MACRO_AVG: acc 0.4986 - f1-score 0.58235\n",
      "0          tp: 4787 - fp: 615 - fn: 155 - tn: 121 - precision: 0.8862 - recall: 0.9686 - accuracy: 0.8614 - f1-score: 0.9256\n",
      "1          tp: 121 - fp: 155 - fn: 615 - tn: 4787 - precision: 0.4384 - recall: 0.1644 - accuracy: 0.1358 - f1-score: 0.2391\n",
      "2019-07-23 00:00:28,570 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dev_loss_history': [tensor(0.3487, device='cuda:0'),\n",
       "  tensor(0.3408, device='cuda:0'),\n",
       "  tensor(0.3467, device='cuda:0'),\n",
       "  tensor(0.3376, device='cuda:0'),\n",
       "  tensor(0.3457, device='cuda:0'),\n",
       "  tensor(0.3596, device='cuda:0'),\n",
       "  tensor(0.3453, device='cuda:0'),\n",
       "  tensor(0.3433, device='cuda:0'),\n",
       "  tensor(0.3375, device='cuda:0'),\n",
       "  tensor(0.3363, device='cuda:0')],\n",
       " 'dev_score_history': [0.0584,\n",
       "  0.1454,\n",
       "  0.0696,\n",
       "  0.1314,\n",
       "  0.0695,\n",
       "  0.0442,\n",
       "  0.2562,\n",
       "  0.0967,\n",
       "  0.2216,\n",
       "  0.1982],\n",
       " 'test_score': 0.2391,\n",
       " 'train_loss_history': [0.35602582874992084,\n",
       "  0.34077189323348056,\n",
       "  0.3368952038142603,\n",
       "  0.33451063412595805,\n",
       "  0.33370714184124145,\n",
       "  0.3325438851271996,\n",
       "  0.3293878852271698,\n",
       "  0.3294598066750826,\n",
       "  0.32925278885123876,\n",
       "  0.3289960900033024]}"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'),\n",
    "                                                       test_file='flair_test.csv',\n",
    "                                                       dev_file='flair_dev.csv',\n",
    "                                                       train_file='flair_train.csv')\n",
    "word_embeddings = [#BertEmbeddings(),\n",
    "                   WordEmbeddings(\"glove\")] # bert and fasttext\n",
    "document_embeddings = DocumentPoolEmbeddings(word_embeddings)\n",
    "\n",
    "print(\"Create Classifier *************************\")\n",
    "classifier = TextClassifier(document_embeddings,\n",
    "                            label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "print(\"Create Trainer *******************************\")\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "print(\"Begin Training *******************************\")\n",
    "trainer.train(max_epochs=10, base_path = \"glove_pooled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KgXZmcqTIxSJ",
    "outputId": "13d20b7b-55d9-434e-e8c4-57e94f9861fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 00:05:41,436 Reading data from .\n",
      "2019-07-23 00:05:41,437 Train: flair_train.csv\n",
      "2019-07-23 00:05:41,438 Dev: flair_dev.csv\n",
      "2019-07-23 00:05:41,439 Test: flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:447: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:454: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:463: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Classifier *************************\n",
      "2019-07-23 00:06:07,633 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34066/34066 [00:00<00:00, 253648.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 00:06:07,772 [b'0', b'1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Trainer *******************************\n",
      "Begin Training *******************************\n",
      "2019-07-23 00:06:07,945 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:06:07,947 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): WordEmbeddings('news')\n",
      "      (list_embedding_3): WordEmbeddings('glove')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2448, out_features=2448, bias=True)\n",
      "    (rnn): GRU(2448, 128)\n",
      "    (dropout): Dropout(p=0.5)\n",
      "  )\n",
      "  (decoder): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-07-23 00:06:07,948 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:06:07,950 Corpus: \"Corpus: 34066 train + 5678 dev + 5678 test sentences\"\n",
      "2019-07-23 00:06:07,952 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:06:07,953 Parameters:\n",
      "2019-07-23 00:06:07,954  - learning_rate: \"0.1\"\n",
      "2019-07-23 00:06:07,956  - mini_batch_size: \"32\"\n",
      "2019-07-23 00:06:07,958  - patience: \"3\"\n",
      "2019-07-23 00:06:07,959  - anneal_factor: \"0.5\"\n",
      "2019-07-23 00:06:07,960  - max_epochs: \"100\"\n",
      "2019-07-23 00:06:07,961  - shuffle: \"True\"\n",
      "2019-07-23 00:06:07,962  - train_with_dev: \"False\"\n",
      "2019-07-23 00:06:07,964 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:06:07,966 Model training base path: \"all_rnn\"\n",
      "2019-07-23 00:06:07,967 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:06:07,969 Device: cuda:0\n",
      "2019-07-23 00:06:07,970 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:06:07,973 Embedding storage mode: cpu\n",
      "2019-07-23 00:06:07,975 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:06:08,283 epoch 1 - iter 0/1065 - loss 0.71749187\n",
      "2019-07-23 00:06:27,903 epoch 1 - iter 106/1065 - loss 0.46165419\n",
      "2019-07-23 00:06:48,015 epoch 1 - iter 212/1065 - loss 0.42681816\n",
      "2019-07-23 00:07:07,299 epoch 1 - iter 318/1065 - loss 0.40547351\n",
      "2019-07-23 00:07:26,522 epoch 1 - iter 424/1065 - loss 0.39813679\n",
      "2019-07-23 00:07:46,819 epoch 1 - iter 530/1065 - loss 0.38818090\n",
      "2019-07-23 00:08:15,832 epoch 1 - iter 636/1065 - loss 0.38009249\n",
      "2019-07-23 00:08:35,752 epoch 1 - iter 742/1065 - loss 0.37689801\n",
      "2019-07-23 00:08:55,908 epoch 1 - iter 848/1065 - loss 0.37284788\n",
      "2019-07-23 00:09:19,896 epoch 1 - iter 954/1065 - loss 0.36869731\n",
      "2019-07-23 00:09:40,114 epoch 1 - iter 1060/1065 - loss 0.36571711\n",
      "2019-07-23 00:09:40,822 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:09:40,824 EPOCH 1 done: loss 0.3653 - lr 0.1000\n",
      "2019-07-23 00:10:02,906 DEV : loss 0.3253400921821594 - score 0.1578\n",
      "2019-07-23 00:10:13,978 BAD EPOCHS (no improvement): 0\n",
      "2019-07-23 00:10:34,445 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:10:34,865 epoch 2 - iter 0/1065 - loss 0.15585619\n",
      "2019-07-23 00:10:55,260 epoch 2 - iter 106/1065 - loss 0.32159071\n",
      "2019-07-23 00:11:14,604 epoch 2 - iter 212/1065 - loss 0.33254131\n",
      "2019-07-23 00:11:33,617 epoch 2 - iter 318/1065 - loss 0.33137083\n",
      "2019-07-23 00:11:54,716 epoch 2 - iter 424/1065 - loss 0.33305116\n",
      "2019-07-23 00:12:13,240 epoch 2 - iter 530/1065 - loss 0.32979078\n",
      "2019-07-23 00:12:33,032 epoch 2 - iter 636/1065 - loss 0.32718956\n",
      "2019-07-23 00:12:53,490 epoch 2 - iter 742/1065 - loss 0.32565527\n",
      "2019-07-23 00:13:13,437 epoch 2 - iter 848/1065 - loss 0.32570138\n",
      "2019-07-23 00:13:33,101 epoch 2 - iter 954/1065 - loss 0.32709146\n",
      "2019-07-23 00:13:54,253 epoch 2 - iter 1060/1065 - loss 0.32584977\n",
      "2019-07-23 00:13:55,092 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:13:55,093 EPOCH 2 done: loss 0.3258 - lr 0.1000\n",
      "2019-07-23 00:14:23,137 DEV : loss 0.31442776322364807 - score 0.2781\n",
      "2019-07-23 00:14:23,709 BAD EPOCHS (no improvement): 0\n",
      "2019-07-23 00:14:42,695 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:14:42,945 epoch 3 - iter 0/1065 - loss 0.22712767\n",
      "2019-07-23 00:15:01,398 epoch 3 - iter 106/1065 - loss 0.31943097\n",
      "2019-07-23 00:15:20,632 epoch 3 - iter 212/1065 - loss 0.32431121\n",
      "2019-07-23 00:15:39,888 epoch 3 - iter 318/1065 - loss 0.31936933\n",
      "2019-07-23 00:15:58,572 epoch 3 - iter 424/1065 - loss 0.31807004\n",
      "2019-07-23 00:16:15,977 epoch 3 - iter 530/1065 - loss 0.31516083\n",
      "2019-07-23 00:16:34,083 epoch 3 - iter 636/1065 - loss 0.31243427\n",
      "2019-07-23 00:16:51,628 epoch 3 - iter 742/1065 - loss 0.31231866\n",
      "2019-07-23 00:17:09,824 epoch 3 - iter 848/1065 - loss 0.31305939\n",
      "2019-07-23 00:17:29,260 epoch 3 - iter 954/1065 - loss 0.31149489\n",
      "2019-07-23 00:17:49,103 epoch 3 - iter 1060/1065 - loss 0.31097482\n",
      "2019-07-23 00:17:49,885 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:17:49,886 EPOCH 3 done: loss 0.3112 - lr 0.1000\n",
      "2019-07-23 00:18:26,810 DEV : loss 0.3111754059791565 - score 0.3195\n",
      "2019-07-23 00:18:27,445 BAD EPOCHS (no improvement): 0\n",
      "2019-07-23 00:18:46,669 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:18:46,902 epoch 4 - iter 0/1065 - loss 0.27225587\n",
      "2019-07-23 00:19:05,345 epoch 4 - iter 106/1065 - loss 0.29998232\n",
      "2019-07-23 00:19:24,241 epoch 4 - iter 212/1065 - loss 0.30825587\n",
      "2019-07-23 00:19:42,947 epoch 4 - iter 318/1065 - loss 0.30673135\n",
      "2019-07-23 00:20:03,358 epoch 4 - iter 424/1065 - loss 0.30619762\n",
      "2019-07-23 00:20:26,266 epoch 4 - iter 530/1065 - loss 0.30776714\n",
      "2019-07-23 00:20:48,591 epoch 4 - iter 636/1065 - loss 0.30539733\n",
      "2019-07-23 00:21:11,445 epoch 4 - iter 742/1065 - loss 0.30067982\n",
      "2019-07-23 00:21:34,172 epoch 4 - iter 848/1065 - loss 0.30121070\n",
      "2019-07-23 00:21:57,088 epoch 4 - iter 954/1065 - loss 0.30201907\n",
      "2019-07-23 00:22:19,380 epoch 4 - iter 1060/1065 - loss 0.30230539\n",
      "2019-07-23 00:22:20,101 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:22:20,102 EPOCH 4 done: loss 0.3022 - lr 0.1000\n",
      "2019-07-23 00:22:52,200 DEV : loss 0.30182334780693054 - score 0.3441\n",
      "2019-07-23 00:22:52,756 BAD EPOCHS (no improvement): 0\n",
      "2019-07-23 00:23:11,490 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:23:11,713 epoch 5 - iter 0/1065 - loss 0.14719486\n",
      "2019-07-23 00:23:27,277 epoch 5 - iter 106/1065 - loss 0.29933462\n",
      "2019-07-23 00:23:45,572 epoch 5 - iter 212/1065 - loss 0.29743584\n",
      "2019-07-23 00:24:03,290 epoch 5 - iter 318/1065 - loss 0.29552517\n",
      "2019-07-23 00:24:21,826 epoch 5 - iter 424/1065 - loss 0.29532556\n",
      "2019-07-23 00:24:40,435 epoch 5 - iter 530/1065 - loss 0.29373225\n",
      "2019-07-23 00:24:59,390 epoch 5 - iter 636/1065 - loss 0.29579873\n",
      "2019-07-23 00:25:18,321 epoch 5 - iter 742/1065 - loss 0.29241245\n",
      "2019-07-23 00:25:35,405 epoch 5 - iter 848/1065 - loss 0.29228853\n",
      "2019-07-23 00:25:51,932 epoch 5 - iter 954/1065 - loss 0.29322628\n",
      "2019-07-23 00:26:09,816 epoch 5 - iter 1060/1065 - loss 0.29265744\n",
      "2019-07-23 00:26:10,540 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:26:10,542 EPOCH 5 done: loss 0.2927 - lr 0.1000\n",
      "2019-07-23 00:26:42,316 DEV : loss 0.29649725556373596 - score 0.3062\n",
      "2019-07-23 00:26:42,902 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 00:26:42,904 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:26:43,156 epoch 6 - iter 0/1065 - loss 0.28509814\n",
      "2019-07-23 00:27:03,309 epoch 6 - iter 106/1065 - loss 0.29022152\n",
      "2019-07-23 00:27:23,176 epoch 6 - iter 212/1065 - loss 0.29351203\n",
      "2019-07-23 00:27:41,851 epoch 6 - iter 318/1065 - loss 0.28806063\n",
      "2019-07-23 00:28:00,523 epoch 6 - iter 424/1065 - loss 0.29644342\n",
      "2019-07-23 00:28:18,963 epoch 6 - iter 530/1065 - loss 0.29243153\n",
      "2019-07-23 00:28:38,050 epoch 6 - iter 636/1065 - loss 0.29076132\n",
      "2019-07-23 00:28:56,172 epoch 6 - iter 742/1065 - loss 0.28946181\n",
      "2019-07-23 00:29:13,871 epoch 6 - iter 848/1065 - loss 0.28998792\n",
      "2019-07-23 00:29:31,861 epoch 6 - iter 954/1065 - loss 0.28778414\n",
      "2019-07-23 00:29:47,252 epoch 6 - iter 1060/1065 - loss 0.28696089\n",
      "2019-07-23 00:29:47,525 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:29:47,526 EPOCH 6 done: loss 0.2868 - lr 0.1000\n",
      "2019-07-23 00:29:57,838 DEV : loss 0.29394811391830444 - score 0.4206\n",
      "2019-07-23 00:29:58,420 BAD EPOCHS (no improvement): 0\n",
      "2019-07-23 00:30:16,771 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:30:16,983 epoch 7 - iter 0/1065 - loss 0.24190211\n",
      "2019-07-23 00:30:34,486 epoch 7 - iter 106/1065 - loss 0.28172129\n",
      "2019-07-23 00:30:53,215 epoch 7 - iter 212/1065 - loss 0.28450572\n",
      "2019-07-23 00:31:12,769 epoch 7 - iter 318/1065 - loss 0.28138440\n",
      "2019-07-23 00:31:31,765 epoch 7 - iter 424/1065 - loss 0.28038772\n",
      "2019-07-23 00:31:49,459 epoch 7 - iter 530/1065 - loss 0.27655201\n",
      "2019-07-23 00:32:09,224 epoch 7 - iter 636/1065 - loss 0.27674360\n",
      "2019-07-23 00:32:29,256 epoch 7 - iter 742/1065 - loss 0.27817570\n",
      "2019-07-23 00:32:47,707 epoch 7 - iter 848/1065 - loss 0.27869546\n",
      "2019-07-23 00:33:06,277 epoch 7 - iter 954/1065 - loss 0.28080667\n",
      "2019-07-23 00:33:21,398 epoch 7 - iter 1060/1065 - loss 0.27853409\n",
      "2019-07-23 00:33:21,759 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:33:21,760 EPOCH 7 done: loss 0.2786 - lr 0.1000\n",
      "2019-07-23 00:33:32,285 DEV : loss 0.2895182967185974 - score 0.5192\n",
      "2019-07-23 00:33:32,847 BAD EPOCHS (no improvement): 0\n",
      "2019-07-23 00:33:50,990 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:33:51,868 epoch 8 - iter 0/1065 - loss 0.18601282\n",
      "2019-07-23 00:34:12,146 epoch 8 - iter 106/1065 - loss 0.28619863\n",
      "2019-07-23 00:34:31,935 epoch 8 - iter 212/1065 - loss 0.27725847\n",
      "2019-07-23 00:34:50,543 epoch 8 - iter 318/1065 - loss 0.27324730\n",
      "2019-07-23 00:35:09,012 epoch 8 - iter 424/1065 - loss 0.27363556\n",
      "2019-07-23 00:35:27,000 epoch 8 - iter 530/1065 - loss 0.27771847\n",
      "2019-07-23 00:35:44,058 epoch 8 - iter 636/1065 - loss 0.27696280\n",
      "2019-07-23 00:36:01,228 epoch 8 - iter 742/1065 - loss 0.27645149\n",
      "2019-07-23 00:36:17,445 epoch 8 - iter 848/1065 - loss 0.27474583\n",
      "2019-07-23 00:36:35,079 epoch 8 - iter 954/1065 - loss 0.27464045\n",
      "2019-07-23 00:36:49,802 epoch 8 - iter 1060/1065 - loss 0.27476007\n",
      "2019-07-23 00:36:50,046 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:36:50,047 EPOCH 8 done: loss 0.2747 - lr 0.1000\n",
      "2019-07-23 00:37:01,355 DEV : loss 0.28651031851768494 - score 0.4671\n",
      "2019-07-23 00:37:01,930 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 00:37:01,932 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:37:02,028 epoch 9 - iter 0/1065 - loss 0.36784834\n",
      "2019-07-23 00:37:10,015 epoch 9 - iter 106/1065 - loss 0.27053024\n",
      "2019-07-23 00:37:18,602 epoch 9 - iter 212/1065 - loss 0.27605553\n",
      "2019-07-23 00:37:27,476 epoch 9 - iter 318/1065 - loss 0.26684764\n",
      "2019-07-23 00:37:35,220 epoch 9 - iter 424/1065 - loss 0.26570682\n",
      "2019-07-23 00:37:43,231 epoch 9 - iter 530/1065 - loss 0.26273840\n",
      "2019-07-23 00:37:51,016 epoch 9 - iter 636/1065 - loss 0.26304678\n",
      "2019-07-23 00:37:59,178 epoch 9 - iter 742/1065 - loss 0.26461611\n",
      "2019-07-23 00:38:07,379 epoch 9 - iter 848/1065 - loss 0.26710012\n",
      "2019-07-23 00:38:15,528 epoch 9 - iter 954/1065 - loss 0.26778740\n",
      "2019-07-23 00:38:24,055 epoch 9 - iter 1060/1065 - loss 0.26716269\n",
      "2019-07-23 00:38:24,428 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:38:24,429 EPOCH 9 done: loss 0.2673 - lr 0.1000\n",
      "2019-07-23 00:38:36,417 DEV : loss 0.4043290317058563 - score 0.5025\n",
      "2019-07-23 00:38:37,088 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 00:38:37,090 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:38:37,217 epoch 10 - iter 0/1065 - loss 0.37009212\n",
      "2019-07-23 00:38:46,862 epoch 10 - iter 106/1065 - loss 0.25028453\n",
      "2019-07-23 00:38:56,256 epoch 10 - iter 212/1065 - loss 0.25261548\n",
      "2019-07-23 00:39:05,048 epoch 10 - iter 318/1065 - loss 0.25848890\n",
      "2019-07-23 00:39:14,270 epoch 10 - iter 424/1065 - loss 0.25982438\n",
      "2019-07-23 00:39:22,866 epoch 10 - iter 530/1065 - loss 0.25978853\n",
      "2019-07-23 00:39:31,392 epoch 10 - iter 636/1065 - loss 0.25859811\n",
      "2019-07-23 00:39:41,054 epoch 10 - iter 742/1065 - loss 0.25890755\n",
      "2019-07-23 00:39:50,665 epoch 10 - iter 848/1065 - loss 0.26099954\n",
      "2019-07-23 00:39:59,979 epoch 10 - iter 954/1065 - loss 0.25969737\n",
      "2019-07-23 00:40:08,481 epoch 10 - iter 1060/1065 - loss 0.25967137\n",
      "2019-07-23 00:40:08,964 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:40:08,966 EPOCH 10 done: loss 0.2599 - lr 0.1000\n",
      "2019-07-23 00:40:20,293 DEV : loss 0.28561437129974365 - score 0.3661\n",
      "2019-07-23 00:40:20,930 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 00:40:20,932 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:40:21,015 epoch 11 - iter 0/1065 - loss 0.12955172\n",
      "2019-07-23 00:40:30,768 epoch 11 - iter 106/1065 - loss 0.24974873\n",
      "2019-07-23 00:40:39,553 epoch 11 - iter 212/1065 - loss 0.25093978\n",
      "2019-07-23 00:40:48,470 epoch 11 - iter 318/1065 - loss 0.25594170\n",
      "2019-07-23 00:40:56,879 epoch 11 - iter 424/1065 - loss 0.25445129\n",
      "2019-07-23 00:41:06,157 epoch 11 - iter 530/1065 - loss 0.25094720\n",
      "2019-07-23 00:41:14,929 epoch 11 - iter 636/1065 - loss 0.25123133\n",
      "2019-07-23 00:41:24,483 epoch 11 - iter 742/1065 - loss 0.25293957\n",
      "2019-07-23 00:41:34,009 epoch 11 - iter 848/1065 - loss 0.25268511\n",
      "2019-07-23 00:41:43,787 epoch 11 - iter 954/1065 - loss 0.25389273\n",
      "2019-07-23 00:41:52,660 epoch 11 - iter 1060/1065 - loss 0.25450840\n",
      "2019-07-23 00:41:52,942 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:41:52,943 EPOCH 11 done: loss 0.2543 - lr 0.1000\n",
      "2019-07-23 00:42:02,845 DEV : loss 0.28166836500167847 - score 0.5097\n",
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-07-23 00:42:03,421 BAD EPOCHS (no improvement): 4\n",
      "2019-07-23 00:42:03,423 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:42:03,478 epoch 12 - iter 0/1065 - loss 0.21269646\n",
      "2019-07-23 00:42:10,855 epoch 12 - iter 106/1065 - loss 0.25304000\n",
      "2019-07-23 00:42:18,430 epoch 12 - iter 212/1065 - loss 0.23978839\n",
      "2019-07-23 00:42:26,297 epoch 12 - iter 318/1065 - loss 0.23807988\n",
      "2019-07-23 00:42:33,129 epoch 12 - iter 424/1065 - loss 0.23611492\n",
      "2019-07-23 00:42:41,117 epoch 12 - iter 530/1065 - loss 0.23685125\n",
      "2019-07-23 00:42:49,242 epoch 12 - iter 636/1065 - loss 0.23868475\n",
      "2019-07-23 00:42:57,842 epoch 12 - iter 742/1065 - loss 0.23845205\n",
      "2019-07-23 00:43:05,713 epoch 12 - iter 848/1065 - loss 0.23668318\n",
      "2019-07-23 00:43:13,314 epoch 12 - iter 954/1065 - loss 0.23629337\n",
      "2019-07-23 00:43:21,416 epoch 12 - iter 1060/1065 - loss 0.23581010\n",
      "2019-07-23 00:43:21,742 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:43:21,744 EPOCH 12 done: loss 0.2357 - lr 0.0500\n",
      "2019-07-23 00:43:32,986 DEV : loss 0.28460493683815 - score 0.495\n",
      "2019-07-23 00:43:33,582 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 00:43:33,584 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:43:33,628 epoch 13 - iter 0/1065 - loss 0.22017795\n",
      "2019-07-23 00:43:41,906 epoch 13 - iter 106/1065 - loss 0.21991805\n",
      "2019-07-23 00:43:50,227 epoch 13 - iter 212/1065 - loss 0.22522604\n",
      "2019-07-23 00:43:58,963 epoch 13 - iter 318/1065 - loss 0.22965912\n",
      "2019-07-23 00:44:06,536 epoch 13 - iter 424/1065 - loss 0.22929934\n",
      "2019-07-23 00:44:14,743 epoch 13 - iter 530/1065 - loss 0.22749065\n",
      "2019-07-23 00:44:23,372 epoch 13 - iter 636/1065 - loss 0.22806117\n",
      "2019-07-23 00:44:31,653 epoch 13 - iter 742/1065 - loss 0.22945038\n",
      "2019-07-23 00:44:40,346 epoch 13 - iter 848/1065 - loss 0.23008754\n",
      "2019-07-23 00:44:49,085 epoch 13 - iter 954/1065 - loss 0.22901321\n",
      "2019-07-23 00:44:57,824 epoch 13 - iter 1060/1065 - loss 0.23021095\n",
      "2019-07-23 00:44:58,104 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:44:58,105 EPOCH 13 done: loss 0.2299 - lr 0.0500\n",
      "2019-07-23 00:45:06,864 DEV : loss 0.27664023637771606 - score 0.5182\n",
      "2019-07-23 00:45:07,430 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 00:45:07,432 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:45:07,504 epoch 14 - iter 0/1065 - loss 0.24185225\n",
      "2019-07-23 00:45:14,778 epoch 14 - iter 106/1065 - loss 0.21830228\n",
      "2019-07-23 00:45:21,249 epoch 14 - iter 212/1065 - loss 0.22454117\n",
      "2019-07-23 00:45:27,746 epoch 14 - iter 318/1065 - loss 0.22614301\n",
      "2019-07-23 00:45:34,643 epoch 14 - iter 424/1065 - loss 0.22547005\n",
      "2019-07-23 00:45:41,566 epoch 14 - iter 530/1065 - loss 0.22506905\n",
      "2019-07-23 00:45:47,660 epoch 14 - iter 636/1065 - loss 0.22504285\n",
      "2019-07-23 00:45:54,566 epoch 14 - iter 742/1065 - loss 0.22479724\n",
      "2019-07-23 00:46:01,368 epoch 14 - iter 848/1065 - loss 0.22359390\n",
      "2019-07-23 00:46:08,538 epoch 14 - iter 954/1065 - loss 0.22324476\n",
      "2019-07-23 00:46:15,942 epoch 14 - iter 1060/1065 - loss 0.22374875\n",
      "2019-07-23 00:46:16,255 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:46:16,257 EPOCH 14 done: loss 0.2237 - lr 0.0500\n",
      "2019-07-23 00:46:26,332 DEV : loss 0.2778368592262268 - score 0.5491\n",
      "2019-07-23 00:46:26,957 BAD EPOCHS (no improvement): 0\n",
      "2019-07-23 00:46:45,795 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:46:45,901 epoch 15 - iter 0/1065 - loss 0.33560085\n",
      "2019-07-23 00:47:03,361 epoch 15 - iter 106/1065 - loss 0.23791992\n",
      "2019-07-23 00:47:22,505 epoch 15 - iter 212/1065 - loss 0.22564510\n",
      "2019-07-23 00:47:41,018 epoch 15 - iter 318/1065 - loss 0.22957257\n",
      "2019-07-23 00:47:58,754 epoch 15 - iter 424/1065 - loss 0.22842986\n",
      "2019-07-23 00:48:16,977 epoch 15 - iter 530/1065 - loss 0.22450096\n",
      "2019-07-23 00:48:34,940 epoch 15 - iter 636/1065 - loss 0.22357210\n",
      "2019-07-23 00:48:54,113 epoch 15 - iter 742/1065 - loss 0.22268298\n",
      "2019-07-23 00:49:12,337 epoch 15 - iter 848/1065 - loss 0.22042360\n",
      "2019-07-23 00:49:30,029 epoch 15 - iter 954/1065 - loss 0.22093835\n",
      "2019-07-23 00:49:44,531 epoch 15 - iter 1060/1065 - loss 0.22049721\n",
      "2019-07-23 00:49:44,738 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:49:44,739 EPOCH 15 done: loss 0.2205 - lr 0.0500\n",
      "2019-07-23 00:49:49,434 DEV : loss 0.28492435812950134 - score 0.5216\n",
      "2019-07-23 00:49:49,999 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 00:49:50,000 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:49:50,140 epoch 16 - iter 0/1065 - loss 0.21755247\n",
      "2019-07-23 00:50:07,960 epoch 16 - iter 106/1065 - loss 0.20163988\n",
      "2019-07-23 00:50:26,652 epoch 16 - iter 212/1065 - loss 0.21210871\n",
      "2019-07-23 00:50:44,763 epoch 16 - iter 318/1065 - loss 0.21008238\n",
      "2019-07-23 00:51:03,235 epoch 16 - iter 424/1065 - loss 0.21006696\n",
      "2019-07-23 00:51:22,315 epoch 16 - iter 530/1065 - loss 0.21224285\n",
      "2019-07-23 00:51:42,859 epoch 16 - iter 636/1065 - loss 0.21211164\n",
      "2019-07-23 00:52:03,022 epoch 16 - iter 742/1065 - loss 0.21282536\n",
      "2019-07-23 00:52:21,395 epoch 16 - iter 848/1065 - loss 0.21482059\n",
      "2019-07-23 00:52:39,335 epoch 16 - iter 954/1065 - loss 0.21496165\n",
      "2019-07-23 00:52:58,591 epoch 16 - iter 1060/1065 - loss 0.21498865\n",
      "2019-07-23 00:52:59,278 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:52:59,279 EPOCH 16 done: loss 0.2149 - lr 0.0500\n",
      "2019-07-23 00:53:07,224 DEV : loss 0.2877218723297119 - score 0.4845\n",
      "2019-07-23 00:53:07,807 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 00:53:07,809 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:53:07,860 epoch 17 - iter 0/1065 - loss 0.07002510\n",
      "2019-07-23 00:53:13,707 epoch 17 - iter 106/1065 - loss 0.19952708\n",
      "2019-07-23 00:53:19,967 epoch 17 - iter 212/1065 - loss 0.20335447\n",
      "2019-07-23 00:53:26,454 epoch 17 - iter 318/1065 - loss 0.20791220\n",
      "2019-07-23 00:53:32,382 epoch 17 - iter 424/1065 - loss 0.21010513\n",
      "2019-07-23 00:53:38,118 epoch 17 - iter 530/1065 - loss 0.20727983\n",
      "2019-07-23 00:53:43,768 epoch 17 - iter 636/1065 - loss 0.20844106\n",
      "2019-07-23 00:53:49,765 epoch 17 - iter 742/1065 - loss 0.20979827\n",
      "2019-07-23 00:53:55,981 epoch 17 - iter 848/1065 - loss 0.20910757\n",
      "2019-07-23 00:54:02,206 epoch 17 - iter 954/1065 - loss 0.21017888\n",
      "2019-07-23 00:54:09,600 epoch 17 - iter 1060/1065 - loss 0.20935231\n",
      "2019-07-23 00:54:09,967 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:54:09,969 EPOCH 17 done: loss 0.2093 - lr 0.0500\n",
      "2019-07-23 00:54:24,884 DEV : loss 0.2932078242301941 - score 0.5251\n",
      "2019-07-23 00:54:25,516 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 00:54:25,518 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:54:25,603 epoch 18 - iter 0/1065 - loss 0.24189422\n",
      "2019-07-23 00:54:36,272 epoch 18 - iter 106/1065 - loss 0.21893416\n",
      "2019-07-23 00:54:46,099 epoch 18 - iter 212/1065 - loss 0.20591586\n",
      "2019-07-23 00:54:55,921 epoch 18 - iter 318/1065 - loss 0.20737523\n",
      "2019-07-23 00:55:05,181 epoch 18 - iter 424/1065 - loss 0.20563735\n",
      "2019-07-23 00:55:15,176 epoch 18 - iter 530/1065 - loss 0.20428085\n",
      "2019-07-23 00:55:25,366 epoch 18 - iter 636/1065 - loss 0.20395237\n",
      "2019-07-23 00:55:35,705 epoch 18 - iter 742/1065 - loss 0.20521191\n",
      "2019-07-23 00:55:45,002 epoch 18 - iter 848/1065 - loss 0.20534149\n",
      "2019-07-23 00:55:54,778 epoch 18 - iter 954/1065 - loss 0.20622162\n",
      "2019-07-23 00:56:04,891 epoch 18 - iter 1060/1065 - loss 0.20607385\n",
      "2019-07-23 00:56:05,208 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:56:05,209 EPOCH 18 done: loss 0.2062 - lr 0.0500\n",
      "2019-07-23 00:56:18,469 DEV : loss 0.2822670042514801 - score 0.5555\n",
      "2019-07-23 00:56:19,053 BAD EPOCHS (no improvement): 0\n",
      "2019-07-23 00:56:38,003 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:56:38,314 epoch 19 - iter 0/1065 - loss 0.19467691\n",
      "2019-07-23 00:56:58,844 epoch 19 - iter 106/1065 - loss 0.20447971\n",
      "2019-07-23 00:57:18,337 epoch 19 - iter 212/1065 - loss 0.20497547\n",
      "2019-07-23 00:57:36,718 epoch 19 - iter 318/1065 - loss 0.20751664\n",
      "2019-07-23 00:57:54,521 epoch 19 - iter 424/1065 - loss 0.20514203\n",
      "2019-07-23 00:58:11,795 epoch 19 - iter 530/1065 - loss 0.20409035\n",
      "2019-07-23 00:58:29,703 epoch 19 - iter 636/1065 - loss 0.20325416\n",
      "2019-07-23 00:58:48,003 epoch 19 - iter 742/1065 - loss 0.20324820\n",
      "2019-07-23 00:59:06,114 epoch 19 - iter 848/1065 - loss 0.20285908\n",
      "2019-07-23 00:59:25,539 epoch 19 - iter 954/1065 - loss 0.20138866\n",
      "2019-07-23 00:59:40,680 epoch 19 - iter 1060/1065 - loss 0.19992838\n",
      "2019-07-23 00:59:40,882 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 00:59:40,884 EPOCH 19 done: loss 0.2000 - lr 0.0500\n",
      "2019-07-23 00:59:45,917 DEV : loss 0.3033309876918793 - score 0.5831\n",
      "2019-07-23 00:59:46,568 BAD EPOCHS (no improvement): 0\n",
      "2019-07-23 01:00:04,806 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:00:05,117 epoch 20 - iter 0/1065 - loss 0.24665612\n",
      "2019-07-23 01:00:23,651 epoch 20 - iter 106/1065 - loss 0.19304410\n",
      "2019-07-23 01:00:40,891 epoch 20 - iter 212/1065 - loss 0.20047021\n",
      "2019-07-23 01:00:59,428 epoch 20 - iter 318/1065 - loss 0.19942688\n",
      "2019-07-23 01:01:18,353 epoch 20 - iter 424/1065 - loss 0.19814794\n",
      "2019-07-23 01:01:36,656 epoch 20 - iter 530/1065 - loss 0.19917187\n",
      "2019-07-23 01:01:53,821 epoch 20 - iter 636/1065 - loss 0.19724943\n",
      "2019-07-23 01:02:13,551 epoch 20 - iter 742/1065 - loss 0.19572067\n",
      "2019-07-23 01:02:32,900 epoch 20 - iter 848/1065 - loss 0.19625356\n",
      "2019-07-23 01:02:52,796 epoch 20 - iter 954/1065 - loss 0.19706725\n",
      "2019-07-23 01:03:12,880 epoch 20 - iter 1060/1065 - loss 0.19845630\n",
      "2019-07-23 01:03:13,788 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:03:13,789 EPOCH 20 done: loss 0.1981 - lr 0.0500\n",
      "2019-07-23 01:03:37,868 DEV : loss 0.2856917679309845 - score 0.5587\n",
      "2019-07-23 01:03:38,513 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 01:03:38,515 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:03:38,580 epoch 21 - iter 0/1065 - loss 0.10754682\n",
      "2019-07-23 01:03:54,825 epoch 21 - iter 106/1065 - loss 0.17815257\n",
      "2019-07-23 01:04:09,765 epoch 21 - iter 212/1065 - loss 0.17525693\n",
      "2019-07-23 01:04:26,565 epoch 21 - iter 318/1065 - loss 0.18300163\n",
      "2019-07-23 01:04:41,644 epoch 21 - iter 424/1065 - loss 0.18610167\n",
      "2019-07-23 01:04:58,697 epoch 21 - iter 530/1065 - loss 0.18687713\n",
      "2019-07-23 01:05:14,418 epoch 21 - iter 636/1065 - loss 0.18922375\n",
      "2019-07-23 01:05:30,776 epoch 21 - iter 742/1065 - loss 0.18758167\n",
      "2019-07-23 01:05:47,167 epoch 21 - iter 848/1065 - loss 0.18675524\n",
      "2019-07-23 01:06:04,690 epoch 21 - iter 954/1065 - loss 0.18806361\n",
      "2019-07-23 01:06:20,584 epoch 21 - iter 1060/1065 - loss 0.18835317\n",
      "2019-07-23 01:06:21,007 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:06:21,008 EPOCH 21 done: loss 0.1884 - lr 0.0500\n",
      "2019-07-23 01:06:38,995 DEV : loss 0.2907153367996216 - score 0.5631\n",
      "2019-07-23 01:06:39,678 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 01:06:39,679 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:06:39,731 epoch 22 - iter 0/1065 - loss 0.08030514\n",
      "2019-07-23 01:06:53,048 epoch 22 - iter 106/1065 - loss 0.17835893\n",
      "2019-07-23 01:07:05,915 epoch 22 - iter 212/1065 - loss 0.17193502\n",
      "2019-07-23 01:07:19,293 epoch 22 - iter 318/1065 - loss 0.17927372\n",
      "2019-07-23 01:07:31,712 epoch 22 - iter 424/1065 - loss 0.18253465\n",
      "2019-07-23 01:07:44,809 epoch 22 - iter 530/1065 - loss 0.18504207\n",
      "2019-07-23 01:07:57,137 epoch 22 - iter 636/1065 - loss 0.18442756\n",
      "2019-07-23 01:08:09,589 epoch 22 - iter 742/1065 - loss 0.18408747\n",
      "2019-07-23 01:08:21,757 epoch 22 - iter 848/1065 - loss 0.18264868\n",
      "2019-07-23 01:08:34,448 epoch 22 - iter 954/1065 - loss 0.18386881\n",
      "2019-07-23 01:08:46,766 epoch 22 - iter 1060/1065 - loss 0.18487377\n",
      "2019-07-23 01:08:47,199 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:08:47,201 EPOCH 22 done: loss 0.1850 - lr 0.0500\n",
      "2019-07-23 01:09:04,203 DEV : loss 0.2971858084201813 - score 0.548\n",
      "2019-07-23 01:09:04,773 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 01:09:04,775 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:09:04,851 epoch 23 - iter 0/1065 - loss 0.39398405\n",
      "2019-07-23 01:09:18,177 epoch 23 - iter 106/1065 - loss 0.17511661\n",
      "2019-07-23 01:09:31,269 epoch 23 - iter 212/1065 - loss 0.17451847\n",
      "2019-07-23 01:09:44,381 epoch 23 - iter 318/1065 - loss 0.17988385\n",
      "2019-07-23 01:09:57,699 epoch 23 - iter 424/1065 - loss 0.18035503\n",
      "2019-07-23 01:10:10,841 epoch 23 - iter 530/1065 - loss 0.17855924\n",
      "2019-07-23 01:10:23,716 epoch 23 - iter 636/1065 - loss 0.17771129\n",
      "2019-07-23 01:10:36,544 epoch 23 - iter 742/1065 - loss 0.18240794\n",
      "2019-07-23 01:10:48,467 epoch 23 - iter 848/1065 - loss 0.18253026\n",
      "2019-07-23 01:11:00,968 epoch 23 - iter 954/1065 - loss 0.18247202\n",
      "2019-07-23 01:11:13,068 epoch 23 - iter 1060/1065 - loss 0.18154096\n",
      "2019-07-23 01:11:13,633 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:11:13,634 EPOCH 23 done: loss 0.1817 - lr 0.0500\n",
      "2019-07-23 01:11:37,403 DEV : loss 0.32338953018188477 - score 0.5738\n",
      "Epoch    22: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2019-07-23 01:11:38,069 BAD EPOCHS (no improvement): 4\n",
      "2019-07-23 01:11:38,070 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:11:38,308 epoch 24 - iter 0/1065 - loss 0.18206476\n",
      "2019-07-23 01:11:54,213 epoch 24 - iter 106/1065 - loss 0.15078255\n",
      "2019-07-23 01:12:10,841 epoch 24 - iter 212/1065 - loss 0.15585869\n",
      "2019-07-23 01:12:27,022 epoch 24 - iter 318/1065 - loss 0.15505175\n",
      "2019-07-23 01:12:42,274 epoch 24 - iter 424/1065 - loss 0.15689045\n",
      "2019-07-23 01:12:59,109 epoch 24 - iter 530/1065 - loss 0.15837085\n",
      "2019-07-23 01:13:14,348 epoch 24 - iter 636/1065 - loss 0.15796867\n",
      "2019-07-23 01:13:30,449 epoch 24 - iter 742/1065 - loss 0.15903949\n",
      "2019-07-23 01:13:45,785 epoch 24 - iter 848/1065 - loss 0.15968687\n",
      "2019-07-23 01:14:01,715 epoch 24 - iter 954/1065 - loss 0.16132425\n",
      "2019-07-23 01:14:17,863 epoch 24 - iter 1060/1065 - loss 0.16221758\n",
      "2019-07-23 01:14:18,344 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:14:18,345 EPOCH 24 done: loss 0.1620 - lr 0.0250\n",
      "2019-07-23 01:14:41,342 DEV : loss 0.32006269693374634 - score 0.5452\n",
      "2019-07-23 01:14:41,990 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 01:14:41,992 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:14:42,145 epoch 25 - iter 0/1065 - loss 0.10409418\n",
      "2019-07-23 01:14:56,913 epoch 25 - iter 106/1065 - loss 0.16393147\n",
      "2019-07-23 01:15:13,641 epoch 25 - iter 212/1065 - loss 0.16004507\n",
      "2019-07-23 01:15:30,826 epoch 25 - iter 318/1065 - loss 0.15961931\n",
      "2019-07-23 01:15:46,892 epoch 25 - iter 424/1065 - loss 0.16100225\n",
      "2019-07-23 01:16:04,513 epoch 25 - iter 530/1065 - loss 0.16163918\n",
      "2019-07-23 01:16:19,884 epoch 25 - iter 636/1065 - loss 0.16257034\n",
      "2019-07-23 01:16:36,185 epoch 25 - iter 742/1065 - loss 0.16068989\n",
      "2019-07-23 01:16:52,153 epoch 25 - iter 848/1065 - loss 0.15930935\n",
      "2019-07-23 01:17:09,051 epoch 25 - iter 954/1065 - loss 0.15866252\n",
      "2019-07-23 01:17:25,229 epoch 25 - iter 1060/1065 - loss 0.15795989\n",
      "2019-07-23 01:17:26,131 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:17:26,133 EPOCH 25 done: loss 0.1582 - lr 0.0250\n",
      "2019-07-23 01:17:47,492 DEV : loss 0.32946109771728516 - score 0.5693\n",
      "2019-07-23 01:17:48,074 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 01:17:48,076 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:17:48,189 epoch 26 - iter 0/1065 - loss 0.16873251\n",
      "2019-07-23 01:18:03,858 epoch 26 - iter 106/1065 - loss 0.14582395\n",
      "2019-07-23 01:18:19,106 epoch 26 - iter 212/1065 - loss 0.14664517\n",
      "2019-07-23 01:18:34,635 epoch 26 - iter 318/1065 - loss 0.14706682\n",
      "2019-07-23 01:18:50,396 epoch 26 - iter 424/1065 - loss 0.14558689\n",
      "2019-07-23 01:19:06,622 epoch 26 - iter 530/1065 - loss 0.14881530\n",
      "2019-07-23 01:19:22,264 epoch 26 - iter 636/1065 - loss 0.15016885\n",
      "2019-07-23 01:19:39,208 epoch 26 - iter 742/1065 - loss 0.15170726\n",
      "2019-07-23 01:19:55,962 epoch 26 - iter 848/1065 - loss 0.15197411\n",
      "2019-07-23 01:20:11,228 epoch 26 - iter 954/1065 - loss 0.15285596\n",
      "2019-07-23 01:20:27,187 epoch 26 - iter 1060/1065 - loss 0.15234353\n",
      "2019-07-23 01:20:27,923 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:20:27,925 EPOCH 26 done: loss 0.1523 - lr 0.0250\n",
      "2019-07-23 01:20:59,574 DEV : loss 0.3379247784614563 - score 0.5546\n",
      "2019-07-23 01:21:00,139 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 01:21:00,141 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:21:00,218 epoch 27 - iter 0/1065 - loss 0.09529742\n",
      "2019-07-23 01:21:18,652 epoch 27 - iter 106/1065 - loss 0.14306600\n",
      "2019-07-23 01:21:37,632 epoch 27 - iter 212/1065 - loss 0.14284164\n",
      "2019-07-23 01:21:55,203 epoch 27 - iter 318/1065 - loss 0.14771766\n",
      "2019-07-23 01:22:13,062 epoch 27 - iter 424/1065 - loss 0.14883399\n",
      "2019-07-23 01:22:32,559 epoch 27 - iter 530/1065 - loss 0.14906298\n",
      "2019-07-23 01:22:50,744 epoch 27 - iter 636/1065 - loss 0.14903593\n",
      "2019-07-23 01:23:07,118 epoch 27 - iter 742/1065 - loss 0.14969442\n",
      "2019-07-23 01:23:25,226 epoch 27 - iter 848/1065 - loss 0.15259237\n",
      "2019-07-23 01:23:44,955 epoch 27 - iter 954/1065 - loss 0.15270252\n",
      "2019-07-23 01:24:02,019 epoch 27 - iter 1060/1065 - loss 0.15291999\n",
      "2019-07-23 01:24:02,380 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:24:02,381 EPOCH 27 done: loss 0.1527 - lr 0.0250\n",
      "2019-07-23 01:24:35,338 DEV : loss 0.34946903586387634 - score 0.5506\n",
      "Epoch    26: reducing learning rate of group 0 to 1.2500e-02.\n",
      "2019-07-23 01:24:35,911 BAD EPOCHS (no improvement): 4\n",
      "2019-07-23 01:24:35,912 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:24:36,101 epoch 28 - iter 0/1065 - loss 0.11262003\n",
      "2019-07-23 01:24:54,402 epoch 28 - iter 106/1065 - loss 0.13974817\n",
      "2019-07-23 01:25:11,385 epoch 28 - iter 212/1065 - loss 0.13543672\n",
      "2019-07-23 01:25:29,941 epoch 28 - iter 318/1065 - loss 0.13397473\n",
      "2019-07-23 01:25:48,411 epoch 28 - iter 424/1065 - loss 0.13298192\n",
      "2019-07-23 01:26:07,511 epoch 28 - iter 530/1065 - loss 0.13551028\n",
      "2019-07-23 01:26:25,571 epoch 28 - iter 636/1065 - loss 0.13582858\n",
      "2019-07-23 01:26:45,622 epoch 28 - iter 742/1065 - loss 0.13513053\n",
      "2019-07-23 01:27:04,196 epoch 28 - iter 848/1065 - loss 0.13628595\n",
      "2019-07-23 01:27:22,922 epoch 28 - iter 954/1065 - loss 0.13583296\n",
      "2019-07-23 01:27:40,699 epoch 28 - iter 1060/1065 - loss 0.13623149\n",
      "2019-07-23 01:27:40,925 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:27:40,927 EPOCH 28 done: loss 0.1364 - lr 0.0125\n",
      "2019-07-23 01:27:50,426 DEV : loss 0.35224536061286926 - score 0.5691\n",
      "2019-07-23 01:27:51,087 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 01:27:51,089 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:27:51,181 epoch 29 - iter 0/1065 - loss 0.12421028\n",
      "2019-07-23 01:27:59,721 epoch 29 - iter 106/1065 - loss 0.13235005\n",
      "2019-07-23 01:28:07,178 epoch 29 - iter 212/1065 - loss 0.13350808\n",
      "2019-07-23 01:28:14,631 epoch 29 - iter 318/1065 - loss 0.13268816\n",
      "2019-07-23 01:28:22,504 epoch 29 - iter 424/1065 - loss 0.13366413\n",
      "2019-07-23 01:28:29,633 epoch 29 - iter 530/1065 - loss 0.13491598\n",
      "2019-07-23 01:28:37,187 epoch 29 - iter 636/1065 - loss 0.13658066\n",
      "2019-07-23 01:28:44,027 epoch 29 - iter 742/1065 - loss 0.13615278\n",
      "2019-07-23 01:28:50,814 epoch 29 - iter 848/1065 - loss 0.13579190\n",
      "2019-07-23 01:28:58,099 epoch 29 - iter 954/1065 - loss 0.13619069\n",
      "2019-07-23 01:29:06,118 epoch 29 - iter 1060/1065 - loss 0.13582448\n",
      "2019-07-23 01:29:06,456 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:29:06,457 EPOCH 29 done: loss 0.1359 - lr 0.0125\n",
      "2019-07-23 01:29:16,465 DEV : loss 0.3593159019947052 - score 0.5661\n",
      "2019-07-23 01:29:17,104 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 01:29:17,106 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:29:17,193 epoch 30 - iter 0/1065 - loss 0.34117153\n",
      "2019-07-23 01:29:24,529 epoch 30 - iter 106/1065 - loss 0.12690186\n",
      "2019-07-23 01:29:32,193 epoch 30 - iter 212/1065 - loss 0.12403548\n",
      "2019-07-23 01:29:39,158 epoch 30 - iter 318/1065 - loss 0.12684394\n",
      "2019-07-23 01:29:46,430 epoch 30 - iter 424/1065 - loss 0.12745568\n",
      "2019-07-23 01:29:53,010 epoch 30 - iter 530/1065 - loss 0.12785198\n",
      "2019-07-23 01:30:00,270 epoch 30 - iter 636/1065 - loss 0.13010825\n",
      "2019-07-23 01:30:06,925 epoch 30 - iter 742/1065 - loss 0.12969128\n",
      "2019-07-23 01:30:14,015 epoch 30 - iter 848/1065 - loss 0.13043120\n",
      "2019-07-23 01:30:21,935 epoch 30 - iter 954/1065 - loss 0.13045066\n",
      "2019-07-23 01:30:29,921 epoch 30 - iter 1060/1065 - loss 0.13032860\n",
      "2019-07-23 01:30:30,159 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:30:30,160 EPOCH 30 done: loss 0.1305 - lr 0.0125\n",
      "2019-07-23 01:30:40,399 DEV : loss 0.37238067388534546 - score 0.5482\n",
      "2019-07-23 01:30:40,989 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 01:30:40,991 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:30:41,103 epoch 31 - iter 0/1065 - loss 0.09710280\n",
      "2019-07-23 01:30:48,337 epoch 31 - iter 106/1065 - loss 0.13101283\n",
      "2019-07-23 01:30:55,686 epoch 31 - iter 212/1065 - loss 0.12823625\n",
      "2019-07-23 01:31:03,210 epoch 31 - iter 318/1065 - loss 0.12930516\n",
      "2019-07-23 01:31:10,554 epoch 31 - iter 424/1065 - loss 0.12621738\n",
      "2019-07-23 01:31:18,604 epoch 31 - iter 530/1065 - loss 0.12646658\n",
      "2019-07-23 01:31:25,923 epoch 31 - iter 636/1065 - loss 0.12760493\n",
      "2019-07-23 01:31:32,674 epoch 31 - iter 742/1065 - loss 0.12955826\n",
      "2019-07-23 01:31:39,906 epoch 31 - iter 848/1065 - loss 0.12928603\n",
      "2019-07-23 01:31:47,513 epoch 31 - iter 954/1065 - loss 0.12879810\n",
      "2019-07-23 01:31:55,371 epoch 31 - iter 1060/1065 - loss 0.12897689\n",
      "2019-07-23 01:31:55,740 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:31:55,742 EPOCH 31 done: loss 0.1289 - lr 0.0125\n",
      "2019-07-23 01:32:05,454 DEV : loss 0.37518826127052307 - score 0.5622\n",
      "Epoch    30: reducing learning rate of group 0 to 6.2500e-03.\n",
      "2019-07-23 01:32:06,013 BAD EPOCHS (no improvement): 4\n",
      "2019-07-23 01:32:06,015 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:32:06,062 epoch 32 - iter 0/1065 - loss 0.32868105\n",
      "2019-07-23 01:32:14,299 epoch 32 - iter 106/1065 - loss 0.13104332\n",
      "2019-07-23 01:32:21,713 epoch 32 - iter 212/1065 - loss 0.12375822\n",
      "2019-07-23 01:32:28,824 epoch 32 - iter 318/1065 - loss 0.12506324\n",
      "2019-07-23 01:32:36,725 epoch 32 - iter 424/1065 - loss 0.12489931\n",
      "2019-07-23 01:32:44,621 epoch 32 - iter 530/1065 - loss 0.12444349\n",
      "2019-07-23 01:32:51,166 epoch 32 - iter 636/1065 - loss 0.12650958\n",
      "2019-07-23 01:32:59,033 epoch 32 - iter 742/1065 - loss 0.12456482\n",
      "2019-07-23 01:33:07,135 epoch 32 - iter 848/1065 - loss 0.12495855\n",
      "2019-07-23 01:33:14,973 epoch 32 - iter 954/1065 - loss 0.12535967\n",
      "2019-07-23 01:33:22,363 epoch 32 - iter 1060/1065 - loss 0.12553835\n",
      "2019-07-23 01:33:22,790 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:33:22,792 EPOCH 32 done: loss 0.1256 - lr 0.0063\n",
      "2019-07-23 01:33:32,708 DEV : loss 0.3795841932296753 - score 0.569\n",
      "2019-07-23 01:33:33,275 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 01:33:33,276 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:33:33,328 epoch 33 - iter 0/1065 - loss 0.24686149\n",
      "2019-07-23 01:33:40,372 epoch 33 - iter 106/1065 - loss 0.13563441\n",
      "2019-07-23 01:33:47,644 epoch 33 - iter 212/1065 - loss 0.12888745\n",
      "2019-07-23 01:33:54,588 epoch 33 - iter 318/1065 - loss 0.12964914\n",
      "2019-07-23 01:34:03,049 epoch 33 - iter 424/1065 - loss 0.12825899\n",
      "2019-07-23 01:34:10,549 epoch 33 - iter 530/1065 - loss 0.12817745\n",
      "2019-07-23 01:34:17,682 epoch 33 - iter 636/1065 - loss 0.12657117\n",
      "2019-07-23 01:34:26,114 epoch 33 - iter 742/1065 - loss 0.12477038\n",
      "2019-07-23 01:34:33,306 epoch 33 - iter 848/1065 - loss 0.12465480\n",
      "2019-07-23 01:34:40,699 epoch 33 - iter 954/1065 - loss 0.12331235\n",
      "2019-07-23 01:34:48,855 epoch 33 - iter 1060/1065 - loss 0.12373016\n",
      "2019-07-23 01:34:49,143 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:34:49,144 EPOCH 33 done: loss 0.1236 - lr 0.0063\n",
      "2019-07-23 01:34:58,917 DEV : loss 0.38284817337989807 - score 0.5648\n",
      "2019-07-23 01:34:59,531 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 01:34:59,533 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:34:59,584 epoch 34 - iter 0/1065 - loss 0.15920863\n",
      "2019-07-23 01:35:06,905 epoch 34 - iter 106/1065 - loss 0.10661979\n",
      "2019-07-23 01:35:13,944 epoch 34 - iter 212/1065 - loss 0.11962175\n",
      "2019-07-23 01:35:20,814 epoch 34 - iter 318/1065 - loss 0.12352781\n",
      "2019-07-23 01:35:27,900 epoch 34 - iter 424/1065 - loss 0.12232298\n",
      "2019-07-23 01:35:35,098 epoch 34 - iter 530/1065 - loss 0.12294359\n",
      "2019-07-23 01:35:43,046 epoch 34 - iter 636/1065 - loss 0.12099036\n",
      "2019-07-23 01:35:50,466 epoch 34 - iter 742/1065 - loss 0.12064139\n",
      "2019-07-23 01:35:58,087 epoch 34 - iter 848/1065 - loss 0.12168200\n",
      "2019-07-23 01:36:05,089 epoch 34 - iter 954/1065 - loss 0.12243949\n",
      "2019-07-23 01:36:12,367 epoch 34 - iter 1060/1065 - loss 0.12246751\n",
      "2019-07-23 01:36:12,641 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:36:12,642 EPOCH 34 done: loss 0.1224 - lr 0.0063\n",
      "2019-07-23 01:36:22,771 DEV : loss 0.3789568841457367 - score 0.5614\n",
      "2019-07-23 01:36:23,415 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 01:36:23,417 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:36:23,592 epoch 35 - iter 0/1065 - loss 0.13701981\n",
      "2019-07-23 01:36:31,053 epoch 35 - iter 106/1065 - loss 0.11620218\n",
      "2019-07-23 01:36:38,870 epoch 35 - iter 212/1065 - loss 0.11307899\n",
      "2019-07-23 01:36:46,795 epoch 35 - iter 318/1065 - loss 0.11668913\n",
      "2019-07-23 01:36:53,960 epoch 35 - iter 424/1065 - loss 0.11813089\n",
      "2019-07-23 01:37:01,064 epoch 35 - iter 530/1065 - loss 0.11955261\n",
      "2019-07-23 01:37:08,448 epoch 35 - iter 636/1065 - loss 0.11967316\n",
      "2019-07-23 01:37:16,527 epoch 35 - iter 742/1065 - loss 0.11909291\n",
      "2019-07-23 01:37:24,440 epoch 35 - iter 848/1065 - loss 0.11734135\n",
      "2019-07-23 01:37:31,102 epoch 35 - iter 954/1065 - loss 0.11772773\n",
      "2019-07-23 01:37:39,114 epoch 35 - iter 1060/1065 - loss 0.11906880\n",
      "2019-07-23 01:37:39,497 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:37:39,498 EPOCH 35 done: loss 0.1191 - lr 0.0063\n",
      "2019-07-23 01:37:49,687 DEV : loss 0.3859367370605469 - score 0.558\n",
      "Epoch    34: reducing learning rate of group 0 to 3.1250e-03.\n",
      "2019-07-23 01:37:50,296 BAD EPOCHS (no improvement): 4\n",
      "2019-07-23 01:37:50,298 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:37:50,375 epoch 36 - iter 0/1065 - loss 0.07343253\n",
      "2019-07-23 01:37:57,785 epoch 36 - iter 106/1065 - loss 0.11941306\n",
      "2019-07-23 01:38:05,489 epoch 36 - iter 212/1065 - loss 0.11257740\n",
      "2019-07-23 01:38:13,226 epoch 36 - iter 318/1065 - loss 0.11594800\n",
      "2019-07-23 01:38:20,216 epoch 36 - iter 424/1065 - loss 0.11702359\n",
      "2019-07-23 01:38:27,531 epoch 36 - iter 530/1065 - loss 0.11465888\n",
      "2019-07-23 01:38:34,673 epoch 36 - iter 636/1065 - loss 0.11591929\n",
      "2019-07-23 01:38:42,314 epoch 36 - iter 742/1065 - loss 0.11593468\n",
      "2019-07-23 01:38:50,410 epoch 36 - iter 848/1065 - loss 0.11603327\n",
      "2019-07-23 01:38:58,670 epoch 36 - iter 954/1065 - loss 0.11649405\n",
      "2019-07-23 01:39:06,353 epoch 36 - iter 1060/1065 - loss 0.11579283\n",
      "2019-07-23 01:39:06,817 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:39:06,819 EPOCH 36 done: loss 0.1158 - lr 0.0031\n",
      "2019-07-23 01:39:17,131 DEV : loss 0.39131680130958557 - score 0.5646\n",
      "2019-07-23 01:39:17,722 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 01:39:17,723 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:39:17,799 epoch 37 - iter 0/1065 - loss 0.01965953\n",
      "2019-07-23 01:39:25,161 epoch 37 - iter 106/1065 - loss 0.11124397\n",
      "2019-07-23 01:39:32,729 epoch 37 - iter 212/1065 - loss 0.11293473\n",
      "2019-07-23 01:39:41,185 epoch 37 - iter 318/1065 - loss 0.11388999\n",
      "2019-07-23 01:39:49,069 epoch 37 - iter 424/1065 - loss 0.11424727\n",
      "2019-07-23 01:39:57,241 epoch 37 - iter 530/1065 - loss 0.11203446\n",
      "2019-07-23 01:40:04,309 epoch 37 - iter 636/1065 - loss 0.11290653\n",
      "2019-07-23 01:40:11,577 epoch 37 - iter 742/1065 - loss 0.11401587\n",
      "2019-07-23 01:40:19,282 epoch 37 - iter 848/1065 - loss 0.11316561\n",
      "2019-07-23 01:40:26,445 epoch 37 - iter 954/1065 - loss 0.11247009\n",
      "2019-07-23 01:40:34,194 epoch 37 - iter 1060/1065 - loss 0.11382371\n",
      "2019-07-23 01:40:34,441 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:40:34,442 EPOCH 37 done: loss 0.1138 - lr 0.0031\n",
      "2019-07-23 01:40:43,696 DEV : loss 0.3980424404144287 - score 0.5591\n",
      "2019-07-23 01:40:44,351 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 01:40:44,353 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:40:44,408 epoch 38 - iter 0/1065 - loss 0.19079281\n",
      "2019-07-23 01:40:52,264 epoch 38 - iter 106/1065 - loss 0.11478107\n",
      "2019-07-23 01:40:59,995 epoch 38 - iter 212/1065 - loss 0.11939859\n",
      "2019-07-23 01:41:07,035 epoch 38 - iter 318/1065 - loss 0.11967547\n",
      "2019-07-23 01:41:13,819 epoch 38 - iter 424/1065 - loss 0.11975928\n",
      "2019-07-23 01:41:21,851 epoch 38 - iter 530/1065 - loss 0.11698224\n",
      "2019-07-23 01:41:29,678 epoch 38 - iter 636/1065 - loss 0.11594410\n",
      "2019-07-23 01:41:37,621 epoch 38 - iter 742/1065 - loss 0.11366579\n",
      "2019-07-23 01:41:45,045 epoch 38 - iter 848/1065 - loss 0.11389718\n",
      "2019-07-23 01:41:52,496 epoch 38 - iter 954/1065 - loss 0.11432068\n",
      "2019-07-23 01:42:00,178 epoch 38 - iter 1060/1065 - loss 0.11441133\n",
      "2019-07-23 01:42:00,509 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:42:00,510 EPOCH 38 done: loss 0.1143 - lr 0.0031\n",
      "2019-07-23 01:42:10,336 DEV : loss 0.3999635577201843 - score 0.5623\n",
      "2019-07-23 01:42:10,963 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 01:42:10,965 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:42:11,021 epoch 39 - iter 0/1065 - loss 0.02477145\n",
      "2019-07-23 01:42:18,517 epoch 39 - iter 106/1065 - loss 0.11769218\n",
      "2019-07-23 01:42:26,604 epoch 39 - iter 212/1065 - loss 0.11105873\n",
      "2019-07-23 01:42:33,939 epoch 39 - iter 318/1065 - loss 0.10989343\n",
      "2019-07-23 01:42:41,093 epoch 39 - iter 424/1065 - loss 0.11257778\n",
      "2019-07-23 01:42:48,828 epoch 39 - iter 530/1065 - loss 0.11562991\n",
      "2019-07-23 01:42:57,037 epoch 39 - iter 636/1065 - loss 0.11702651\n",
      "2019-07-23 01:43:03,986 epoch 39 - iter 742/1065 - loss 0.11756360\n",
      "2019-07-23 01:43:11,809 epoch 39 - iter 848/1065 - loss 0.11607498\n",
      "2019-07-23 01:43:19,622 epoch 39 - iter 954/1065 - loss 0.11821851\n",
      "2019-07-23 01:43:28,514 epoch 39 - iter 1060/1065 - loss 0.11594004\n",
      "2019-07-23 01:43:28,920 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:43:28,922 EPOCH 39 done: loss 0.1160 - lr 0.0031\n",
      "2019-07-23 01:43:38,739 DEV : loss 0.39913713932037354 - score 0.565\n",
      "Epoch    38: reducing learning rate of group 0 to 1.5625e-03.\n",
      "2019-07-23 01:43:39,394 BAD EPOCHS (no improvement): 4\n",
      "2019-07-23 01:43:39,395 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:43:39,487 epoch 40 - iter 0/1065 - loss 0.05098192\n",
      "2019-07-23 01:43:46,892 epoch 40 - iter 106/1065 - loss 0.10467840\n",
      "2019-07-23 01:43:54,391 epoch 40 - iter 212/1065 - loss 0.11243461\n",
      "2019-07-23 01:44:01,873 epoch 40 - iter 318/1065 - loss 0.10925713\n",
      "2019-07-23 01:44:09,524 epoch 40 - iter 424/1065 - loss 0.10857297\n",
      "2019-07-23 01:44:16,845 epoch 40 - iter 530/1065 - loss 0.11029504\n",
      "2019-07-23 01:44:25,206 epoch 40 - iter 636/1065 - loss 0.11035016\n",
      "2019-07-23 01:44:32,381 epoch 40 - iter 742/1065 - loss 0.11076784\n",
      "2019-07-23 01:44:40,565 epoch 40 - iter 848/1065 - loss 0.11082838\n",
      "2019-07-23 01:44:49,137 epoch 40 - iter 954/1065 - loss 0.11125515\n",
      "2019-07-23 01:44:56,844 epoch 40 - iter 1060/1065 - loss 0.11063335\n",
      "2019-07-23 01:44:57,159 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:44:57,160 EPOCH 40 done: loss 0.1106 - lr 0.0016\n",
      "2019-07-23 01:45:06,624 DEV : loss 0.40240371227264404 - score 0.5653\n",
      "2019-07-23 01:45:07,182 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 01:45:07,183 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:45:07,234 epoch 41 - iter 0/1065 - loss 0.10662874\n",
      "2019-07-23 01:45:14,667 epoch 41 - iter 106/1065 - loss 0.11912998\n",
      "2019-07-23 01:45:22,544 epoch 41 - iter 212/1065 - loss 0.11339621\n",
      "2019-07-23 01:45:29,434 epoch 41 - iter 318/1065 - loss 0.11298019\n",
      "2019-07-23 01:45:36,683 epoch 41 - iter 424/1065 - loss 0.11279101\n",
      "2019-07-23 01:45:44,024 epoch 41 - iter 530/1065 - loss 0.11241661\n",
      "2019-07-23 01:45:52,208 epoch 41 - iter 636/1065 - loss 0.11286142\n",
      "2019-07-23 01:46:00,183 epoch 41 - iter 742/1065 - loss 0.11319804\n",
      "2019-07-23 01:46:07,625 epoch 41 - iter 848/1065 - loss 0.11435607\n",
      "2019-07-23 01:46:14,913 epoch 41 - iter 954/1065 - loss 0.11377600\n",
      "2019-07-23 01:46:23,103 epoch 41 - iter 1060/1065 - loss 0.11275325\n",
      "2019-07-23 01:46:23,349 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:46:23,350 EPOCH 41 done: loss 0.1130 - lr 0.0016\n",
      "2019-07-23 01:46:32,855 DEV : loss 0.40088769793510437 - score 0.5654\n",
      "2019-07-23 01:46:33,499 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 01:46:33,501 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:46:33,620 epoch 42 - iter 0/1065 - loss 0.12844677\n",
      "2019-07-23 01:46:41,062 epoch 42 - iter 106/1065 - loss 0.11188397\n",
      "2019-07-23 01:46:48,265 epoch 42 - iter 212/1065 - loss 0.11121131\n",
      "2019-07-23 01:46:55,670 epoch 42 - iter 318/1065 - loss 0.11139787\n",
      "2019-07-23 01:47:04,070 epoch 42 - iter 424/1065 - loss 0.11364562\n",
      "2019-07-23 01:47:11,766 epoch 42 - iter 530/1065 - loss 0.11283524\n",
      "2019-07-23 01:47:18,631 epoch 42 - iter 636/1065 - loss 0.11193051\n",
      "2019-07-23 01:47:26,937 epoch 42 - iter 742/1065 - loss 0.11043184\n",
      "2019-07-23 01:47:35,065 epoch 42 - iter 848/1065 - loss 0.11011441\n",
      "2019-07-23 01:47:42,740 epoch 42 - iter 954/1065 - loss 0.11113664\n",
      "2019-07-23 01:47:50,980 epoch 42 - iter 1060/1065 - loss 0.10989894\n",
      "2019-07-23 01:47:51,245 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:47:51,246 EPOCH 42 done: loss 0.1099 - lr 0.0016\n",
      "2019-07-23 01:48:00,801 DEV : loss 0.4025755822658539 - score 0.5685\n",
      "2019-07-23 01:48:01,397 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 01:48:01,400 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:48:01,455 epoch 43 - iter 0/1065 - loss 0.22853535\n",
      "2019-07-23 01:48:08,691 epoch 43 - iter 106/1065 - loss 0.11021521\n",
      "2019-07-23 01:48:16,513 epoch 43 - iter 212/1065 - loss 0.11302926\n",
      "2019-07-23 01:48:23,440 epoch 43 - iter 318/1065 - loss 0.11260054\n",
      "2019-07-23 01:48:30,766 epoch 43 - iter 424/1065 - loss 0.11070428\n",
      "2019-07-23 01:48:38,528 epoch 43 - iter 530/1065 - loss 0.10951803\n",
      "2019-07-23 01:48:45,880 epoch 43 - iter 636/1065 - loss 0.10851082\n",
      "2019-07-23 01:48:53,606 epoch 43 - iter 742/1065 - loss 0.10895081\n",
      "2019-07-23 01:49:01,190 epoch 43 - iter 848/1065 - loss 0.10972151\n",
      "2019-07-23 01:49:07,707 epoch 43 - iter 954/1065 - loss 0.10927842\n",
      "2019-07-23 01:49:15,076 epoch 43 - iter 1060/1065 - loss 0.10876727\n",
      "2019-07-23 01:49:15,364 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:49:15,365 EPOCH 43 done: loss 0.1087 - lr 0.0016\n",
      "2019-07-23 01:49:24,782 DEV : loss 0.4060433804988861 - score 0.5624\n",
      "Epoch    42: reducing learning rate of group 0 to 7.8125e-04.\n",
      "2019-07-23 01:49:25,432 BAD EPOCHS (no improvement): 4\n",
      "2019-07-23 01:49:25,434 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:49:25,520 epoch 44 - iter 0/1065 - loss 0.11425588\n",
      "2019-07-23 01:49:33,276 epoch 44 - iter 106/1065 - loss 0.11492924\n",
      "2019-07-23 01:49:40,318 epoch 44 - iter 212/1065 - loss 0.10442903\n",
      "2019-07-23 01:49:47,492 epoch 44 - iter 318/1065 - loss 0.11019137\n",
      "2019-07-23 01:49:55,179 epoch 44 - iter 424/1065 - loss 0.11048233\n",
      "2019-07-23 01:50:02,513 epoch 44 - iter 530/1065 - loss 0.11181984\n",
      "2019-07-23 01:50:09,643 epoch 44 - iter 636/1065 - loss 0.11188541\n",
      "2019-07-23 01:50:17,143 epoch 44 - iter 742/1065 - loss 0.11186531\n",
      "2019-07-23 01:50:24,794 epoch 44 - iter 848/1065 - loss 0.11251769\n",
      "2019-07-23 01:50:33,194 epoch 44 - iter 954/1065 - loss 0.11037680\n",
      "2019-07-23 01:50:40,860 epoch 44 - iter 1060/1065 - loss 0.11027744\n",
      "2019-07-23 01:50:41,114 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:50:41,116 EPOCH 44 done: loss 0.1102 - lr 0.0008\n",
      "2019-07-23 01:50:50,791 DEV : loss 0.40628960728645325 - score 0.5658\n",
      "2019-07-23 01:50:51,433 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 01:50:51,435 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:50:51,487 epoch 45 - iter 0/1065 - loss 0.17614760\n",
      "2019-07-23 01:50:59,848 epoch 45 - iter 106/1065 - loss 0.11316713\n",
      "2019-07-23 01:51:08,027 epoch 45 - iter 212/1065 - loss 0.11052418\n",
      "2019-07-23 01:51:15,430 epoch 45 - iter 318/1065 - loss 0.11391263\n",
      "2019-07-23 01:51:22,566 epoch 45 - iter 424/1065 - loss 0.11309701\n",
      "2019-07-23 01:51:30,206 epoch 45 - iter 530/1065 - loss 0.11138587\n",
      "2019-07-23 01:51:38,131 epoch 45 - iter 636/1065 - loss 0.11300615\n",
      "2019-07-23 01:51:45,862 epoch 45 - iter 742/1065 - loss 0.11317536\n",
      "2019-07-23 01:51:52,554 epoch 45 - iter 848/1065 - loss 0.11250332\n",
      "2019-07-23 01:51:59,995 epoch 45 - iter 954/1065 - loss 0.11127613\n",
      "2019-07-23 01:52:07,644 epoch 45 - iter 1060/1065 - loss 0.11002534\n",
      "2019-07-23 01:52:07,954 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:52:07,955 EPOCH 45 done: loss 0.1098 - lr 0.0008\n",
      "2019-07-23 01:52:17,142 DEV : loss 0.407364159822464 - score 0.563\n",
      "2019-07-23 01:52:17,786 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 01:52:17,788 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:52:17,914 epoch 46 - iter 0/1065 - loss 0.11540149\n",
      "2019-07-23 01:52:25,605 epoch 46 - iter 106/1065 - loss 0.11835250\n",
      "2019-07-23 01:52:32,505 epoch 46 - iter 212/1065 - loss 0.11149590\n",
      "2019-07-23 01:52:39,460 epoch 46 - iter 318/1065 - loss 0.11012241\n",
      "2019-07-23 01:52:46,274 epoch 46 - iter 424/1065 - loss 0.11083233\n",
      "2019-07-23 01:52:53,168 epoch 46 - iter 530/1065 - loss 0.11003669\n",
      "2019-07-23 01:53:00,500 epoch 46 - iter 636/1065 - loss 0.11017834\n",
      "2019-07-23 01:53:08,025 epoch 46 - iter 742/1065 - loss 0.10995817\n",
      "2019-07-23 01:53:15,743 epoch 46 - iter 848/1065 - loss 0.10923840\n",
      "2019-07-23 01:53:23,456 epoch 46 - iter 954/1065 - loss 0.10932610\n",
      "2019-07-23 01:53:31,537 epoch 46 - iter 1060/1065 - loss 0.10951738\n",
      "2019-07-23 01:53:31,907 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:53:31,908 EPOCH 46 done: loss 0.1093 - lr 0.0008\n",
      "2019-07-23 01:53:41,045 DEV : loss 0.40854132175445557 - score 0.5617\n",
      "2019-07-23 01:53:41,687 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 01:53:41,689 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:53:41,808 epoch 47 - iter 0/1065 - loss 0.21984978\n",
      "2019-07-23 01:53:49,517 epoch 47 - iter 106/1065 - loss 0.09411405\n",
      "2019-07-23 01:53:57,561 epoch 47 - iter 212/1065 - loss 0.10566816\n",
      "2019-07-23 01:54:04,783 epoch 47 - iter 318/1065 - loss 0.10133343\n",
      "2019-07-23 01:54:12,367 epoch 47 - iter 424/1065 - loss 0.10040983\n",
      "2019-07-23 01:54:19,726 epoch 47 - iter 530/1065 - loss 0.10208496\n",
      "2019-07-23 01:54:27,346 epoch 47 - iter 636/1065 - loss 0.10316329\n",
      "2019-07-23 01:54:35,034 epoch 47 - iter 742/1065 - loss 0.10284172\n",
      "2019-07-23 01:54:42,806 epoch 47 - iter 848/1065 - loss 0.10399703\n",
      "2019-07-23 01:54:50,788 epoch 47 - iter 954/1065 - loss 0.10529337\n",
      "2019-07-23 01:54:57,687 epoch 47 - iter 1060/1065 - loss 0.10552123\n",
      "2019-07-23 01:54:57,963 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:54:57,964 EPOCH 47 done: loss 0.1057 - lr 0.0008\n",
      "2019-07-23 01:55:08,060 DEV : loss 0.41091808676719666 - score 0.5603\n",
      "Epoch    46: reducing learning rate of group 0 to 3.9063e-04.\n",
      "2019-07-23 01:55:08,640 BAD EPOCHS (no improvement): 4\n",
      "2019-07-23 01:55:08,641 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:55:08,735 epoch 48 - iter 0/1065 - loss 0.10557494\n",
      "2019-07-23 01:55:16,189 epoch 48 - iter 106/1065 - loss 0.11136694\n",
      "2019-07-23 01:55:24,286 epoch 48 - iter 212/1065 - loss 0.10525726\n",
      "2019-07-23 01:55:32,039 epoch 48 - iter 318/1065 - loss 0.10899068\n",
      "2019-07-23 01:55:40,071 epoch 48 - iter 424/1065 - loss 0.10801552\n",
      "2019-07-23 01:55:47,789 epoch 48 - iter 530/1065 - loss 0.10738493\n",
      "2019-07-23 01:55:55,949 epoch 48 - iter 636/1065 - loss 0.10894032\n",
      "2019-07-23 01:56:04,150 epoch 48 - iter 742/1065 - loss 0.10897564\n",
      "2019-07-23 01:56:11,437 epoch 48 - iter 848/1065 - loss 0.10887230\n",
      "2019-07-23 01:56:19,008 epoch 48 - iter 954/1065 - loss 0.10951910\n",
      "2019-07-23 01:56:26,230 epoch 48 - iter 1060/1065 - loss 0.11010541\n",
      "2019-07-23 01:56:26,634 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:56:26,636 EPOCH 48 done: loss 0.1101 - lr 0.0004\n",
      "2019-07-23 01:56:37,038 DEV : loss 0.4094165861606598 - score 0.5673\n",
      "2019-07-23 01:56:37,699 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 01:56:37,701 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:56:37,793 epoch 49 - iter 0/1065 - loss 0.25597471\n",
      "2019-07-23 01:56:45,966 epoch 49 - iter 106/1065 - loss 0.11421573\n",
      "2019-07-23 01:56:54,012 epoch 49 - iter 212/1065 - loss 0.10978757\n",
      "2019-07-23 01:57:01,942 epoch 49 - iter 318/1065 - loss 0.10825259\n",
      "2019-07-23 01:57:10,032 epoch 49 - iter 424/1065 - loss 0.10551362\n",
      "2019-07-23 01:57:18,072 epoch 49 - iter 530/1065 - loss 0.10779460\n",
      "2019-07-23 01:57:25,805 epoch 49 - iter 636/1065 - loss 0.10850397\n",
      "2019-07-23 01:57:33,572 epoch 49 - iter 742/1065 - loss 0.10956007\n",
      "2019-07-23 01:57:41,956 epoch 49 - iter 848/1065 - loss 0.11027451\n",
      "2019-07-23 01:57:49,503 epoch 49 - iter 954/1065 - loss 0.11012384\n",
      "2019-07-23 01:57:56,857 epoch 49 - iter 1060/1065 - loss 0.10904600\n",
      "2019-07-23 01:57:57,187 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:57:57,188 EPOCH 49 done: loss 0.1089 - lr 0.0004\n",
      "2019-07-23 01:58:06,672 DEV : loss 0.4102781116962433 - score 0.564\n",
      "2019-07-23 01:58:07,312 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 01:58:07,314 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:58:07,371 epoch 50 - iter 0/1065 - loss 0.31514564\n",
      "2019-07-23 01:58:15,028 epoch 50 - iter 106/1065 - loss 0.10770885\n",
      "2019-07-23 01:58:22,516 epoch 50 - iter 212/1065 - loss 0.10887421\n",
      "2019-07-23 01:58:29,991 epoch 50 - iter 318/1065 - loss 0.11140386\n",
      "2019-07-23 01:58:36,948 epoch 50 - iter 424/1065 - loss 0.10949763\n",
      "2019-07-23 01:58:43,931 epoch 50 - iter 530/1065 - loss 0.10801050\n",
      "2019-07-23 01:58:51,698 epoch 50 - iter 636/1065 - loss 0.10714653\n",
      "2019-07-23 01:58:59,873 epoch 50 - iter 742/1065 - loss 0.10808738\n",
      "2019-07-23 01:59:06,841 epoch 50 - iter 848/1065 - loss 0.10877503\n",
      "2019-07-23 01:59:13,659 epoch 50 - iter 954/1065 - loss 0.10805420\n",
      "2019-07-23 01:59:21,312 epoch 50 - iter 1060/1065 - loss 0.10781730\n",
      "2019-07-23 01:59:21,612 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:59:21,614 EPOCH 50 done: loss 0.1080 - lr 0.0004\n",
      "2019-07-23 01:59:31,017 DEV : loss 0.41157570481300354 - score 0.5607\n",
      "2019-07-23 01:59:31,700 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 01:59:31,702 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 01:59:31,758 epoch 51 - iter 0/1065 - loss 0.05198931\n",
      "2019-07-23 01:59:39,302 epoch 51 - iter 106/1065 - loss 0.11792867\n",
      "2019-07-23 01:59:46,282 epoch 51 - iter 212/1065 - loss 0.10915672\n",
      "2019-07-23 01:59:53,677 epoch 51 - iter 318/1065 - loss 0.11187939\n",
      "2019-07-23 02:00:00,853 epoch 51 - iter 424/1065 - loss 0.10754400\n",
      "2019-07-23 02:00:08,782 epoch 51 - iter 530/1065 - loss 0.10839324\n",
      "2019-07-23 02:00:16,769 epoch 51 - iter 636/1065 - loss 0.10849980\n",
      "2019-07-23 02:00:24,002 epoch 51 - iter 742/1065 - loss 0.10762367\n",
      "2019-07-23 02:00:30,864 epoch 51 - iter 848/1065 - loss 0.10731451\n",
      "2019-07-23 02:00:38,395 epoch 51 - iter 954/1065 - loss 0.10794840\n",
      "2019-07-23 02:00:46,795 epoch 51 - iter 1060/1065 - loss 0.10886713\n",
      "2019-07-23 02:00:47,094 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:00:47,095 EPOCH 51 done: loss 0.1088 - lr 0.0004\n",
      "2019-07-23 02:00:56,608 DEV : loss 0.41032740473747253 - score 0.5656\n",
      "Epoch    50: reducing learning rate of group 0 to 1.9531e-04.\n",
      "2019-07-23 02:00:57,179 BAD EPOCHS (no improvement): 4\n",
      "2019-07-23 02:00:57,181 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:00:57,285 epoch 52 - iter 0/1065 - loss 0.11597084\n",
      "2019-07-23 02:01:04,688 epoch 52 - iter 106/1065 - loss 0.11176320\n",
      "2019-07-23 02:01:12,234 epoch 52 - iter 212/1065 - loss 0.10813058\n",
      "2019-07-23 02:01:19,921 epoch 52 - iter 318/1065 - loss 0.10952229\n",
      "2019-07-23 02:01:28,598 epoch 52 - iter 424/1065 - loss 0.10828927\n",
      "2019-07-23 02:01:36,766 epoch 52 - iter 530/1065 - loss 0.10789202\n",
      "2019-07-23 02:01:43,843 epoch 52 - iter 636/1065 - loss 0.10860977\n",
      "2019-07-23 02:01:51,923 epoch 52 - iter 742/1065 - loss 0.11061020\n",
      "2019-07-23 02:01:59,867 epoch 52 - iter 848/1065 - loss 0.11070080\n",
      "2019-07-23 02:02:07,821 epoch 52 - iter 954/1065 - loss 0.10969595\n",
      "2019-07-23 02:02:16,365 epoch 52 - iter 1060/1065 - loss 0.10924599\n",
      "2019-07-23 02:02:16,654 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:02:16,655 EPOCH 52 done: loss 0.1091 - lr 0.0002\n",
      "2019-07-23 02:02:26,730 DEV : loss 0.4110410511493683 - score 0.5672\n",
      "2019-07-23 02:02:27,349 BAD EPOCHS (no improvement): 1\n",
      "2019-07-23 02:02:27,350 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:02:27,406 epoch 53 - iter 0/1065 - loss 0.04486549\n",
      "2019-07-23 02:02:35,562 epoch 53 - iter 106/1065 - loss 0.10806970\n",
      "2019-07-23 02:02:44,070 epoch 53 - iter 212/1065 - loss 0.10620047\n",
      "2019-07-23 02:02:51,355 epoch 53 - iter 318/1065 - loss 0.10651006\n",
      "2019-07-23 02:02:58,989 epoch 53 - iter 424/1065 - loss 0.10500505\n",
      "2019-07-23 02:03:06,330 epoch 53 - iter 530/1065 - loss 0.10500814\n",
      "2019-07-23 02:03:14,149 epoch 53 - iter 636/1065 - loss 0.10533381\n",
      "2019-07-23 02:03:21,734 epoch 53 - iter 742/1065 - loss 0.10759115\n",
      "2019-07-23 02:03:29,791 epoch 53 - iter 848/1065 - loss 0.10762184\n",
      "2019-07-23 02:03:37,408 epoch 53 - iter 954/1065 - loss 0.10832943\n",
      "2019-07-23 02:03:45,605 epoch 53 - iter 1060/1065 - loss 0.10857444\n",
      "2019-07-23 02:03:46,033 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:03:46,035 EPOCH 53 done: loss 0.1084 - lr 0.0002\n",
      "2019-07-23 02:03:56,154 DEV : loss 0.41137897968292236 - score 0.5642\n",
      "2019-07-23 02:03:56,834 BAD EPOCHS (no improvement): 2\n",
      "2019-07-23 02:03:56,836 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:03:56,954 epoch 54 - iter 0/1065 - loss 0.02286599\n",
      "2019-07-23 02:04:05,408 epoch 54 - iter 106/1065 - loss 0.11011581\n",
      "2019-07-23 02:04:13,642 epoch 54 - iter 212/1065 - loss 0.11043950\n",
      "2019-07-23 02:04:21,174 epoch 54 - iter 318/1065 - loss 0.11355084\n",
      "2019-07-23 02:04:28,839 epoch 54 - iter 424/1065 - loss 0.11397451\n",
      "2019-07-23 02:04:36,828 epoch 54 - iter 530/1065 - loss 0.11177534\n",
      "2019-07-23 02:04:44,896 epoch 54 - iter 636/1065 - loss 0.11060721\n",
      "2019-07-23 02:04:52,923 epoch 54 - iter 742/1065 - loss 0.10961688\n",
      "2019-07-23 02:05:00,221 epoch 54 - iter 848/1065 - loss 0.10954631\n",
      "2019-07-23 02:05:08,054 epoch 54 - iter 954/1065 - loss 0.10895961\n",
      "2019-07-23 02:05:15,826 epoch 54 - iter 1060/1065 - loss 0.10815792\n",
      "2019-07-23 02:05:16,130 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:05:16,132 EPOCH 54 done: loss 0.1082 - lr 0.0002\n",
      "2019-07-23 02:05:26,499 DEV : loss 0.41088736057281494 - score 0.5667\n",
      "2019-07-23 02:05:27,124 BAD EPOCHS (no improvement): 3\n",
      "2019-07-23 02:05:27,125 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:05:27,206 epoch 55 - iter 0/1065 - loss 0.13152182\n",
      "2019-07-23 02:05:34,719 epoch 55 - iter 106/1065 - loss 0.11275701\n",
      "2019-07-23 02:05:42,450 epoch 55 - iter 212/1065 - loss 0.10682697\n",
      "2019-07-23 02:05:50,117 epoch 55 - iter 318/1065 - loss 0.10195597\n",
      "2019-07-23 02:05:57,665 epoch 55 - iter 424/1065 - loss 0.10652061\n",
      "2019-07-23 02:06:06,188 epoch 55 - iter 530/1065 - loss 0.10590557\n",
      "2019-07-23 02:06:14,601 epoch 55 - iter 636/1065 - loss 0.10569808\n",
      "2019-07-23 02:06:22,125 epoch 55 - iter 742/1065 - loss 0.10581342\n",
      "2019-07-23 02:06:29,724 epoch 55 - iter 848/1065 - loss 0.10530044\n",
      "2019-07-23 02:06:37,758 epoch 55 - iter 954/1065 - loss 0.10483635\n",
      "2019-07-23 02:06:45,293 epoch 55 - iter 1060/1065 - loss 0.10661326\n",
      "2019-07-23 02:06:45,672 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:06:45,673 EPOCH 55 done: loss 0.1065 - lr 0.0002\n",
      "2019-07-23 02:06:55,558 DEV : loss 0.41098111867904663 - score 0.5667\n",
      "Epoch    54: reducing learning rate of group 0 to 9.7656e-05.\n",
      "2019-07-23 02:06:56,187 BAD EPOCHS (no improvement): 4\n",
      "2019-07-23 02:06:56,188 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:06:56,189 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:06:56,190 learning rate too small - quitting training!\n",
      "2019-07-23 02:06:56,192 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:07:15,114 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-23 02:07:15,115 Testing using best model ...\n",
      "2019-07-23 02:07:15,117 loading file all_rnn/best-model.pt\n",
      "2019-07-23 02:08:09,190 0.5463\t0.5285\t0.5373\n",
      "2019-07-23 02:08:09,192 \n",
      "MICRO_AVG: acc 0.7889 - f1-score 0.882\n",
      "MACRO_AVG: acc 0.6203 - f1-score 0.7348\n",
      "0          tp: 4619 - fp: 347 - fn: 323 - tn: 389 - precision: 0.9301 - recall: 0.9346 - accuracy: 0.8733 - f1-score: 0.9323\n",
      "1          tp: 389 - fp: 323 - fn: 347 - tn: 4619 - precision: 0.5463 - recall: 0.5285 - accuracy: 0.3673 - f1-score: 0.5373\n",
      "2019-07-23 02:08:09,193 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dev_loss_history': [tensor(0.3253, device='cuda:0'),\n",
       "  tensor(0.3144, device='cuda:0'),\n",
       "  tensor(0.3112, device='cuda:0'),\n",
       "  tensor(0.3018, device='cuda:0'),\n",
       "  tensor(0.2965, device='cuda:0'),\n",
       "  tensor(0.2939, device='cuda:0'),\n",
       "  tensor(0.2895, device='cuda:0'),\n",
       "  tensor(0.2865, device='cuda:0'),\n",
       "  tensor(0.4043, device='cuda:0'),\n",
       "  tensor(0.2856, device='cuda:0'),\n",
       "  tensor(0.2817, device='cuda:0'),\n",
       "  tensor(0.2846, device='cuda:0'),\n",
       "  tensor(0.2766, device='cuda:0'),\n",
       "  tensor(0.2778, device='cuda:0'),\n",
       "  tensor(0.2849, device='cuda:0'),\n",
       "  tensor(0.2877, device='cuda:0'),\n",
       "  tensor(0.2932, device='cuda:0'),\n",
       "  tensor(0.2823, device='cuda:0'),\n",
       "  tensor(0.3033, device='cuda:0'),\n",
       "  tensor(0.2857, device='cuda:0'),\n",
       "  tensor(0.2907, device='cuda:0'),\n",
       "  tensor(0.2972, device='cuda:0'),\n",
       "  tensor(0.3234, device='cuda:0'),\n",
       "  tensor(0.3201, device='cuda:0'),\n",
       "  tensor(0.3295, device='cuda:0'),\n",
       "  tensor(0.3379, device='cuda:0'),\n",
       "  tensor(0.3495, device='cuda:0'),\n",
       "  tensor(0.3522, device='cuda:0'),\n",
       "  tensor(0.3593, device='cuda:0'),\n",
       "  tensor(0.3724, device='cuda:0'),\n",
       "  tensor(0.3752, device='cuda:0'),\n",
       "  tensor(0.3796, device='cuda:0'),\n",
       "  tensor(0.3828, device='cuda:0'),\n",
       "  tensor(0.3790, device='cuda:0'),\n",
       "  tensor(0.3859, device='cuda:0'),\n",
       "  tensor(0.3913, device='cuda:0'),\n",
       "  tensor(0.3980, device='cuda:0'),\n",
       "  tensor(0.4000, device='cuda:0'),\n",
       "  tensor(0.3991, device='cuda:0'),\n",
       "  tensor(0.4024, device='cuda:0'),\n",
       "  tensor(0.4009, device='cuda:0'),\n",
       "  tensor(0.4026, device='cuda:0'),\n",
       "  tensor(0.4060, device='cuda:0'),\n",
       "  tensor(0.4063, device='cuda:0'),\n",
       "  tensor(0.4074, device='cuda:0'),\n",
       "  tensor(0.4085, device='cuda:0'),\n",
       "  tensor(0.4109, device='cuda:0'),\n",
       "  tensor(0.4094, device='cuda:0'),\n",
       "  tensor(0.4103, device='cuda:0'),\n",
       "  tensor(0.4116, device='cuda:0'),\n",
       "  tensor(0.4103, device='cuda:0'),\n",
       "  tensor(0.4110, device='cuda:0'),\n",
       "  tensor(0.4114, device='cuda:0'),\n",
       "  tensor(0.4109, device='cuda:0'),\n",
       "  tensor(0.4110, device='cuda:0')],\n",
       " 'dev_score_history': [0.1578,\n",
       "  0.2781,\n",
       "  0.3195,\n",
       "  0.3441,\n",
       "  0.3062,\n",
       "  0.4206,\n",
       "  0.5192,\n",
       "  0.4671,\n",
       "  0.5025,\n",
       "  0.3661,\n",
       "  0.5097,\n",
       "  0.495,\n",
       "  0.5182,\n",
       "  0.5491,\n",
       "  0.5216,\n",
       "  0.4845,\n",
       "  0.5251,\n",
       "  0.5555,\n",
       "  0.5831,\n",
       "  0.5587,\n",
       "  0.5631,\n",
       "  0.548,\n",
       "  0.5738,\n",
       "  0.5452,\n",
       "  0.5693,\n",
       "  0.5546,\n",
       "  0.5506,\n",
       "  0.5691,\n",
       "  0.5661,\n",
       "  0.5482,\n",
       "  0.5622,\n",
       "  0.569,\n",
       "  0.5648,\n",
       "  0.5614,\n",
       "  0.558,\n",
       "  0.5646,\n",
       "  0.5591,\n",
       "  0.5623,\n",
       "  0.565,\n",
       "  0.5653,\n",
       "  0.5654,\n",
       "  0.5685,\n",
       "  0.5624,\n",
       "  0.5658,\n",
       "  0.563,\n",
       "  0.5617,\n",
       "  0.5603,\n",
       "  0.5673,\n",
       "  0.564,\n",
       "  0.5607,\n",
       "  0.5656,\n",
       "  0.5672,\n",
       "  0.5642,\n",
       "  0.5667,\n",
       "  0.5667],\n",
       " 'test_score': 0.5373,\n",
       " 'train_loss_history': [0.36529210136390067,\n",
       "  0.3258053827173833,\n",
       "  0.311163760791642,\n",
       "  0.30221931050431,\n",
       "  0.2926965565503763,\n",
       "  0.2868003012918531,\n",
       "  0.2785550392499552,\n",
       "  0.27474569295633566,\n",
       "  0.267313207518047,\n",
       "  0.2599352598365204,\n",
       "  0.25433220162707876,\n",
       "  0.2356900437302153,\n",
       "  0.22992320043240355,\n",
       "  0.22371718807609428,\n",
       "  0.2205176121856965,\n",
       "  0.21489059597148863,\n",
       "  0.2092960653240692,\n",
       "  0.206167691228955,\n",
       "  0.2000414900254336,\n",
       "  0.19807584010974064,\n",
       "  0.18843862358218347,\n",
       "  0.18495951497778645,\n",
       "  0.18169445031860346,\n",
       "  0.16197759655124985,\n",
       "  0.15819135436289747,\n",
       "  0.1522785513211444,\n",
       "  0.15274776141181098,\n",
       "  0.13636599435108088,\n",
       "  0.13586872189345076,\n",
       "  0.13053066919911915,\n",
       "  0.12894114055299144,\n",
       "  0.12564992244560882,\n",
       "  0.12357388294166383,\n",
       "  0.12238622016225342,\n",
       "  0.11911453186346331,\n",
       "  0.11584780601600946,\n",
       "  0.11378236209772562,\n",
       "  0.11430380829115848,\n",
       "  0.11599701062421984,\n",
       "  0.11064182289749244,\n",
       "  0.1129503885992396,\n",
       "  0.10988434529360472,\n",
       "  0.10868708533125584,\n",
       "  0.11020917727091642,\n",
       "  0.10981108343902049,\n",
       "  0.10934474788116458,\n",
       "  0.10569835324015275,\n",
       "  0.11006489263086672,\n",
       "  0.1089190369638059,\n",
       "  0.10796661755690971,\n",
       "  0.10877308812552075,\n",
       "  0.10908221295322051,\n",
       "  0.10844485845434274,\n",
       "  0.10815799798151837,\n",
       "  0.10654799485395491]}"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.embeddings import DocumentRNNEmbeddings\n",
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'),\n",
    "                                                       test_file='flair_test.csv',\n",
    "                                                       dev_file='flair_dev.csv',\n",
    "                                                       train_file='flair_train.csv')\n",
    "word_embeddings = [#BertEmbeddings(),\n",
    "                   FlairEmbeddings('news-forward-fast'),\n",
    "                   FlairEmbeddings('news-backward-fast'),\n",
    "                   WordEmbeddings(\"news\"),\n",
    "                   WordEmbeddings(\"glove\")] # bert and fasttext\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings)\n",
    "\n",
    "print(\"Create Classifier *************************\")\n",
    "classifier = TextClassifier(document_embeddings,\n",
    "                            label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "print(\"Create Trainer *******************************\")\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "print(\"Begin Training *******************************\")\n",
    "trainer.train(max_epochs=100, base_path = \"all_rnn\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "flair.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
