{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifier with BERT-Policy2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjapago/AnalyzeAccountability/blob/master/Classifier_with_BERT_Policy2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ",
        "colab_type": "text"
      },
      "source": [
        "# Binary Classifier BERT on TF Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiYrZKaHwV81",
        "colab_type": "text"
      },
      "source": [
        "Bidirectional Encoder Representations from Transformers(BERT) is a neural network architecture designed by Google researchers is a state-of-the-art approach or NLP tasks, including text classification, translation, summarization, and question answering.\n",
        "\n",
        "BERT has been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, and in an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. \n",
        "\n",
        "[Finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
        "\n",
        "Here, we'll train a a classifier to detect accountability in news articles using BERT in Tensorflow with tf hub. Code was adapted from [this colab notebook](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5wfXDx5SPH",
        "colab_type": "text"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab_type": "code",
        "outputId": "820ebae4-d5dc-40eb-e8e8-4be038df481a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbGEfwgdEtw",
        "colab_type": "code",
        "outputId": "48ec87ee-3c63-4d5f-efeb-0e1e4f4bbe95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0825 21:14:02.093044 140561002514304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVB3eOcjxxm1",
        "colab_type": "text"
      },
      "source": [
        "Below, we'll set an output directory location to store our model output and checkpoints. We are running this code in Google's hosted Colab, so the directory won't persist after the Colab session ends.\n",
        "\n",
        "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n0QQStRcJut",
        "colab_type": "code",
        "outputId": "8eac2e6f-de65-457a-cfee-958ea96bce2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "OUTPUT_DIR = 'OUTPUT_DIR_NAME'#@param {type:\"string\"}\n",
        "DO_DELETE = True #@param {type:\"boolean\"}\n",
        "USE_BUCKET = False #@param {type:\"boolean\"}\n",
        "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
        "\n",
        "if USE_BUCKET:\n",
        "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "  except:\n",
        "    # Doesn't matter if the directory didn't exist\n",
        "    pass\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: OUTPUT_DIR_NAME *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_w8SRqN0fr",
        "colab_type": "text"
      },
      "source": [
        "Load the dataset of news excerpts annotated with the accountability label. The code below loads the data from xlsx files, formats it as a pandas data frame, and splits it into test and training sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it",
        "colab_type": "code",
        "outputId": "46a2d7fb-ad4d-4e16-b647-79dde575fa17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNAOq_kMpIq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filenames = [filename for filename in os.listdir() if 'xlsx' in filename]\n",
        "\n",
        "DATA_COLUMN = 'excerpt'\n",
        "LABEL_COLUMN = 'label'\n",
        "label_list = [0, 1]\n",
        "max_sent = 5\n",
        "label = 'policy'\n",
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZBNnY_P3qWx",
        "colab_type": "code",
        "outputId": "4401ff10-4d60-4216-e23a-310dbfe5140d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "train_list = []\n",
        "test_list = []\n",
        "for file_name in filenames:\n",
        "  data = pd.read_excel(file_name, sheet_name='Dedoose Excerpts Export')\n",
        "  data = data.dropna(axis=0)\n",
        "\n",
        "  # get relevant columns:\n",
        "  label_cols = [l for l in data.columns if label in l.lower()]\n",
        "  excerpt_col = [l for l in data.columns if DATA_COLUMN in l.lower()][0]\n",
        "  data_subcols = data.loc[:, label_cols+[excerpt_col]]\n",
        "\n",
        "  for colname in label_cols:\n",
        "    data_subcols = data_subcols.astype({colname: int})\n",
        "    id0 = [val not in [0, 1] for val in data_subcols.loc[:, colname]]\n",
        "    data_subcols.loc[id0, colname] = 0\n",
        "\n",
        "  #print(data_subcols.shape)\n",
        "  print(label_cols)\n",
        "\n",
        "  # filter out rows that do not have any policy subtype label\n",
        "  label_ids = data_subcols.loc[:, label_cols].sum(axis=1) > 1 \n",
        "  df_label = data_subcols.loc[label_ids,:]\n",
        "  #print(df_label.shape)\n",
        "\n",
        "  # filter out long excerpts\n",
        "\n",
        "  short_ex_ids = [len(sent_tokenize(sent))>max_sent for sent in df_label.loc[:, excerpt_col]]\n",
        "  df_label_short = df_label.loc[short_ex_ids, :]\n",
        "  print(df_label_short.shape)\n",
        "\n",
        "  # split into train and test dfs\n",
        "  train, test = train_test_split(df_label_short, test_size=0.25, random_state=42)\n",
        "  #print(train.shape)\n",
        "  #print(test.shape)\n",
        "  train_list.append(train)\n",
        "  test_list.append(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Code: Policy Applied', 'Code: Policy\\\\Advocacy by others Applied', 'Code: Policy\\\\Advocacy by victims families Applied', 'Code: Policy\\\\Guns Applied', 'Code: Policy\\\\Immigration Applied', 'Code: Policy\\\\Information Sharing Applied', 'Code: Policy\\\\Mental Health Applied', 'Code: Policy\\\\Other Applied', 'Code: Policy\\\\Practice Applied']\n",
            "(56, 10)\n",
            "['Code: Policy Applied', 'Code: Policy\\\\Advocacy by others Applied', 'Code: Policy\\\\Advocacy by victims families Applied', 'Code: Policy\\\\Guns Applied', 'Code: Policy\\\\Immigration Applied', 'Code: Policy\\\\Information Sharing Applied', 'Code: Policy\\\\Mental Health Applied', 'Code: Policy\\\\Other Applied', 'Code: Policy\\\\Practice Applied']\n",
            "(59, 10)\n",
            "['Code: Policy Applied', 'Code: Policy\\\\Advocacy by others Applied', 'Code: Policy\\\\Advocacy by victims families Applied', 'Code: Policy\\\\Guns Applied', 'Code: Policy\\\\Immigration Applied', 'Code: Policy\\\\Information Sharing Applied', 'Code: Policy\\\\Mental Health Applied', 'Code: Policy\\\\Other Applied', 'Code: Policy\\\\Practice Applied']\n",
            "(50, 10)\n",
            "['POLICY', 'POLICY_OtherAdv', 'POLICY_VictimAdv', 'POLICY_Guns', 'POLICY_InfoSharing', 'POLICY_MentalHealth', 'POLICY_Other', 'POLICY_Practice']\n",
            "(560, 9)\n",
            "['POLICY', 'POLICY_Guns', 'POLICY_InfoSharing', 'POLICY_MentalHealth', 'POLICY_Other', 'POLICY_VictimAdv', 'POLICY_OtherAdv', 'POLICY_Practice']\n",
            "(272, 9)\n",
            "['POLICY', 'POLICY_Guns', 'POLICY_InfoSharing', 'POLICY_MentalHealth', 'POLICY_Other', 'POLICY_VictimAdv', 'POLICY_OtherAdv', 'POLICY_Practice']\n",
            "(16, 9)\n",
            "['Code: Policy Applied', 'Code: Policy\\\\Advocacy by others Applied', 'Code: Policy\\\\Advocacy by victims families Applied', 'Code: Policy\\\\Guns Applied', 'Code: Policy\\\\Immigration Applied', 'Code: Policy\\\\Information Sharing Applied', 'Code: Policy\\\\Mental Health Applied', 'Code: Policy\\\\Other Applied', 'Code: Policy\\\\Practice Applied']\n",
            "(52, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSUa6438Bf0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transform columns for all data frames to be the same\n",
        "col_dict = {\n",
        "    'OtherAdv': ['POLICY_OtherAdv', 'Code: Policy\\Advocacy by others Applied'],\n",
        "    'VictimAdv': ['POLICY_VictimAdv', 'Code: Policy\\Advocacy by victims families Applied'],\n",
        "    'Guns': [ 'POLICY_Guns', 'Code: Policy\\Guns Applied'],\n",
        "    'InfoSharing': ['POLICY_InfoSharing', 'Code: Policy\\Information Sharing Applied'],\n",
        "    'MentalHealth': ['POLICY_MentalHealth', 'Code: Policy\\Mental Health Applied'],\n",
        "    'Other': ['POLICY_Other', 'Code: Policy\\Other Applied'],\n",
        "    'Practice': ['POLICY_Practice', 'Code: Policy\\Practice Applied'],\n",
        "    'Immigration': ['Code: Policy\\Immigration Applied']\n",
        "}\n",
        "\n",
        "LABEL_COLUMNS = list(col_dict.keys())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5f57Z2rMHoX",
        "colab_type": "code",
        "outputId": "846d456d-c94b-4f87-8a60-aa46536b3089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "def merge_dfs(df_list):\n",
        "  merged_df = pd.DataFrame(columns = list(col_dict.keys())+[DATA_COLUMN])\n",
        "\n",
        "  for df in df_list:\n",
        "    df_renamed = pd.DataFrame(columns = col_dict.keys(), index = df.index)\n",
        "    print(df.shape)\n",
        "\n",
        "    #renamed ex col\n",
        "    df_renamed[DATA_COLUMN] = df.loc[:,[l for l in df.columns if DATA_COLUMN in l.lower()][0]]\n",
        "\n",
        "    # make each dict in the list to have the columns in col_dict\n",
        "    for new_colname in col_dict.keys():\n",
        "      #check if df has subtype:\n",
        "      col = [colname for colname in df.columns if colname in col_dict[new_colname]]\n",
        "      if len(col) ==0:\n",
        "        df_renamed[new_colname] = 0\n",
        "      else:\n",
        "        df_renamed[new_colname] = df.loc[:, col]\n",
        "    merged_df = merged_df.append(df_renamed, ignore_index=True)\n",
        "  print(merged_df.shape)\n",
        "  return merged_df\n",
        "\n",
        "test_merged = merge_dfs(test_list)\n",
        "train_merged = merge_dfs(train_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14, 10)\n",
            "(15, 10)\n",
            "(13, 10)\n",
            "(140, 9)\n",
            "(68, 9)\n",
            "(4, 9)\n",
            "(13, 10)\n",
            "(267, 9)\n",
            "(42, 10)\n",
            "(44, 10)\n",
            "(37, 10)\n",
            "(420, 9)\n",
            "(204, 9)\n",
            "(12, 9)\n",
            "(39, 10)\n",
            "(798, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdO02XhaMw4O",
        "colab_type": "code",
        "outputId": "514fcfbd-bbb1-40dc-ed29-663fd5b93674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_merged.loc[:, col_dict.keys()].head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OtherAdv</th>\n",
              "      <th>VictimAdv</th>\n",
              "      <th>Guns</th>\n",
              "      <th>InfoSharing</th>\n",
              "      <th>MentalHealth</th>\n",
              "      <th>Other</th>\n",
              "      <th>Practice</th>\n",
              "      <th>Immigration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  OtherAdv VictimAdv Guns InfoSharing MentalHealth Other Practice Immigration\n",
              "0        0         0    0           1            0     0        0           0\n",
              "1        0         0    0           1            0     0        0           0\n",
              "2        1         0    1           0            0     0        0           0\n",
              "3        1         0    0           0            0     0        0           1\n",
              "4        1         0    0           0            0     1        0           0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Okn5alNLar",
        "colab_type": "code",
        "outputId": "f523fe43-70aa-49a4-8e88-a21c464fa85b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_merged.OtherAdv.unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKs-vuaC6BiU",
        "colab_type": "code",
        "outputId": "7bac5e12-6243-449e-e834-7cdf2290998d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "LABEL_COLUMNS"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['OtherAdv',\n",
              " 'VictimAdv',\n",
              " 'Guns',\n",
              " 'InfoSharing',\n",
              " 'MentalHealth',\n",
              " 'Other',\n",
              " 'Practice',\n",
              " 'Immigration']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ3leLRiUvXb",
        "colab_type": "text"
      },
      "source": [
        "View the loaded data, and inspect the first few entries in the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V399W0rqNJ-Z",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJSe0QHNG7U",
        "colab_type": "code",
        "outputId": "962483c6-a821-40fc-c095-713bee24168f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0825 21:14:17.072914 140561002514304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBo6RCtQmwx",
        "colab_type": "code",
        "outputId": "c2d652ef-8cb3-4e3e-a9c6-963677fa3e39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr",
        "colab_type": "text"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our accountability detection task. This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # beta for L2 regularizer\n",
        "  beta = 0.1\n",
        "  \n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for accountability data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    regularizer = tf.nn.l2_loss(output_weights)\n",
        "    #loss = tf.reduce_mean(per_example_loss + beta*regularizer)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpE0ZIDOCQzE",
        "colab_type": "text"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        #f1_score = tf.contrib.metrics.f1_score(\n",
        "        #    label_ids,\n",
        "        #    predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            #\"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100\n",
        "\n",
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyULe-HYdhUJ",
        "colab_type": "code",
        "outputId": "a9869f35-871d-45a4-bd0e-34186352033c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Run for each label\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "LABEL_COLUMNS.reverse()\n",
        "\n",
        "for LABEL_COLUMN in LABEL_COLUMNS:\n",
        "    print(\" \")\n",
        "    print(\" \")\n",
        "    print(\"_______________________\"+str(LABEL_COLUMN)+\"_______________________\")\n",
        "    print(\"train num label=1: \"+str(sum(train_merged[LABEL_COLUMN])))\n",
        "    print(\"train percent = 1: \"+str(sum(train_merged[LABEL_COLUMN])/len(train_merged[LABEL_COLUMN])))\n",
        "    print(\"train size: \"+str(len(train_merged[LABEL_COLUMN])))\n",
        "    \n",
        "    print(\"test num label=1: \"+str(sum(test_merged[LABEL_COLUMN])))\n",
        "    print(\"test percent = 1: \"+str(sum(test_merged[LABEL_COLUMN])/len(test_merged[LABEL_COLUMN])))\n",
        "    print(\"test size: \"+str(len(test_merged[LABEL_COLUMN])))\n",
        "          \n",
        "    # Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "    train_InputExamples = train_merged.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                       text_a = x[DATA_COLUMN], \n",
        "                                                                       text_b = None, \n",
        "                                                                       label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "    test_InputExamples = test_merged.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                       text_a = x[DATA_COLUMN], \n",
        "                                                                       text_b = None, \n",
        "                                                                       label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "    # We'll set sequences to be at most 128 tokens long.\n",
        "    MAX_SEQ_LENGTH = 128\n",
        "    # Convert our train and test features to InputFeatures that BERT understands.\n",
        "    train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, \n",
        "                                                                      label_list, \n",
        "                                                                      MAX_SEQ_LENGTH,\n",
        "                                                                      tokenizer)\n",
        "    test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples,\n",
        "                                                                     label_list,\n",
        "                                                                     MAX_SEQ_LENGTH,\n",
        "                                                                     tokenizer)\n",
        "\n",
        "    # Compute # train and warmup steps from batch size\n",
        "    num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "    model_fn = model_fn_builder(\n",
        "      num_labels=len(label_list),\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "    # Create an input function for training. drop_remainder = True for using TPUs.\n",
        "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "        features=train_features,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "\n",
        "    print(f'Beginning Training!')\n",
        "    current_time = datetime.now()\n",
        "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "    print(\"Training took time \", datetime.now() - current_time)\n",
        "\n",
        "    test_input_fn = run_classifier.input_fn_builder(\n",
        "        features=test_features,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)\n",
        "\n",
        "    print(\"test results:\")\n",
        "    test_result = estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "    print(test_result)\n",
        "    \n",
        "\n",
        "    test_train_input_fn = run_classifier.input_fn_builder(\n",
        "        features=train_features,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)\n",
        "    print(\"train results:\")\n",
        "    train_result = estimator.evaluate(input_fn=test_train_input_fn, steps=None)\n",
        "    print(train_result)\n",
        "    \n",
        "    # delete output checkpoints to reset training for every label\n",
        "    if DO_DELETE:\n",
        "      try:\n",
        "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "      except:\n",
        "        # Doesn't matter if the directory didn't exist\n",
        "        print(OUTPUT_DIR+\" didn't exist, could not delete\")\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            " \n",
            "_______________________OtherAdv_______________________\n",
            "train num label=1: 324\n",
            "train percent = 1: 0.40601503759398494\n",
            "train size: 798\n",
            "test num label=1: 101\n",
            "test percent = 1: 0.3782771535580524\n",
            "test size: 267\n",
            "Beginning Training!\n",
            "Training took time  0:02:04.500236\n",
            "test results:\n",
            "{'auc': 0.6447274, 'eval_accuracy': 0.6329588, 'false_negatives': 31.0, 'false_positives': 67.0, 'loss': 0.64986295, 'precision': 0.5109489, 'recall': 0.6930693, 'true_negatives': 99.0, 'true_positives': 70.0, 'global_step': 74}\n",
            "train results:\n",
            "{'auc': 0.81833106, 'eval_accuracy': 0.8120301, 'false_negatives': 48.0, 'false_positives': 102.0, 'loss': 0.460791, 'precision': 0.73015875, 'recall': 0.8518519, 'true_negatives': 372.0, 'true_positives': 276.0, 'global_step': 74}\n",
            " \n",
            " \n",
            "_______________________VictimAdv_______________________\n",
            "train num label=1: 40\n",
            "train percent = 1: 0.05012531328320802\n",
            "train size: 798\n",
            "test num label=1: 11\n",
            "test percent = 1: 0.04119850187265917\n",
            "test size: 267\n",
            "Beginning Training!\n",
            "Training took time  0:02:04.486238\n",
            "test results:\n",
            "{'auc': 0.50000006, 'eval_accuracy': 0.9588015, 'false_negatives': 11.0, 'false_positives': 0.0, 'loss': 0.16003779, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 256.0, 'true_positives': 0.0, 'global_step': 74}\n",
            "train results:\n",
            "{'auc': 0.5, 'eval_accuracy': 0.9498747, 'false_negatives': 40.0, 'false_positives': 0.0, 'loss': 0.11725294, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 758.0, 'true_positives': 0.0, 'global_step': 74}\n",
            " \n",
            " \n",
            "_______________________Guns_______________________\n",
            "train num label=1: 558\n",
            "train percent = 1: 0.6992481203007519\n",
            "train size: 798\n",
            "test num label=1: 187\n",
            "test percent = 1: 0.700374531835206\n",
            "test size: 267\n",
            "Beginning Training!\n",
            "Training took time  0:02:04.014939\n",
            "test results:\n",
            "{'auc': 0.92947865, 'eval_accuracy': 0.9513109, 'false_negatives': 3.0, 'false_positives': 10.0, 'loss': 0.16256286, 'precision': 0.9484536, 'recall': 0.98395723, 'true_negatives': 70.0, 'true_positives': 184.0, 'global_step': 74}\n",
            "train results:\n",
            "{'auc': 0.9660394, 'eval_accuracy': 0.97243106, 'false_negatives': 10.0, 'false_positives': 12.0, 'loss': 0.10079224, 'precision': 0.9785714, 'recall': 0.98207885, 'true_negatives': 228.0, 'true_positives': 548.0, 'global_step': 74}\n",
            " \n",
            " \n",
            "_______________________InfoSharing_______________________\n",
            "train num label=1: 41\n",
            "train percent = 1: 0.05137844611528822\n",
            "train size: 798\n",
            "test num label=1: 10\n",
            "test percent = 1: 0.03745318352059925\n",
            "test size: 267\n",
            "Beginning Training!\n",
            "Training took time  0:02:04.122161\n",
            "test results:\n",
            "{'auc': 0.9480545, 'eval_accuracy': 0.99250937, 'false_negatives': 1.0, 'false_positives': 1.0, 'loss': 0.0448863, 'precision': 0.9, 'recall': 0.9, 'true_negatives': 256.0, 'true_positives': 9.0, 'global_step': 74}\n",
            "train results:\n",
            "{'auc': 0.9268293, 'eval_accuracy': 0.99248123, 'false_negatives': 6.0, 'false_positives': 0.0, 'loss': 0.043643937, 'precision': 1.0, 'recall': 0.85365856, 'true_negatives': 757.0, 'true_positives': 35.0, 'global_step': 74}\n",
            " \n",
            " \n",
            "_______________________MentalHealth_______________________\n",
            "train num label=1: 174\n",
            "train percent = 1: 0.21804511278195488\n",
            "train size: 798\n",
            "test num label=1: 59\n",
            "test percent = 1: 0.2209737827715356\n",
            "test size: 267\n",
            "Beginning Training!\n",
            "Training took time  0:02:03.621187\n",
            "test results:\n",
            "{'auc': 0.86819583, 'eval_accuracy': 0.917603, 'false_negatives': 13.0, 'false_positives': 9.0, 'loss': 0.2525643, 'precision': 0.8363636, 'recall': 0.779661, 'true_negatives': 199.0, 'true_positives': 46.0, 'global_step': 74}\n",
            "train results:\n",
            "{'auc': 0.92028624, 'eval_accuracy': 0.9498747, 'false_negatives': 23.0, 'false_positives': 17.0, 'loss': 0.1480912, 'precision': 0.89880955, 'recall': 0.8678161, 'true_negatives': 607.0, 'true_positives': 151.0, 'global_step': 74}\n",
            " \n",
            " \n",
            "_______________________Other_______________________\n",
            "train num label=1: 78\n",
            "train percent = 1: 0.09774436090225563\n",
            "train size: 798\n",
            "test num label=1: 30\n",
            "test percent = 1: 0.11235955056179775\n",
            "test size: 267\n",
            "Beginning Training!\n",
            "Training took time  0:02:03.984338\n",
            "test results:\n",
            "{'auc': 0.72911394, 'eval_accuracy': 0.9325843, 'false_negatives': 16.0, 'false_positives': 2.0, 'loss': 0.18177693, 'precision': 0.875, 'recall': 0.46666667, 'true_negatives': 235.0, 'true_positives': 14.0, 'global_step': 74}\n",
            "train results:\n",
            "{'auc': 0.83904916, 'eval_accuracy': 0.96741855, 'false_negatives': 25.0, 'false_positives': 1.0, 'loss': 0.104762405, 'precision': 0.9814815, 'recall': 0.67948717, 'true_negatives': 719.0, 'true_positives': 53.0, 'global_step': 74}\n",
            " \n",
            " \n",
            "_______________________Practice_______________________\n",
            "train num label=1: 47\n",
            "train percent = 1: 0.05889724310776942\n",
            "train size: 798\n",
            "test num label=1: 20\n",
            "test percent = 1: 0.0749063670411985\n",
            "test size: 267\n",
            "Beginning Training!\n",
            "Training took time  0:02:04.041493\n",
            "test results:\n",
            "{'auc': 0.675, 'eval_accuracy': 0.9513109, 'false_negatives': 13.0, 'false_positives': 0.0, 'loss': 0.23085767, 'precision': 1.0, 'recall': 0.35, 'true_negatives': 247.0, 'true_positives': 7.0, 'global_step': 74}\n",
            "train results:\n",
            "{'auc': 0.9674193, 'eval_accuracy': 0.9949875, 'false_negatives': 3.0, 'false_positives': 1.0, 'loss': 0.02618777, 'precision': 0.9777778, 'recall': 0.9361702, 'true_negatives': 750.0, 'true_positives': 44.0, 'global_step': 74}\n",
            " \n",
            " \n",
            "_______________________Immigration_______________________\n",
            "train num label=1: 10\n",
            "train percent = 1: 0.012531328320802004\n",
            "train size: 798\n",
            "test num label=1: 2\n",
            "test percent = 1: 0.00749063670411985\n",
            "test size: 267\n",
            "Beginning Training!\n",
            "Training took time  0:02:03.435013\n",
            "test results:\n",
            "{'auc': 0.50000024, 'eval_accuracy': 0.99250937, 'false_negatives': 2.0, 'false_positives': 0.0, 'loss': 0.042053975, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 265.0, 'true_positives': 0.0, 'global_step': 74}\n",
            "train results:\n",
            "{'auc': 0.50000006, 'eval_accuracy': 0.98746866, 'false_negatives': 10.0, 'false_positives': 0.0, 'loss': 0.071802266, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 788.0, 'true_positives': 0.0, 'global_step': 74}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OEpuPLnth_x",
        "colab_type": "text"
      },
      "source": [
        "## Results From Each Label Individually: run 1\n",
        " (WITH deleting output dir that saves check points, to ensure re-training for each label)\n",
        " \n",
        " _______________________Immigration_______________________\n",
        "train num label=1: 10\n",
        "train percent = 1: 0.012531328320802004\n",
        "test num label=1: 2\n",
        "test percent = 1: 0.00749063670411985\n",
        "Beginning Training!\n",
        "Training took time  0:00:00.007491\n",
        "test results:\n",
        "{'auc': 0.49433985, 'eval_accuracy': 0.9812734, 'false_negatives': 2.0, 'false_positives': 3.0, 'loss': 0.5698759, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 262.0, 'true_positives': 0.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.49175134, 'eval_accuracy': 0.97117794, 'false_negatives': 10.0, 'false_positives': 13.0, 'loss': 0.5714542, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 775.0, 'true_positives': 0.0, 'global_step': 74}\n",
        "_______________________Practice_______________________\n",
        "train num label=1: 47\n",
        "train percent = 1: 0.05889724310776942\n",
        "test num label=1: 20\n",
        "test percent = 1: 0.0749063670411985\n",
        "Beginning Training!\n",
        "Training took time  0:01:57.388289\n",
        "test results:\n",
        "{'auc': 0.57297575, 'eval_accuracy': 0.9325843, 'false_negatives': 17.0, 'false_positives': 1.0, 'loss': 0.22094125, 'precision': 0.75, 'recall': 0.15, 'true_negatives': 246.0, 'true_positives': 3.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.60371983, 'eval_accuracy': 0.9486216, 'false_negatives': 37.0, 'false_positives': 4.0, 'loss': 0.13745503, 'precision': 0.71428573, 'recall': 0.21276596, 'true_negatives': 747.0, 'true_positives': 10.0, 'global_step': 74}\n",
        "_______________________Other_______________________\n",
        "train num label=1: 78\n",
        "train percent = 1: 0.09774436090225563\n",
        "test num label=1: 30\n",
        "test percent = 1: 0.11235955056179775\n",
        "Beginning Training!\n",
        "Training took time  0:01:59.498562\n",
        "test results:\n",
        "{'auc': 0.75, 'eval_accuracy': 0.94382024, 'false_negatives': 15.0, 'false_positives': 0.0, 'loss': 0.19897617, 'precision': 1.0, 'recall': 0.5, 'true_negatives': 237.0, 'true_positives': 15.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.935203, 'eval_accuracy': 0.98621553, 'false_negatives': 10.0, 'false_positives': 1.0, 'loss': 0.06465396, 'precision': 0.98550725, 'recall': 0.8717949, 'true_negatives': 719.0, 'true_positives': 68.0, 'global_step': 74}\n",
        "_______________________MentalHealth_______________________\n",
        "train num label=1: 174\n",
        "train percent = 1: 0.21804511278195488\n",
        "test num label=1: 59\n",
        "test percent = 1: 0.2209737827715356\n",
        "Beginning Training!\n",
        "Training took time  0:01:59.424367\n",
        "test results:\n",
        "{'auc': 0.8525098, 'eval_accuracy': 0.90262175, 'false_negatives': 14.0, 'false_positives': 12.0, 'loss': 0.2861044, 'precision': 0.7894737, 'recall': 0.7627119, 'true_negatives': 196.0, 'true_positives': 45.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.96706456, 'eval_accuracy': 0.97117794, 'false_negatives': 7.0, 'false_positives': 16.0, 'loss': 0.08802146, 'precision': 0.91256833, 'recall': 0.95977014, 'true_negatives': 608.0, 'true_positives': 167.0, 'global_step': 74}\n",
        "_______________________InfoSharing_______________________\n",
        "train num label=1: 41\n",
        "train percent = 1: 0.05137844611528822\n",
        "test num label=1: 10\n",
        "test percent = 1: 0.03745318352059925\n",
        "Beginning Training!\n",
        "Training took time  0:01:59.466103\n",
        "test results:\n",
        "{'auc': 0.9480545, 'eval_accuracy': 0.99250937, 'false_negatives': 1.0, 'false_positives': 1.0, 'loss': 0.04676785, 'precision': 0.9, 'recall': 0.9, 'true_negatives': 256.0, 'true_positives': 9.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.9627542, 'eval_accuracy': 0.9949875, 'false_negatives': 3.0, 'false_positives': 1.0, 'loss': 0.025847204, 'precision': 0.974359, 'recall': 0.9268293, 'true_negatives': 756.0, 'true_positives': 38.0, 'global_step': 74}\n",
        "_______________________Guns_______________________\n",
        "train num label=1: 558\n",
        "train percent = 1: 0.6992481203007519\n",
        "test num label=1: 187\n",
        "test percent = 1: 0.700374531835206\n",
        "Beginning Training!\n",
        "Training took time  0:01:59.264699\n",
        "test results:\n",
        "{'auc': 0.9420121, 'eval_accuracy': 0.94382024, 'false_negatives': 10.0, 'false_positives': 5.0, 'loss': 0.15438452, 'precision': 0.97252744, 'recall': 0.9465241, 'true_negatives': 75.0, 'true_positives': 177.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.96247756, 'eval_accuracy': 0.97243106, 'false_negatives': 7.0, 'false_positives': 15.0, 'loss': 0.10229945, 'precision': 0.9734982, 'recall': 0.9874552, 'true_negatives': 225.0, 'true_positives': 551.0, 'global_step': 74}\n",
        "_______________________VictimAdv_______________________\n",
        "train num label=1: 40\n",
        "train percent = 1: 0.05012531328320802\n",
        "test num label=1: 11\n",
        "test percent = 1: 0.04119850187265917\n",
        "Beginning Training!\n",
        "Training took time  0:02:01.486868\n",
        "test results:\n",
        "{'auc': 0.50000006, 'eval_accuracy': 0.9588015, 'false_negatives': 11.0, 'false_positives': 0.0, 'loss': 0.15847327, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 256.0, 'true_positives': 0.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.5, 'eval_accuracy': 0.9498747, 'false_negatives': 40.0, 'false_positives': 0.0, 'loss': 0.18727362, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 758.0, 'true_positives': 0.0, 'global_step': 74}\n",
        "_______________________OtherAdv_______________________\n",
        "train num label=1: 324\n",
        "train percent = 1: 0.40601503759398494\n",
        "test num label=1: 101\n",
        "test percent = 1: 0.3782771535580524\n",
        "Beginning Training!\n",
        "Training took time  0:02:03.128609\n",
        "test results:\n",
        "{'auc': 0.5, 'eval_accuracy': 0.6217228, 'false_negatives': 101.0, 'false_positives': 0.0, 'loss': 0.65677285, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 166.0, 'true_positives': 0.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.4999219, 'eval_accuracy': 0.59273183, 'false_negatives': 322.0, 'false_positives': 3.0, 'loss': 0.68304825, 'precision': 0.4, 'recall': 0.0061728396, 'true_negatives': 471.0, 'true_positives': 2.0, 'global_step': 74}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WVlgljRxJwM",
        "colab_type": "text"
      },
      "source": [
        "## Results From Each Label Individually: run 2\n",
        " (WITH deleting output dir that saves check points, to ensure re-training for each label)\n",
        " \n",
        " _______________________OtherAdv_______________________\n",
        "train num label=1: 324\n",
        "train percent = 1: 0.40601503759398494\n",
        "train size: 798\n",
        "test num label=1: 101\n",
        "test percent = 1: 0.3782771535580524\n",
        "test size: 267\n",
        "Beginning Training!\n",
        "Training took time  0:02:04.500236\n",
        "test results:\n",
        "{'auc': 0.6447274, 'eval_accuracy': 0.6329588, 'false_negatives': 31.0, 'false_positives': 67.0, 'loss': 0.64986295, 'precision': 0.5109489, 'recall': 0.6930693, 'true_negatives': 99.0, 'true_positives': 70.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.81833106, 'eval_accuracy': 0.8120301, 'false_negatives': 48.0, 'false_positives': 102.0, 'loss': 0.460791, 'precision': 0.73015875, 'recall': 0.8518519, 'true_negatives': 372.0, 'true_positives': 276.0, 'global_step': 74}\n",
        " \n",
        " \n",
        "_______________________VictimAdv_______________________\n",
        "train num label=1: 40\n",
        "train percent = 1: 0.05012531328320802\n",
        "train size: 798\n",
        "test num label=1: 11\n",
        "test percent = 1: 0.04119850187265917\n",
        "test size: 267\n",
        "Beginning Training!\n",
        "Training took time  0:02:04.486238\n",
        "test results:\n",
        "{'auc': 0.50000006, 'eval_accuracy': 0.9588015, 'false_negatives': 11.0, 'false_positives': 0.0, 'loss': 0.16003779, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 256.0, 'true_positives': 0.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.5, 'eval_accuracy': 0.9498747, 'false_negatives': 40.0, 'false_positives': 0.0, 'loss': 0.11725294, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 758.0, 'true_positives': 0.0, 'global_step': 74}\n",
        " \n",
        " \n",
        "_______________________Guns_______________________\n",
        "train num label=1: 558\n",
        "train percent = 1: 0.6992481203007519\n",
        "train size: 798\n",
        "test num label=1: 187\n",
        "test percent = 1: 0.700374531835206\n",
        "test size: 267\n",
        "Beginning Training!\n",
        "Training took time  0:02:04.014939\n",
        "test results:\n",
        "{'auc': 0.92947865, 'eval_accuracy': 0.9513109, 'false_negatives': 3.0, 'false_positives': 10.0, 'loss': 0.16256286, 'precision': 0.9484536, 'recall': 0.98395723, 'true_negatives': 70.0, 'true_positives': 184.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.9660394, 'eval_accuracy': 0.97243106, 'false_negatives': 10.0, 'false_positives': 12.0, 'loss': 0.10079224, 'precision': 0.9785714, 'recall': 0.98207885, 'true_negatives': 228.0, 'true_positives': 548.0, 'global_step': 74}\n",
        " \n",
        " \n",
        "_______________________InfoSharing_______________________\n",
        "train num label=1: 41\n",
        "train percent = 1: 0.05137844611528822\n",
        "train size: 798\n",
        "test num label=1: 10\n",
        "test percent = 1: 0.03745318352059925\n",
        "test size: 267\n",
        "Beginning Training!\n",
        "Training took time  0:02:04.122161\n",
        "test results:\n",
        "{'auc': 0.9480545, 'eval_accuracy': 0.99250937, 'false_negatives': 1.0, 'false_positives': 1.0, 'loss': 0.0448863, 'precision': 0.9, 'recall': 0.9, 'true_negatives': 256.0, 'true_positives': 9.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.9268293, 'eval_accuracy': 0.99248123, 'false_negatives': 6.0, 'false_positives': 0.0, 'loss': 0.043643937, 'precision': 1.0, 'recall': 0.85365856, 'true_negatives': 757.0, 'true_positives': 35.0, 'global_step': 74}\n",
        " \n",
        " \n",
        "_______________________MentalHealth_______________________\n",
        "train num label=1: 174\n",
        "train percent = 1: 0.21804511278195488\n",
        "train size: 798\n",
        "test num label=1: 59\n",
        "test percent = 1: 0.2209737827715356\n",
        "test size: 267\n",
        "Beginning Training!\n",
        "Training took time  0:02:03.621187\n",
        "test results:\n",
        "{'auc': 0.86819583, 'eval_accuracy': 0.917603, 'false_negatives': 13.0, 'false_positives': 9.0, 'loss': 0.2525643, 'precision': 0.8363636, 'recall': 0.779661, 'true_negatives': 199.0, 'true_positives': 46.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.92028624, 'eval_accuracy': 0.9498747, 'false_negatives': 23.0, 'false_positives': 17.0, 'loss': 0.1480912, 'precision': 0.89880955, 'recall': 0.8678161, 'true_negatives': 607.0, 'true_positives': 151.0, 'global_step': 74}\n",
        " \n",
        " \n",
        "_______________________Other_______________________\n",
        "train num label=1: 78\n",
        "train percent = 1: 0.09774436090225563\n",
        "train size: 798\n",
        "test num label=1: 30\n",
        "test percent = 1: 0.11235955056179775\n",
        "test size: 267\n",
        "Beginning Training!\n",
        "Training took time  0:02:03.984338\n",
        "test results:\n",
        "{'auc': 0.72911394, 'eval_accuracy': 0.9325843, 'false_negatives': 16.0, 'false_positives': 2.0, 'loss': 0.18177693, 'precision': 0.875, 'recall': 0.46666667, 'true_negatives': 235.0, 'true_positives': 14.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.83904916, 'eval_accuracy': 0.96741855, 'false_negatives': 25.0, 'false_positives': 1.0, 'loss': 0.104762405, 'precision': 0.9814815, 'recall': 0.67948717, 'true_negatives': 719.0, 'true_positives': 53.0, 'global_step': 74}\n",
        " \n",
        " \n",
        "_______________________Practice_______________________\n",
        "train num label=1: 47\n",
        "train percent = 1: 0.05889724310776942\n",
        "train size: 798\n",
        "test num label=1: 20\n",
        "test percent = 1: 0.0749063670411985\n",
        "test size: 267\n",
        "Beginning Training!\n",
        "Training took time  0:02:04.041493\n",
        "test results:\n",
        "{'auc': 0.675, 'eval_accuracy': 0.9513109, 'false_negatives': 13.0, 'false_positives': 0.0, 'loss': 0.23085767, 'precision': 1.0, 'recall': 0.35, 'true_negatives': 247.0, 'true_positives': 7.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.9674193, 'eval_accuracy': 0.9949875, 'false_negatives': 3.0, 'false_positives': 1.0, 'loss': 0.02618777, 'precision': 0.9777778, 'recall': 0.9361702, 'true_negatives': 750.0, 'true_positives': 44.0, 'global_step': 74}\n",
        " \n",
        " \n",
        "_______________________Immigration_______________________\n",
        "train num label=1: 10\n",
        "train percent = 1: 0.012531328320802004\n",
        "train size: 798\n",
        "test num label=1: 2\n",
        "test percent = 1: 0.00749063670411985\n",
        "test size: 267\n",
        "Beginning Training!\n",
        "Training took time  0:02:03.435013\n",
        "test results:\n",
        "{'auc': 0.50000024, 'eval_accuracy': 0.99250937, 'false_negatives': 2.0, 'false_positives': 0.0, 'loss': 0.042053975, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 265.0, 'true_positives': 0.0, 'global_step': 74}\n",
        "train results:\n",
        "{'auc': 0.50000006, 'eval_accuracy': 0.98746866, 'false_negatives': 10.0, 'false_positives': 0.0, 'loss': 0.071802266, 'precision': 0.0, 'recall': 0.0, 'true_negatives': 788.0, 'true_positives': 0.0, 'global_step': 74}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cBOD2bakFdF",
        "colab_type": "text"
      },
      "source": [
        "## Results from each label individually\n",
        "\n",
        "(before deleting output dir)\n",
        "\n",
        "**************OtherAdv**********************\n",
        "train num label=1: 324\n",
        "train percent = 1: 0.40601503759398494\n",
        "test num label=1: 101\n",
        "test percent = 1: 0.3782771535580524\n",
        "Beginning Training!\n",
        "Training took time  0:00:00.003997\n",
        "{'auc': 0.65659666, 'eval_accuracy': 0.60674155, 'false_negatives': 14.0, 'false_positives': 91.0, 'loss': 0.68718004, **'precision': 0.48876405, 'recall': 0.8613861**, 'true_negatives': 75.0, 'true_positives': 87.0, 'global_step': 74}\n",
        "{'auc': 0.7173973, 'eval_accuracy': 0.68922305, 'false_negatives': 43.0, 'false_positives': 205.0, 'loss': 0.6079148, 'precision': 0.5781893, 'recall': 0.86728394, 'true_negatives': 269.0, 'true_positives': 281.0, 'global_step': 74}\n",
        "\n",
        "**************VictimAdv**********************\n",
        "train num label=1: 40\n",
        "train percent = 1: 0.05012531328320802\n",
        "test num label=1: 11\n",
        "test percent = 1: 0.04119850187265917\n",
        "Beginning Training!\n",
        "Training took time  0:00:00.004482\n",
        "{'auc': 0.49201, 'eval_accuracy': 0.35955057, 'false_negatives': 4.0, 'false_positives': 167.0, 'loss': 0.8812283, **'precision': 0.040229887, 'recall': 0.6363636**, 'true_negatives': 89.0, 'true_positives': 7.0, 'global_step': 74}\n",
        "{'auc': 0.60976255, 'eval_accuracy': 0.4385965, 'false_negatives': 8.0, 'false_positives': 440.0, 'loss': 0.86696815, 'precision': 0.06779661, 'recall': 0.8, 'true_negatives': 318.0, 'true_positives': 32.0, 'global_step': 74}\n",
        "\n",
        "**************Guns**********************\n",
        "train num label=1: 558\n",
        "train percent = 1: 0.6992481203007519\n",
        "test num label=1: 187\n",
        "test percent = 1: 0.700374531835206\n",
        "Beginning Training!\n",
        "Training took time  0:00:00.004301\n",
        "{'auc': 0.67790776, 'eval_accuracy': 0.70411986, 'false_negatives': 48.0, 'false_positives': 31.0, 'loss': 0.58773005, **'precision': 0.81764704, 'recall': 0.7433155**, 'true_negatives': 49.0, 'true_positives': 139.0, 'global_step': 74}\n",
        "{'auc': 0.640793, 'eval_accuracy': 0.6553885, 'false_negatives': 180.0, 'false_positives': 95.0, 'loss': 0.6048415, 'precision': 0.79915434, 'recall': 0.67741936, 'true_negatives': 145.0, 'true_positives': 378.0, 'global_step': 74}\n",
        "\n",
        "**************InfoSharing**********************\n",
        "train num label=1: 41\n",
        "train percent = 1: 0.05137844611528822\n",
        "test num label=1: 10\n",
        "test percent = 1: 0.03745318352059925\n",
        "Beginning Training!\n",
        "Training took time  0:00:00.004473\n",
        "{'auc': 0.388716, 'eval_accuracy': 0.37827715, 'false_negatives': 6.0, 'false_positives': 160.0, 'loss': 0.89093673, **'precision': 0.024390243, 'recall': 0.4**, 'true_negatives': 97.0, 'true_positives': 4.0, 'global_step': 74}\n",
        "{'auc': 0.5099398, 'eval_accuracy': 0.39849624, 'false_negatives': 15.0, 'false_positives': 465.0, 'loss': 0.8835133, 'precision': 0.052953158, 'recall': 0.63414633, 'true_negatives': 292.0, 'true_positives': 26.0, 'global_step': 74}\n",
        "\n",
        "**************MentalHealth**********************\n",
        "train num label=1: 174\n",
        "train percent = 1: 0.21804511278195488\n",
        "test num label=1: 59\n",
        "test percent = 1: 0.2209737827715356\n",
        "Beginning Training!\n",
        "Training took time  0:00:00.004239\n",
        "{'auc': 0.34888363, 'eval_accuracy': 0.3071161, 'false_negatives': 34.0, 'false_positives': 151.0, 'loss': 0.89569867, **'precision': 0.14204545, 'recall': 0.42372882**, 'true_negatives': 57.0, 'true_positives': 25.0, 'global_step': 74}\n",
        "{'auc': 0.32230884, 'eval_accuracy': 0.32581455, 'false_negatives': 119.0, 'false_positives': 419.0, 'loss': 0.9035461, 'precision': 0.116033755, 'recall': 0.31609195, 'true_negatives': 205.0, 'true_positives': 55.0, 'global_step': 74}\n",
        "\n",
        "**************Other**********************\n",
        "train num label=1: 78\n",
        "train percent = 1: 0.09774436090225563\n",
        "test num label=1: 30\n",
        "test percent = 1: 0.11235955056179775\n",
        "Beginning Training!\n",
        "Training took time  0:00:00.004054\n",
        "{'auc': 0.38143462, 'eval_accuracy': 0.3670412, 'false_negatives': 18.0, 'false_positives': 151.0, 'loss': 0.8866387, **'precision': 0.073619634, 'recall': 0.4**, 'true_negatives': 86.0, 'true_positives': 12.0, 'global_step': 74}\n",
        "{'auc': 0.4758013, 'eval_accuracy': 0.4047619, 'false_negatives': 34.0, 'false_positives': 441.0, 'loss': 0.87344176, 'precision': 0.09072165, 'recall': 0.5641026, 'true_negatives': 279.0, 'true_positives': 44.0, 'global_step': 74}\n",
        "\n",
        "**************Practice**********************\n",
        "train num label=1: 47\n",
        "train percent = 1: 0.05889724310776942\n",
        "test num label=1: 20\n",
        "test percent = 1: 0.0749063670411985\n",
        "Beginning Training!\n",
        "Training took time  0:00:00.003798\n",
        "{'auc': 0.26396763, 'eval_accuracy': 0.31835207, 'false_negatives': 16.0, 'false_positives': 166.0, 'loss': 0.9134971, **'precision': 0.023529412, 'recall': 0.2**, 'true_negatives': 81.0, 'true_positives': 4.0, 'global_step': 74}\n",
        "{'auc': 0.27883393, 'eval_accuracy': 0.35588974, 'false_negatives': 38.0, 'false_positives': 476.0, 'loss': 0.90788925, 'precision': 0.018556701, 'recall': 0.19148937, 'true_negatives': 275.0, 'true_positives': 9.0, 'global_step': 74}\n",
        "\n",
        "**************Immigration**********************\n",
        "train num label=1: 10\n",
        "train percent = 1: 0.012531328320802004\n",
        "test num label=1: 2\n",
        "test percent = 1: 0.00749063670411985\n",
        "Beginning Training!\n",
        "Training took time  0:00:00.004241\n",
        "{'auc': 0.6811322, 'eval_accuracy': 0.3670412, 'false_negatives': 0.0, 'false_positives': 169.0, 'loss': 0.87873244, **'precision': 0.0116959065, 'recall': 1.0**, 'true_negatives': 96.0, 'true_positives': 2.0, 'global_step': 74}\n",
        "{'auc': 0.54860413, 'eval_accuracy': 0.4010025, 'false_negatives': 3.0, 'false_positives': 475.0, 'loss': 0.87476, 'precision': 0.014522822, 'recall': 0.7, 'true_negatives': 313.0, 'true_positives': 7.0, 'global_step': 74}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHkjjwkHk-4-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}